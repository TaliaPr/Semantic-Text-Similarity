{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277ddaed",
   "metadata": {},
   "source": [
    "# Text Classification with TECLA Dataset\n",
    "\n",
    "This notebook demonstrates how to load and use the TECLA dataset from Hugging Face's datasets library for text classification tasks in Catalan language.\n",
    "\n",
    "TECLA (TEChniques for LAnguage processing) is a dataset for Catalan text classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b02577f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves del dataset: dict_keys(['train', 'validation', 'test'])\n",
      "Tamaño del conjunto de entrenamiento: 90700\n",
      "Tamaño del conjunto de validación: 5669\n",
      "Tamaño del conjunto de prueba: 17007\n",
      "\n",
      "Columnas disponibles: ['sentence', 'label1', 'label2']\n",
      "\n",
      "Ejemplo de los datos:\n",
      "{'sentence': \"L'ACA reactiva el retorn del cànon de l'aigua que s'envia a Tarragona i millorarà l'eficiència del canal de l'esquerra. L'obra corregirà pèrdues d'aigua a la sèquia del Cementiri de Deltebre amb una inversió de 900.000 euros. Després d'alguns exercici de paràlisi, la Comunitat de Regants de l'Esquerra de l'Ebre i l'ACA han signat, aquest dilluns, un nou conveni per reactivar obres de millora de l'eficiència d'infraestructures de reg i evitar la pèrdua d'aigua. Les actuacions començaran al febrer de l'any que ve en un tram de 4,9 quilòmetres de la sèquia del Cementiri, al terme municipal de Deltebre (Baix Ebre).Es col·locaran plaques i llits de graves, làmines de geotèxtil, i es revestiran més de 3 quilòmetres de la sèquia. Els 900.000 euros provenen del 20% del cànon de derivació de l'aigua que paga Tarragona i que s'estipula que s'ha de revertir al territori.\", 'label1': 'Economia', 'label2': 'Agroalimentació'}\n",
      "\n",
      "Número de clases en label1: 4\n",
      "Clases disponibles en label1: {'Societat', 'Política', 'Cultura', 'Economia'}\n",
      "\n",
      "Número de clases en label2: 53\n",
      "Clases disponibles en label2: {'Entitats', 'Trànsit', 'Noves tecnologies', 'Comptes públics', 'Policial', 'Energia', 'Festa i cultura popular', 'Llengua', 'Unió Europea', 'Moda', 'Immigració', 'Judicial', 'Salut', 'Música', 'Meteorologia', 'Turisme', 'Mobilitat', 'Cinema', 'Innovació', 'Cooperació', 'Agroalimentació', 'Habitatge', 'Recerca', 'Urbanisme', 'Treball', 'Infraestructures', 'Exteriors', 'Equipaments i patrimoni', 'Govern espanyol', 'Finances', 'Lletres', 'Esports', 'Comerç', 'Universitats', 'Moviments socials', 'Govern', 'Medi ambient', 'Empresa', 'Educació', 'Partits', 'Indústria', 'Parlament', 'Religió', 'Serveis Socials', 'Política municipal', 'Hisenda', 'Castells', 'Memòria històrica', 'Arts', 'Successos', 'Gastronomia', 'Logística', 'Teatre'}\n",
      "\n",
      "Distribución de label1:\n",
      "Societat: 36906 (40.69%)\n",
      "Política: 25568 (28.19%)\n",
      "Economia: 16305 (17.98%)\n",
      "Cultura: 11921 (13.14%)\n",
      "\n",
      "Distribución de label2:\n",
      "Partits: 10055 (11.09%)\n",
      "Successos: 7874 (8.68%)\n",
      "Govern: 6506 (7.17%)\n",
      "Judicial: 5789 (6.38%)\n",
      "Policial: 5557 (6.13%)\n",
      "Salut: 5430 (5.99%)\n",
      "Parlament: 4177 (4.61%)\n",
      "Agroalimentació: 3236 (3.57%)\n",
      "Medi ambient: 3028 (3.34%)\n",
      "Música: 2872 (3.17%)\n",
      "Educació: 2757 (3.04%)\n",
      "Empresa: 2699 (2.98%)\n",
      "Política municipal: 2094 (2.31%)\n",
      "Arts: 1989 (2.19%)\n",
      "Infraestructures: 1741 (1.92%)\n",
      "Serveis Socials: 1683 (1.86%)\n",
      "Treball: 1657 (1.83%)\n",
      "Mobilitat: 1625 (1.79%)\n",
      "Cinema: 1560 (1.72%)\n",
      "Teatre: 1493 (1.65%)\n",
      "Turisme: 1232 (1.36%)\n",
      "Equipaments i patrimoni: 1230 (1.36%)\n",
      "Unió Europea: 1186 (1.31%)\n",
      "Lletres: 1181 (1.30%)\n",
      "Meteorologia: 1080 (1.19%)\n",
      "Govern espanyol: 985 (1.09%)\n",
      "Comerç: 984 (1.08%)\n",
      "Festa i cultura popular: 890 (0.98%)\n",
      "Trànsit: 865 (0.95%)\n",
      "Finances: 774 (0.85%)\n",
      "Universitats: 749 (0.83%)\n",
      "Habitatge: 722 (0.80%)\n",
      "Esports: 498 (0.55%)\n",
      "Recerca: 470 (0.52%)\n",
      "Entitats: 432 (0.48%)\n",
      "Noves tecnologies: 428 (0.47%)\n",
      "Energia: 416 (0.46%)\n",
      "Religió: 314 (0.35%)\n",
      "Llengua: 286 (0.32%)\n",
      "Moviments socials: 282 (0.31%)\n",
      "Cooperació: 250 (0.28%)\n",
      "Indústria: 233 (0.26%)\n",
      "Castells: 231 (0.25%)\n",
      "Comptes públics: 206 (0.23%)\n",
      "Immigració: 189 (0.21%)\n",
      "Gastronomia: 189 (0.21%)\n",
      "Exteriors: 133 (0.15%)\n",
      "Innovació: 114 (0.13%)\n",
      "Memòria històrica: 91 (0.10%)\n",
      "Urbanisme: 86 (0.09%)\n",
      "Logística: 80 (0.09%)\n",
      "Moda: 53 (0.06%)\n",
      "Hisenda: 19 (0.02%)\n",
      "Societat: 36906 (40.69%)\n",
      "Política: 25568 (28.19%)\n",
      "Economia: 16305 (17.98%)\n",
      "Cultura: 11921 (13.14%)\n",
      "\n",
      "Distribución de label2:\n",
      "Partits: 10055 (11.09%)\n",
      "Successos: 7874 (8.68%)\n",
      "Govern: 6506 (7.17%)\n",
      "Judicial: 5789 (6.38%)\n",
      "Policial: 5557 (6.13%)\n",
      "Salut: 5430 (5.99%)\n",
      "Parlament: 4177 (4.61%)\n",
      "Agroalimentació: 3236 (3.57%)\n",
      "Medi ambient: 3028 (3.34%)\n",
      "Música: 2872 (3.17%)\n",
      "Educació: 2757 (3.04%)\n",
      "Empresa: 2699 (2.98%)\n",
      "Política municipal: 2094 (2.31%)\n",
      "Arts: 1989 (2.19%)\n",
      "Infraestructures: 1741 (1.92%)\n",
      "Serveis Socials: 1683 (1.86%)\n",
      "Treball: 1657 (1.83%)\n",
      "Mobilitat: 1625 (1.79%)\n",
      "Cinema: 1560 (1.72%)\n",
      "Teatre: 1493 (1.65%)\n",
      "Turisme: 1232 (1.36%)\n",
      "Equipaments i patrimoni: 1230 (1.36%)\n",
      "Unió Europea: 1186 (1.31%)\n",
      "Lletres: 1181 (1.30%)\n",
      "Meteorologia: 1080 (1.19%)\n",
      "Govern espanyol: 985 (1.09%)\n",
      "Comerç: 984 (1.08%)\n",
      "Festa i cultura popular: 890 (0.98%)\n",
      "Trànsit: 865 (0.95%)\n",
      "Finances: 774 (0.85%)\n",
      "Universitats: 749 (0.83%)\n",
      "Habitatge: 722 (0.80%)\n",
      "Esports: 498 (0.55%)\n",
      "Recerca: 470 (0.52%)\n",
      "Entitats: 432 (0.48%)\n",
      "Noves tecnologies: 428 (0.47%)\n",
      "Energia: 416 (0.46%)\n",
      "Religió: 314 (0.35%)\n",
      "Llengua: 286 (0.32%)\n",
      "Moviments socials: 282 (0.31%)\n",
      "Cooperació: 250 (0.28%)\n",
      "Indústria: 233 (0.26%)\n",
      "Castells: 231 (0.25%)\n",
      "Comptes públics: 206 (0.23%)\n",
      "Immigració: 189 (0.21%)\n",
      "Gastronomia: 189 (0.21%)\n",
      "Exteriors: 133 (0.15%)\n",
      "Innovació: 114 (0.13%)\n",
      "Memòria històrica: 91 (0.10%)\n",
      "Urbanisme: 86 (0.09%)\n",
      "Logística: 80 (0.09%)\n",
      "Moda: 53 (0.06%)\n",
      "Hisenda: 19 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset de projecte-aina/tecla desde Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset TECLA\n",
    "tecla_dataset = load_dataset(\"projecte-aina/tecla\")\n",
    "\n",
    "# Mostrar información sobre el dataset\n",
    "print(f\"Claves del dataset: {tecla_dataset.keys()}\")\n",
    "print(f\"Tamaño del conjunto de entrenamiento: {len(tecla_dataset['train'])}\")\n",
    "print(f\"Tamaño del conjunto de validación: {len(tecla_dataset['validation'])}\")\n",
    "print(f\"Tamaño del conjunto de prueba: {len(tecla_dataset['test'])}\")\n",
    "\n",
    "# Mostrar las columnas disponibles en el dataset\n",
    "print(f\"\\nColumnas disponibles: {tecla_dataset['train'].column_names}\")\n",
    "\n",
    "# Mostrar un ejemplo de los datos\n",
    "print(\"\\nEjemplo de los datos:\")\n",
    "print(tecla_dataset['train'][0])\n",
    "\n",
    "# Explorar las clases/etiquetas disponibles para label1 y label2\n",
    "label1_values = set(tecla_dataset['train']['label1'])\n",
    "label2_values = set(tecla_dataset['train']['label2'])\n",
    "\n",
    "print(f\"\\nNúmero de clases en label1: {len(label1_values)}\")\n",
    "print(f\"Clases disponibles en label1: {label1_values}\")\n",
    "\n",
    "print(f\"\\nNúmero de clases en label2: {len(label2_values)}\")\n",
    "print(f\"Clases disponibles en label2: {label2_values}\")\n",
    "\n",
    "# Ver la distribución de las etiquetas\n",
    "print(\"\\nDistribución de label1:\")\n",
    "label1_counts = {}\n",
    "for label in tecla_dataset['train']['label1']:\n",
    "    label1_counts[label] = label1_counts.get(label, 0) + 1\n",
    "for label, count in sorted(label1_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{label}: {count} ({count/len(tecla_dataset['train'])*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nDistribución de label2:\")\n",
    "label2_counts = {}\n",
    "for label in tecla_dataset['train']['label2']:\n",
    "    label2_counts[label] = label2_counts.get(label, 0) + 1\n",
    "for label, count in sorted(label2_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{label}: {count} ({count/len(tecla_dataset['train'])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db16e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# We'll use a pre-trained model for Catalan language\n",
    "model_name = \"projecte-aina/roberta-base-ca\"\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "# We'll focus on label1 for this classification task\n",
    "# You can change to label2 if needed\n",
    "target_label = \"label1\"\n",
    "print(f\"Classification target: {target_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c176b3",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "We'll tokenize the text data using the tokenizer from the pre-trained model and prepare it for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f103a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Function to tokenize and prepare the dataset\n",
    "def tokenize_dataset(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
    "\n",
    "# Tokenize and prepare the datasets\n",
    "train_dataset = tecla_dataset[\"train\"]\n",
    "val_dataset = tecla_dataset[\"validation\"]\n",
    "test_dataset = tecla_dataset[\"test\"]\n",
    "\n",
    "# Get unique labels and create a mapping\n",
    "labels = sorted(list(set(train_dataset[target_label])))\n",
    "label2id = {label: i for i, label in enumerate(labels)}\n",
    "id2label = {i: label for i, label in enumerate(labels)}\n",
    "num_labels = len(labels)\n",
    "\n",
    "print(f\"Number of labels: {num_labels}\")\n",
    "print(f\"Label mapping: {label2id}\")\n",
    "\n",
    "# Tokenize datasets\n",
    "train_encodings = tokenizer(train_dataset[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(val_dataset[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "test_encodings = tokenizer(test_dataset[\"text\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Convert labels to numeric format\n",
    "train_labels = [label2id[label] for label in train_dataset[target_label]]\n",
    "val_labels = [label2id[label] for label in val_dataset[target_label]]\n",
    "test_labels = [label2id[label] for label in test_dataset[target_label]]\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "train_labels = torch.tensor(train_labels)\n",
    "val_labels = torch.tensor(val_labels)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_encodings.input_ids, train_encodings.attention_mask, train_labels)\n",
    "val_dataset = TensorDataset(val_encodings.input_ids, val_encodings.attention_mask, val_labels)\n",
    "test_dataset = TensorDataset(test_encodings.input_ids, test_encodings.attention_mask, test_labels)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e368b97",
   "metadata": {},
   "source": [
    "## Model Definition\n",
    "\n",
    "We'll fine-tune a pre-trained RoBERTa model for Catalan language specifically for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8327c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained model with a classification head\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "total_steps = len(train_loader) * 5  # 5 epochs\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=total_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08348ba4",
   "metadata": {},
   "source": [
    "## Training Function\n",
    "\n",
    "We'll define functions for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate accuracy\n",
    "def compute_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return accuracy_score(labels_flat, pred_flat)\n",
    "\n",
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch = tuple(b.to(device) for b in batch)\n",
    "        inputs = {\n",
    "            'input_ids': batch[0],\n",
    "            'attention_mask': batch[1],\n",
    "            'labels': batch[2]\n",
    "        }\n",
    "        \n",
    "        # Clear previous gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the norm of gradients to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters and learning rate\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Move logits and labels to CPU for sklearn metrics\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels = inputs['labels'].cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(logits)\n",
    "        all_labels.extend(labels)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    accuracy = compute_accuracy(all_preds, all_labels)\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = tuple(b.to(device) for b in batch)\n",
    "            inputs = {\n",
    "                'input_ids': batch[0],\n",
    "                'attention_mask': batch[1],\n",
    "                'labels': batch[2]\n",
    "            }\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Move logits and labels to CPU for sklearn metrics\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            labels = inputs['labels'].cpu().numpy()\n",
    "            \n",
    "            all_preds.extend(logits)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_labels = np.array(all_labels)\n",
    "    accuracy = compute_accuracy(all_preds, all_labels)\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f6f1cb",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now we'll train the model for a few epochs and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb757dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "epochs = 5\n",
    "best_val_accuracy = 0\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    print('-' * 40)\n",
    "    \n",
    "    # Training\n",
    "    train_loss, train_accuracy = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Validation\n",
    "    val_loss, val_accuracy, val_preds, val_labels = evaluate(model, val_loader, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Save the best model\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        torch.save(model.state_dict(), 'best_tecla_model.pt')\n",
    "        print(\"Saved best model!\")\n",
    "\n",
    "# Plot training and validation metrics\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss over Epochs')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce791c30",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate our best model on the test set and analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c65cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_tecla_model.pt'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy, test_preds, test_labels = evaluate(model, test_loader, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "test_predictions = np.argmax(test_preds, axis=1)\n",
    "\n",
    "# Create classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=labels))\n",
    "\n",
    "# Create confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "cm = confusion_matrix(test_labels, test_predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45857603",
   "metadata": {},
   "source": [
    "## Error Analysis\n",
    "\n",
    "Let's analyze some misclassified examples to understand where our model struggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826a6a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get misclassified examples\n",
    "misclassified_indices = []\n",
    "test_pred_classes = np.argmax(test_preds, axis=1)\n",
    "\n",
    "for i, (pred, true) in enumerate(zip(test_pred_classes, test_labels)):\n",
    "    if pred != true:\n",
    "        misclassified_indices.append(i)\n",
    "\n",
    "# Look at some misclassified examples\n",
    "print(f\"Number of misclassified examples: {len(misclassified_indices)}\")\n",
    "\n",
    "if len(misclassified_indices) > 0:\n",
    "    # Take up to 10 misclassified examples\n",
    "    samples = min(10, len(misclassified_indices))\n",
    "    print(f\"\\nShowing {samples} misclassified examples:\")\n",
    "    \n",
    "    for i in range(samples):\n",
    "        idx = misclassified_indices[i]\n",
    "        text = tecla_dataset[\"test\"][idx][\"text\"]\n",
    "        true_label = tecla_dataset[\"test\"][idx][target_label]\n",
    "        pred_label = id2label[test_pred_classes[idx]]\n",
    "        \n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Text: {text[:100]}...\")  # Show just the beginning of the text\n",
    "        print(f\"True label: {true_label}\")\n",
    "        print(f\"Predicted label: {pred_label}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7691153",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "Let's try to understand which parts of the text are important for classification using a technique like attention visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import random\n",
    "\n",
    "# Load the model for interpretation\n",
    "model.eval()\n",
    "\n",
    "# Create a pipeline for token classification to visualize attention\n",
    "nlp = pipeline(\n",
    "    \"text-classification\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    return_all_scores=True\n",
    ")\n",
    "\n",
    "# Select a few random examples from the test set\n",
    "random_indices = random.sample(range(len(tecla_dataset[\"test\"])), 5)\n",
    "\n",
    "for idx in random_indices:\n",
    "    text = tecla_dataset[\"test\"][idx][\"text\"]\n",
    "    true_label = tecla_dataset[\"test\"][idx][target_label]\n",
    "    \n",
    "    # Get prediction\n",
    "    result = nlp(text)\n",
    "    \n",
    "    # Sort scores and get the predicted label\n",
    "    scores = {labels[int(item['label'].split('_')[-1])]: item['score'] for item in result[0]}\n",
    "    pred_label = max(scores, key=scores.get)\n",
    "    \n",
    "    print(f\"\\nExample:\")\n",
    "    print(f\"Text: {text[:150]}...\")  # Show beginning of text\n",
    "    print(f\"True label: {true_label}\")\n",
    "    print(f\"Predicted label: {pred_label}\")\n",
    "    print(\"Confidence scores:\")\n",
    "    for label, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {label}: {score:.4f}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Save the model and tokenizer for future use\n",
    "model_save_path = \"tecla_classifier_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1cbda1",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We've successfully trained and evaluated a RoBERTa-based classification model for the TECLA dataset in Catalan. The model achieves good performance on the test set and can be used for Catalan text classification tasks.\n",
    "\n",
    "Key observations:\n",
    "1. The model's performance varies across different classes\n",
    "2. Some classes are harder to distinguish than others\n",
    "3. The model achieves good overall accuracy on the test set\n",
    "\n",
    "Future improvements:\n",
    "1. Try different pre-trained models specific to Catalan\n",
    "2. Experiment with data augmentation for underrepresented classes\n",
    "3. Try ensemble methods to improve performance\n",
    "4. Use longer context windows for documents that might be truncated"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

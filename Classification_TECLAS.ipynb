{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "277ddaed",
   "metadata": {},
   "source": [
    "# Text Classification with TECLA Dataset\n",
    "\n",
    "This notebook demonstrates how to load and use the TECLA dataset from Hugging Face's datasets library for text classification tasks in Catalan language.\n",
    "\n",
    "TECLA (TEChniques for LAnguage processing) is a dataset for Catalan text classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f385c05",
   "metadata": {},
   "source": [
    "## Instalaci√≥n de dependencias\n",
    "\n",
    "Antes de comenzar, necesitas instalar las bibliotecas requeridas. Ejecuta el siguiente comando en tu terminal o en una celda de notebook con el prefijo `!`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8849c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "     ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/10.5 MB 7.7 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 1.2/10.5 MB 14.9 MB/s eta 0:00:01\n",
      "     ------ --------------------------------- 1.7/10.5 MB 12.3 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.5/10.5 MB 14.6 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 3.4/10.5 MB 16.6 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 4.6/10.5 MB 17.2 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 5.5/10.5 MB 17.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.2/10.5 MB 18.1 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 7.5/10.5 MB 19.1 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 8.7/10.5 MB 19.9 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 9.9/10.5 MB 20.4 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.5/10.5 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.5/10.5 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.5/10.5 MB 18.7 MB/s eta 0:00:00\n",
      "Collecting datasets\n",
      "  Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "     ---------------------------------------- 0.0/491.5 kB ? eta -:--:--\n",
      "     -------------------------- ---------- 358.4/491.5 kB 11.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 491.5/491.5 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting torch\n",
      "  Downloading torch-2.7.0-cp310-cp310-win_amd64.whl (212.5 MB)\n",
      "     ---------------------------------------- 0.0/212.5 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.9/212.5 MB 29.1 MB/s eta 0:00:08\n",
      "     --------------------------------------- 1.9/212.5 MB 24.8 MB/s eta 0:00:09\n",
      "      -------------------------------------- 3.0/212.5 MB 21.4 MB/s eta 0:00:10\n",
      "      -------------------------------------- 4.3/212.5 MB 22.9 MB/s eta 0:00:10\n",
      "      -------------------------------------- 5.4/212.5 MB 23.0 MB/s eta 0:00:09\n",
      "     - ------------------------------------- 6.3/212.5 MB 23.8 MB/s eta 0:00:09\n",
      "     - ------------------------------------- 7.4/212.5 MB 23.7 MB/s eta 0:00:09\n",
      "     - ------------------------------------- 8.4/212.5 MB 23.5 MB/s eta 0:00:09\n",
      "     - ------------------------------------- 9.6/212.5 MB 23.5 MB/s eta 0:00:09\n",
      "     - ------------------------------------ 10.5/212.5 MB 22.6 MB/s eta 0:00:09\n",
      "     -- ----------------------------------- 11.4/212.5 MB 21.8 MB/s eta 0:00:10\n",
      "     -- ----------------------------------- 12.3/212.5 MB 21.8 MB/s eta 0:00:10\n",
      "     -- ----------------------------------- 13.3/212.5 MB 21.9 MB/s eta 0:00:10\n",
      "     -- ----------------------------------- 14.2/212.5 MB 21.1 MB/s eta 0:00:10\n",
      "     -- ----------------------------------- 15.1/212.5 MB 20.5 MB/s eta 0:00:10\n",
      "     -- ----------------------------------- 16.0/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 16.8/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 17.8/212.5 MB 19.3 MB/s eta 0:00:11\n",
      "     --- ---------------------------------- 19.0/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 19.9/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 20.8/212.5 MB 19.3 MB/s eta 0:00:10\n",
      "     --- ---------------------------------- 21.8/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 22.8/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 24.1/212.5 MB 20.5 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 24.8/212.5 MB 20.5 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 25.9/212.5 MB 20.5 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 26.9/212.5 MB 20.5 MB/s eta 0:00:10\n",
      "     ---- --------------------------------- 27.7/212.5 MB 20.5 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 28.3/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 29.4/212.5 MB 19.8 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 29.9/212.5 MB 18.7 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 30.9/212.5 MB 18.7 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 31.9/212.5 MB 18.7 MB/s eta 0:00:10\n",
      "     ----- -------------------------------- 32.8/212.5 MB 19.3 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 33.7/212.5 MB 19.3 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 34.7/212.5 MB 18.7 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 35.6/212.5 MB 18.2 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 36.1/212.5 MB 17.7 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 37.1/212.5 MB 17.7 MB/s eta 0:00:10\n",
      "     ------ ------------------------------- 37.8/212.5 MB 17.3 MB/s eta 0:00:11\n",
      "     ------ ------------------------------- 38.4/212.5 MB 17.2 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 39.3/212.5 MB 16.8 MB/s eta 0:00:11\n",
      "     ------- ------------------------------ 40.3/212.5 MB 17.7 MB/s eta 0:00:10\n",
      "     ------- ------------------------------ 41.3/212.5 MB 18.2 MB/s eta 0:00:10\n",
      "     ------- ------------------------------ 41.9/212.5 MB 17.3 MB/s eta 0:00:10\n",
      "     ------- ------------------------------ 42.6/212.5 MB 17.2 MB/s eta 0:00:10\n",
      "     ------- ------------------------------ 43.7/212.5 MB 17.2 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 45.0/212.5 MB 17.7 MB/s eta 0:00:10\n",
      "     -------- ----------------------------- 46.4/212.5 MB 19.3 MB/s eta 0:00:09\n",
      "     -------- ----------------------------- 47.6/212.5 MB 19.8 MB/s eta 0:00:09\n",
      "     -------- ----------------------------- 48.9/212.5 MB 22.5 MB/s eta 0:00:08\n",
      "     -------- ----------------------------- 50.3/212.5 MB 22.6 MB/s eta 0:00:08\n",
      "     --------- ---------------------------- 51.6/212.5 MB 23.4 MB/s eta 0:00:07\n",
      "     --------- ---------------------------- 53.0/212.5 MB 26.2 MB/s eta 0:00:07\n",
      "     --------- ---------------------------- 53.7/212.5 MB 26.2 MB/s eta 0:00:07\n",
      "     --------- ---------------------------- 54.6/212.5 MB 25.1 MB/s eta 0:00:07\n",
      "     --------- ---------------------------- 55.9/212.5 MB 24.3 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 57.0/212.5 MB 24.2 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 58.1/212.5 MB 24.2 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 58.8/212.5 MB 24.3 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 59.8/212.5 MB 23.4 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 60.7/212.5 MB 21.8 MB/s eta 0:00:07\n",
      "     ---------- --------------------------- 60.9/212.5 MB 21.1 MB/s eta 0:00:08\n",
      "     ---------- --------------------------- 61.4/212.5 MB 19.8 MB/s eta 0:00:08\n",
      "     ----------- -------------------------- 61.6/212.5 MB 18.7 MB/s eta 0:00:09\n",
      "     ----------- -------------------------- 62.1/212.5 MB 17.2 MB/s eta 0:00:09\n",
      "     ----------- -------------------------- 62.6/212.5 MB 17.2 MB/s eta 0:00:09\n",
      "     ----------- -------------------------- 63.2/212.5 MB 16.0 MB/s eta 0:00:10\n",
      "     ----------- -------------------------- 63.6/212.5 MB 15.2 MB/s eta 0:00:10\n",
      "     ----------- -------------------------- 63.8/212.5 MB 14.9 MB/s eta 0:00:11\n",
      "     ----------- -------------------------- 64.5/212.5 MB 14.9 MB/s eta 0:00:10\n",
      "     ----------- -------------------------- 65.4/212.5 MB 14.6 MB/s eta 0:00:11\n",
      "     ----------- -------------------------- 66.4/212.5 MB 14.2 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 67.2/212.5 MB 13.9 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 67.7/212.5 MB 13.4 MB/s eta 0:00:11\n",
      "     ------------ ------------------------- 68.1/212.5 MB 13.1 MB/s eta 0:00:12\n",
      "     ------------ ------------------------- 68.7/212.5 MB 12.8 MB/s eta 0:00:12\n",
      "     ------------ ------------------------- 69.5/212.5 MB 12.8 MB/s eta 0:00:12\n",
      "     ------------ ------------------------- 70.3/212.5 MB 12.6 MB/s eta 0:00:12\n",
      "     ------------ ------------------------- 71.1/212.5 MB 12.6 MB/s eta 0:00:12\n",
      "     ------------ ------------------------- 71.9/212.5 MB 13.9 MB/s eta 0:00:11\n",
      "     ------------- ------------------------ 72.7/212.5 MB 14.2 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 73.7/212.5 MB 15.2 MB/s eta 0:00:10\n",
      "     ------------- ------------------------ 74.6/212.5 MB 15.6 MB/s eta 0:00:09\n",
      "     ------------- ------------------------ 75.5/212.5 MB 16.0 MB/s eta 0:00:09\n",
      "     ------------- ------------------------ 76.0/212.5 MB 15.6 MB/s eta 0:00:09\n",
      "     ------------- ------------------------ 76.9/212.5 MB 16.0 MB/s eta 0:00:09\n",
      "     ------------- ------------------------ 77.6/212.5 MB 16.0 MB/s eta 0:00:09\n",
      "     ------------- ------------------------ 77.8/212.5 MB 16.0 MB/s eta 0:00:09\n",
      "     -------------- ----------------------- 79.0/212.5 MB 16.0 MB/s eta 0:00:09\n",
      "     -------------- ----------------------- 80.2/212.5 MB 16.8 MB/s eta 0:00:08\n",
      "     -------------- ----------------------- 81.5/212.5 MB 18.2 MB/s eta 0:00:08\n",
      "     -------------- ----------------------- 82.5/212.5 MB 18.2 MB/s eta 0:00:08\n",
      "     -------------- ----------------------- 83.7/212.5 MB 18.7 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 84.9/212.5 MB 19.3 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 86.0/212.5 MB 20.5 MB/s eta 0:00:07\n",
      "     --------------- ---------------------- 87.1/212.5 MB 21.1 MB/s eta 0:00:06\n",
      "     --------------- ---------------------- 88.2/212.5 MB 24.2 MB/s eta 0:00:06\n",
      "     --------------- ---------------------- 89.3/212.5 MB 24.2 MB/s eta 0:00:06\n",
      "     ---------------- --------------------- 90.7/212.5 MB 24.2 MB/s eta 0:00:06\n",
      "     ---------------- --------------------- 92.0/212.5 MB 24.2 MB/s eta 0:00:05\n",
      "     ---------------- --------------------- 92.7/212.5 MB 23.4 MB/s eta 0:00:06\n",
      "     ---------------- --------------------- 93.1/212.5 MB 21.9 MB/s eta 0:00:06\n",
      "     ---------------- --------------------- 93.7/212.5 MB 21.1 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 95.1/212.5 MB 21.8 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 96.5/212.5 MB 21.8 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 97.7/212.5 MB 22.6 MB/s eta 0:00:06\n",
      "     ----------------- -------------------- 98.9/212.5 MB 22.6 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 100.3/212.5 MB 22.5 MB/s eta 0:00:05\n",
      "     ----------------- ------------------- 101.4/212.5 MB 21.8 MB/s eta 0:00:06\n",
      "     ----------------- ------------------- 102.8/212.5 MB 23.4 MB/s eta 0:00:05\n",
      "     ------------------ ------------------ 104.2/212.5 MB 26.2 MB/s eta 0:00:05\n",
      "     ------------------ ------------------ 105.7/212.5 MB 27.3 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 107.3/212.5 MB 28.4 MB/s eta 0:00:04\n",
      "     ------------------ ------------------ 108.3/212.5 MB 27.3 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 109.8/212.5 MB 28.5 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 111.4/212.5 MB 28.5 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 112.6/212.5 MB 29.7 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 113.7/212.5 MB 28.5 MB/s eta 0:00:04\n",
      "     ------------------- ----------------- 114.3/212.5 MB 27.3 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 115.5/212.5 MB 26.2 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 116.1/212.5 MB 24.2 MB/s eta 0:00:04\n",
      "     -------------------- ---------------- 116.8/212.5 MB 22.6 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 117.6/212.5 MB 21.8 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 118.8/212.5 MB 20.5 MB/s eta 0:00:05\n",
      "     -------------------- ---------------- 119.7/212.5 MB 20.5 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 120.8/212.5 MB 19.8 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 122.0/212.5 MB 19.9 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 123.1/212.5 MB 19.9 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 124.3/212.5 MB 20.5 MB/s eta 0:00:05\n",
      "     --------------------- --------------- 125.5/212.5 MB 20.5 MB/s eta 0:00:05\n",
      "     ---------------------- -------------- 126.8/212.5 MB 22.5 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 127.7/212.5 MB 24.2 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 128.6/212.5 MB 23.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 129.8/212.5 MB 23.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 130.8/212.5 MB 23.4 MB/s eta 0:00:04\n",
      "     ---------------------- -------------- 132.0/212.5 MB 23.4 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 132.4/212.5 MB 21.8 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 133.7/212.5 MB 21.8 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 135.1/212.5 MB 22.5 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 136.1/212.5 MB 22.6 MB/s eta 0:00:04\n",
      "     ----------------------- ------------- 137.1/212.5 MB 21.1 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 138.4/212.5 MB 22.6 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 139.7/212.5 MB 23.4 MB/s eta 0:00:04\n",
      "     ------------------------ ------------ 141.3/212.5 MB 24.2 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 142.6/212.5 MB 26.2 MB/s eta 0:00:03\n",
      "     ------------------------ ------------ 143.1/212.5 MB 26.2 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 143.9/212.5 MB 24.2 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 144.8/212.5 MB 25.2 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 145.9/212.5 MB 25.1 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 147.3/212.5 MB 26.2 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 147.9/212.5 MB 25.2 MB/s eta 0:00:03\n",
      "     ------------------------- ----------- 148.9/212.5 MB 23.4 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 150.1/212.5 MB 22.6 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 150.7/212.5 MB 21.1 MB/s eta 0:00:03\n",
      "     -------------------------- ---------- 151.2/212.5 MB 19.8 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 151.5/212.5 MB 17.7 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 152.1/212.5 MB 17.7 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 153.4/212.5 MB 17.7 MB/s eta 0:00:04\n",
      "     -------------------------- ---------- 154.8/212.5 MB 19.3 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 156.0/212.5 MB 19.3 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 157.0/212.5 MB 18.2 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 158.0/212.5 MB 18.2 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 158.8/212.5 MB 18.7 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 159.0/212.5 MB 18.2 MB/s eta 0:00:03\n",
      "     --------------------------- --------- 159.3/212.5 MB 16.8 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 159.6/212.5 MB 16.0 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 160.2/212.5 MB 15.2 MB/s eta 0:00:04\n",
      "     --------------------------- --------- 160.6/212.5 MB 15.2 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 161.3/212.5 MB 15.2 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 162.3/212.5 MB 16.4 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 162.9/212.5 MB 16.0 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 163.6/212.5 MB 14.9 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 164.3/212.5 MB 14.5 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 165.2/212.5 MB 14.2 MB/s eta 0:00:04\n",
      "     ---------------------------- -------- 166.1/212.5 MB 13.9 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 166.6/212.5 MB 13.6 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 167.2/212.5 MB 13.4 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 168.0/212.5 MB 13.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 168.8/212.5 MB 13.1 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 169.3/212.5 MB 13.4 MB/s eta 0:00:04\n",
      "     ----------------------------- ------- 170.5/212.5 MB 15.2 MB/s eta 0:00:03\n",
      "     ----------------------------- ------- 172.0/212.5 MB 16.4 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 173.2/212.5 MB 17.7 MB/s eta 0:00:03\n",
      "     ------------------------------ ------ 174.6/212.5 MB 19.3 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 176.0/212.5 MB 20.5 MB/s eta 0:00:02\n",
      "     ------------------------------ ------ 177.1/212.5 MB 22.6 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 178.3/212.5 MB 23.4 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 179.7/212.5 MB 27.3 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 180.1/212.5 MB 26.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 181.2/212.5 MB 25.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 182.2/212.5 MB 25.2 MB/s eta 0:00:02\n",
      "     ------------------------------- ----- 183.6/212.5 MB 26.2 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 184.7/212.5 MB 25.2 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 185.7/212.5 MB 25.2 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 186.5/212.5 MB 23.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 187.5/212.5 MB 22.6 MB/s eta 0:00:02\n",
      "     -------------------------------- ---- 188.6/212.5 MB 22.5 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 189.8/212.5 MB 21.1 MB/s eta 0:00:02\n",
      "     --------------------------------- --- 191.0/212.5 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 192.3/212.5 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 193.3/212.5 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- --- 194.5/212.5 MB 22.5 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 195.4/212.5 MB 22.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 196.3/212.5 MB 21.9 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 196.9/212.5 MB 21.1 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 198.3/212.5 MB 22.6 MB/s eta 0:00:01\n",
      "     ---------------------------------- -- 199.9/212.5 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 201.1/212.5 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 202.2/212.5 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 203.5/212.5 MB 22.6 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 204.4/212.5 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 205.5/212.5 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- - 206.6/212.5 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  208.1/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  209.4/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  210.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  211.8/212.5 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------  212.5/212.5 MB 26.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 212.5/212.5 MB 9.6 MB/s eta 0:00:00\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "     ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.6/12.9 MB 18.5 MB/s eta 0:00:01\n",
      "     --- ------------------------------------ 1.0/12.9 MB 10.5 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 2.1/12.9 MB 14.9 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 3.0/12.9 MB 16.0 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 4.2/12.9 MB 17.9 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 5.1/12.9 MB 18.0 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 6.1/12.9 MB 19.4 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 7.4/12.9 MB 19.7 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 8.7/12.9 MB 20.5 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.6/12.9 MB 20.4 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 11.0/12.9 MB 22.6 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 12.3/12.9 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  12.9/12.9 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 12.9/12.9 MB 22.6 MB/s eta 0:00:00\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "     ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.1/11.6 MB 35.0 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 2.3/11.6 MB 29.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.7/11.6 MB 29.3 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 4.5/11.6 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 5.5/11.6 MB 24.9 MB/s eta 0:00:01\n",
      "     --------------------- ------------------ 6.3/11.6 MB 23.7 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 6.9/11.6 MB 22.0 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.7/11.6 MB 21.4 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 8.2/11.6 MB 20.3 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.9/11.6 MB 19.7 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 9.4/11.6 MB 18.8 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 10.3/11.6 MB 18.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 11.1/11.6 MB 17.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.6/11.6 MB 16.8 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.6/11.6 MB 16.0 MB/s eta 0:00:00\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.3-cp310-cp310-win_amd64.whl (8.1 MB)\n",
      "     ---------------------------------------- 0.0/8.1 MB ? eta -:--:--\n",
      "     ------ --------------------------------- 1.3/8.1 MB 27.4 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 2.2/8.1 MB 28.1 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 3.2/8.1 MB 25.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 4.6/8.1 MB 26.9 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 5.4/8.1 MB 26.5 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 6.6/8.1 MB 24.7 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 7.5/8.1 MB 25.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  8.0/8.1 MB 25.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 8.1/8.1 MB 23.5 MB/s eta 0:00:00\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "     ---------------------------------------- 0.0/294.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 294.9/294.9 kB ? eta 0:00:00\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "     ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 1.1/11.1 MB 22.7 MB/s eta 0:00:01\n",
      "     --------- ------------------------------ 2.6/11.1 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 3.6/11.1 MB 25.5 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 5.6/11.1 MB 29.6 MB/s eta 0:00:01\n",
      "     -------------------------- ------------- 7.3/11.1 MB 30.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 8.4/11.1 MB 28.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 9.8/11.1 MB 28.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 10.6/11.1 MB 28.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  11.1/11.1 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------- 11.1/11.1 MB 24.2 MB/s eta 0:00:00\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
      "     ---------------------------------------- 0.0/362.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 362.1/362.1 kB 23.5 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 64.9/64.9 kB ? eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.30.0\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "     ---------------------------------------- 0.0/512.1 kB ? eta -:--:--\n",
      "     ------------------------------------- 512.1/512.1 kB 31.4 MB/s eta 0:00:00\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 0.0/78.5 kB ? eta -:--:--\n",
      "     ---------------------------------------- 78.5/78.5 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jiahu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (25.0)\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.5/2.4 MB 14.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------- 1.2/2.4 MB 15.5 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.9/2.4 MB 14.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.4/2.4 MB 15.5 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.3\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "     ---------------------------------------- 0.0/308.9 kB ? eta -:--:--\n",
      "     ------------------------------------- 308.9/308.9 kB 18.7 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "     ---------------------------------------- 0.0/274.0 kB ? eta -:--:--\n",
      "     ------------------------------------- 274.0/274.0 kB 17.6 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.2-cp310-cp310-win_amd64.whl (161 kB)\n",
      "     ---------------------------------------- 0.0/161.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 161.8/161.8 kB 9.5 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0\n",
      "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 116.3/116.3 kB ? eta 0:00:00\n",
      "Collecting fsspec[http]<=2025.3.0,>=2023.1.0\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "     ---------------------------------------- 0.0/193.6 kB ? eta -:--:--\n",
      "     ------------------------------------- 193.6/193.6 kB 11.5 MB/s eta 0:00:00\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-20.0.0-cp310-cp310-win_amd64.whl (25.8 MB)\n",
      "     ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "     -- ------------------------------------- 1.4/25.8 MB 45.3 MB/s eta 0:00:01\n",
      "     -- ------------------------------------- 1.8/25.8 MB 19.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 3.2/25.8 MB 22.3 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 3.9/25.8 MB 20.9 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 5.3/25.8 MB 22.6 MB/s eta 0:00:01\n",
      "     ---------- ----------------------------- 7.0/25.8 MB 23.6 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 8.6/25.8 MB 24.9 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 9.7/25.8 MB 24.8 MB/s eta 0:00:01\n",
      "     ---------------- ---------------------- 10.6/25.8 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 12.1/25.8 MB 27.3 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 13.2/25.8 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 14.2/25.8 MB 26.2 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 15.1/25.8 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 16.3/25.8 MB 25.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 17.3/25.8 MB 24.2 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 18.3/25.8 MB 23.4 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 19.4/25.8 MB 21.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 20.1/25.8 MB 21.9 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 21.1/25.8 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 22.2/25.8 MB 21.1 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 23.2/25.8 MB 21.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ -- 24.2/25.8 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 25.1/25.8 MB 20.5 MB/s eta 0:00:01\n",
      "     --------------------------------------  25.8/25.8 MB 19.9 MB/s eta 0:00:01\n",
      "     --------------------------------------  25.8/25.8 MB 19.9 MB/s eta 0:00:01\n",
      "     --------------------------------------- 25.8/25.8 MB 16.8 MB/s eta 0:00:00\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Collecting multiprocess<0.70.17\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 134.8/134.8 kB ? eta 0:00:00\n",
      "Collecting sympy>=1.13.3\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 0.9/6.3 MB 28.4 MB/s eta 0:00:01\n",
      "     ------------ --------------------------- 1.9/6.3 MB 20.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 2.9/6.3 MB 20.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 3.7/6.3 MB 21.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 4.7/6.3 MB 21.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 5.6/6.3 MB 21.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  6.3/6.3 MB 21.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 6.3/6.3 MB 19.2 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "     --------------------- ------------------ 0.9/1.7 MB 19.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 1.7/1.7 MB 18.4 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "     ---------------------------------------- 0.0/199.1 kB ? eta -:--:--\n",
      "     ---------------------------------------- 199.1/199.1 kB ? eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\jiahu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from torch) (4.14.0)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
      "     ---------------------------------------- 0.0/134.9 kB ? eta -:--:--\n",
      "     -------------------------------------- 134.9/134.9 kB 7.8 MB/s eta 0:00:00\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "     ---------------------------------------- 0.0/509.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 509.2/509.2 kB 16.1 MB/s eta 0:00:00\n",
      "Collecting tzdata>=2022.7\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "     ---------------------------------------- 0.0/347.8 kB ? eta -:--:--\n",
      "     ------------------------------------- 347.8/347.8 kB 21.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\jiahu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting kiwisolver>=1.3.1\n",
      "  Downloading kiwisolver-1.4.8-cp310-cp310-win_amd64.whl (71 kB)\n",
      "     ---------------------------------------- 0.0/71.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 71.9/71.9 kB 3.9 MB/s eta 0:00:00\n",
      "Collecting pillow>=8\n",
      "  Downloading pillow-11.2.1-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "     ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.5/2.7 MB 10.2 MB/s eta 0:00:01\n",
      "     ------------- -------------------------- 0.9/2.7 MB 11.6 MB/s eta 0:00:01\n",
      "     ------------------ --------------------- 1.2/2.7 MB 11.3 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.9/2.7 MB 11.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 2.5/2.7 MB 12.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.7/2.7 MB 11.4 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "     ---------------------------------------- 0.0/111.1 kB ? eta -:--:--\n",
      "     -------------------------------- ------ 92.2/111.1 kB 5.1 MB/s eta 0:00:01\n",
      "     -------------------------------- ------ 92.2/111.1 kB 5.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ 111.1/111.1 kB 807.0 kB/s eta 0:00:00\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.3.2-cp310-cp310-win_amd64.whl (221 kB)\n",
      "     ---------------------------------------- 0.0/221.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 221.2/221.2 kB 14.1 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.58.1-cp310-cp310-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "     ------------------ --------------------- 1.0/2.2 MB 21.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 23.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 23.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 23.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 10.2 MB/s eta 0:00:00\n",
      "Collecting joblib>=1.2.0\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "     ---------------------------------------- 0.0/307.7 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 286.7/307.7 kB ? eta -:--:--\n",
      "     ------------------------------------- -- 286.7/307.7 kB ? eta -:--:--\n",
      "     -------------------------------------- 307.7/307.7 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting scipy>=1.6.0\n",
      "  Downloading scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "     ---------------------------------------- 0.0/41.3 MB ? eta -:--:--\n",
      "     - -------------------------------------- 1.3/41.3 MB 28.1 MB/s eta 0:00:02\n",
      "     -- ------------------------------------- 2.5/41.3 MB 22.8 MB/s eta 0:00:02\n",
      "     --- ------------------------------------ 3.2/41.3 MB 25.3 MB/s eta 0:00:02\n",
      "     ---- ----------------------------------- 4.2/41.3 MB 22.1 MB/s eta 0:00:02\n",
      "     ----- ---------------------------------- 5.5/41.3 MB 23.4 MB/s eta 0:00:02\n",
      "     ------ --------------------------------- 6.6/41.3 MB 23.5 MB/s eta 0:00:02\n",
      "     ------- -------------------------------- 7.7/41.3 MB 23.5 MB/s eta 0:00:02\n",
      "     -------- ------------------------------- 9.2/41.3 MB 23.5 MB/s eta 0:00:02\n",
      "     --------- ----------------------------- 10.5/41.3 MB 24.2 MB/s eta 0:00:02\n",
      "     ----------- --------------------------- 12.1/41.3 MB 25.2 MB/s eta 0:00:02\n",
      "     ------------ -------------------------- 12.9/41.3 MB 25.2 MB/s eta 0:00:02\n",
      "     ------------- ------------------------- 14.0/41.3 MB 25.2 MB/s eta 0:00:02\n",
      "     -------------- ------------------------ 15.2/41.3 MB 25.2 MB/s eta 0:00:02\n",
      "     --------------- ----------------------- 16.3/41.3 MB 24.2 MB/s eta 0:00:02\n",
      "     ---------------- ---------------------- 17.7/41.3 MB 26.2 MB/s eta 0:00:01\n",
      "     ----------------- --------------------- 18.3/41.3 MB 25.2 MB/s eta 0:00:01\n",
      "     ------------------ -------------------- 19.5/41.3 MB 24.2 MB/s eta 0:00:01\n",
      "     ------------------- ------------------- 20.6/41.3 MB 23.4 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 21.5/41.3 MB 21.9 MB/s eta 0:00:01\n",
      "     -------------------- ------------------ 21.8/41.3 MB 21.1 MB/s eta 0:00:01\n",
      "     --------------------- ----------------- 22.5/41.3 MB 19.9 MB/s eta 0:00:01\n",
      "     ---------------------- ---------------- 23.5/41.3 MB 20.5 MB/s eta 0:00:01\n",
      "     ----------------------- --------------- 24.8/41.3 MB 20.5 MB/s eta 0:00:01\n",
      "     ------------------------ -------------- 26.0/41.3 MB 19.8 MB/s eta 0:00:01\n",
      "     ------------------------- ------------- 26.6/41.3 MB 19.2 MB/s eta 0:00:01\n",
      "     -------------------------- ------------ 27.7/41.3 MB 19.3 MB/s eta 0:00:01\n",
      "     --------------------------- ----------- 29.0/41.3 MB 20.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ---------- 30.1/41.3 MB 19.3 MB/s eta 0:00:01\n",
      "     ----------------------------- --------- 31.2/41.3 MB 19.8 MB/s eta 0:00:01\n",
      "     ------------------------------ -------- 32.3/41.3 MB 23.4 MB/s eta 0:00:01\n",
      "     ------------------------------- ------- 33.5/41.3 MB 22.6 MB/s eta 0:00:01\n",
      "     --------------------------------- ----- 35.0/41.3 MB 23.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ---- 36.5/41.3 MB 24.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- --- 37.8/41.3 MB 26.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- - 39.2/41.3 MB 26.2 MB/s eta 0:00:01\n",
      "     --------------------------------------  40.4/41.3 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.3/41.3 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------  41.3/41.3 MB 27.3 MB/s eta 0:00:01\n",
      "     --------------------------------------- 41.3/41.3 MB 20.4 MB/s eta 0:00:00\n",
      "Collecting threadpoolctl>=3.1.0\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: psutil in c:\\users\\jiahu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from accelerate) (7.0.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.12.7-cp310-cp310-win_amd64.whl (448 kB)\n",
      "     ---------------------------------------- 0.0/448.4 kB ? eta -:--:--\n",
      "     ------------------------------------- 448.4/448.4 kB 27.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jiahu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "     ---------------------------------------- 0.0/70.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 70.4/70.4 kB 4.0 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "     ---------------------------------------- 0.0/159.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 159.6/159.6 kB ? eta 0:00:00\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "     ---------------------------------------- 0.0/128.7 kB ? eta -:--:--\n",
      "     ---------------------------------------- 128.7/128.7 kB ? eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.2-cp310-cp310-win_amd64.whl (105 kB)\n",
      "     ---------------------------------------- 0.0/105.8 kB ? eta -:--:--\n",
      "     -------------------------------------- 105.8/105.8 kB 6.0 MB/s eta 0:00:00\n",
      "Collecting mpmath<1.4,>=1.1.0\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "     ------------------------------------- 536.2/536.2 kB 32.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\jiahu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.6.2-cp310-cp310-win_amd64.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.3 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.3/45.3 kB ? eta 0:00:00\n",
      "Collecting async-timeout<6.0,>=4.0\n",
      "  Downloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Collecting propcache>=0.2.0\n",
      "  Downloading propcache-0.3.1-cp310-cp310-win_amd64.whl (45 kB)\n",
      "     ---------------------------------------- 0.0/45.2 kB ? eta -:--:--\n",
      "     --------------------------- ------------ 30.7/45.2 kB ? eta -:--:--\n",
      "     --------------------------- ------------ 30.7/45.2 kB ? eta -:--:--\n",
      "     -------------------------------------- 45.2/45.2 kB 279.4 kB/s eta 0:00:00\n",
      "Collecting yarl<2.0,>=1.17.0\n",
      "  Downloading yarl-1.20.0-cp310-cp310-win_amd64.whl (92 kB)\n",
      "     ---------------------------------------- 0.0/92.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 92.9/92.9 kB ? eta 0:00:00\n",
      "Collecting aiohappyeyeballs>=2.5.0\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "     ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 63.8/63.8 kB ? eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.4.4-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Installing collected packages: pytz, mpmath, xxhash, urllib3, tzdata, tqdm, threadpoolctl, sympy, safetensors, regex, pyyaml, pyparsing, pyarrow, propcache, pillow, numpy, networkx, multidict, MarkupSafe, kiwisolver, joblib, idna, fsspec, frozenlist, fonttools, filelock, dill, cycler, charset-normalizer, certifi, attrs, async-timeout, aiohappyeyeballs, yarl, scipy, requests, pandas, multiprocess, jinja2, contourpy, aiosignal, torch, scikit-learn, matplotlib, huggingface-hub, aiohttp, tokenizers, seaborn, accelerate, transformers, datasets\n",
      "Successfully installed MarkupSafe-3.0.2 accelerate-1.7.0 aiohappyeyeballs-2.6.1 aiohttp-3.12.7 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 certifi-2025.4.26 charset-normalizer-3.4.2 contourpy-1.3.2 cycler-0.12.1 datasets-3.6.0 dill-0.3.8 filelock-3.18.0 fonttools-4.58.1 frozenlist-1.6.2 fsspec-2025.3.0 huggingface-hub-0.32.4 idna-3.10 jinja2-3.1.6 joblib-1.5.1 kiwisolver-1.4.8 matplotlib-3.10.3 mpmath-1.3.0 multidict-6.4.4 multiprocess-0.70.16 networkx-3.4.2 numpy-2.2.6 pandas-2.2.3 pillow-11.2.1 propcache-0.3.1 pyarrow-20.0.0 pyparsing-3.2.3 pytz-2025.2 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 scipy-1.15.3 seaborn-0.13.2 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.1 torch-2.7.0 tqdm-4.67.1 transformers-4.52.4 tzdata-2025.2 urllib3-2.4.0 xxhash-3.5.0 yarl-1.20.0\n",
      "Ejecuta uno de los comandos anteriores para instalar las dependencias.\n",
      "Descomenta la l√≠nea correspondiente a tu sistema y necesidades.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 25.1.1\n",
      "[notice] To update, run: C:\\Users\\jiahu\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Ejecuta este comando para instalar todas las dependencias necesarias\n",
    "# Puedes ejecutarlo directamente en una celda del notebook con el prefijo !\n",
    "\n",
    "!pip install -U transformers datasets torch numpy pandas matplotlib seaborn scikit-learn accelerate\n",
    "\n",
    "# Si prefieres ejecutarlo en tu terminal (sin el prefijo !), usa:\n",
    "# pip install -U transformers datasets torch numpy pandas matplotlib seaborn scikit-learn accelerate\n",
    "\n",
    "# Si est√°s utilizando una GPU y quieres instalar PyTorch con soporte para CUDA:\n",
    "# Para CUDA 11.8 (ajusta segons la teva versi√≥ de CUDA):\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Per a un entorn conda, pots fer servir:\n",
    "# conda install -c huggingface -c conda-forge transformers datasets pytorch numpy pandas matplotlib seaborn scikit-learn accelerate\n",
    "\n",
    "print(\"Ejecuta uno de los comandos anteriores para instalar las dependencias.\")\n",
    "print(\"Descomenta la l√≠nea correspondiente a tu sistema y necesidades.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b02577f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jiahu\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claves del dataset: dict_keys(['train', 'validation', 'test'])\n",
      "Tama√±o del conjunto de entrenamiento: 90700\n",
      "Tama√±o del conjunto de validaci√≥n: 5669\n",
      "Tama√±o del conjunto de prueba: 17007\n",
      "\n",
      "Columnas disponibles: ['sentence', 'label1', 'label2']\n",
      "\n",
      "Ejemplo de los datos:\n",
      "{'sentence': \"L'ACA reactiva el retorn del c√†non de l'aigua que s'envia a Tarragona i millorar√† l'efici√®ncia del canal de l'esquerra. L'obra corregir√† p√®rdues d'aigua a la s√®quia del Cementiri de Deltebre amb una inversi√≥ de 900.000 euros. Despr√©s d'alguns exercici de par√†lisi, la Comunitat de Regants de l'Esquerra de l'Ebre i l'ACA han signat, aquest dilluns, un nou conveni per reactivar obres de millora de l'efici√®ncia d'infraestructures de reg i evitar la p√®rdua d'aigua. Les actuacions comen√ßaran al febrer de l'any que ve en un tram de 4,9 quil√≤metres de la s√®quia del Cementiri, al terme municipal de Deltebre (Baix Ebre).Es col¬∑locaran plaques i llits de graves, l√†mines de geot√®xtil, i es revestiran m√©s de 3 quil√≤metres de la s√®quia. Els 900.000 euros provenen del 20% del c√†non de derivaci√≥ de l'aigua que paga Tarragona i que s'estipula que s'ha de revertir al territori.\", 'label1': 'Economia', 'label2': 'Agroalimentaci√≥'}\n",
      "\n",
      "N√∫mero de clases en label1: 4\n",
      "Clases disponibles en label1: {'Pol√≠tica', 'Societat', 'Economia', 'Cultura'}\n",
      "\n",
      "N√∫mero de clases en label2: 53\n",
      "Clases disponibles en label2: {'Lletres', 'Policial', 'Tr√†nsit', 'Turisme', 'Ind√∫stria', 'Comer√ß', 'Cooperaci√≥', 'Noves tecnologies', 'Innovaci√≥', 'Mem√≤ria hist√≤rica', 'Meteorologia', 'Govern espanyol', 'Festa i cultura popular', 'Partits', 'Infraestructures', 'Judicial', 'M√∫sica', 'Agroalimentaci√≥', 'Govern', 'Universitats', 'Mobilitat', 'Castells', 'Serveis Socials', 'Gastronomia', 'Exteriors', 'Immigraci√≥', 'Salut', 'Moda', 'Habitatge', 'Arts', 'Pol√≠tica municipal', 'Moviments socials', 'Educaci√≥', 'Recerca', 'Treball', 'Successos', 'Log√≠stica', 'Comptes p√∫blics', 'Energia', 'Hisenda', 'Religi√≥', 'Uni√≥ Europea', 'Equipaments i patrimoni', 'Teatre', 'Entitats', 'Parlament', 'Finances', 'Llengua', 'Esports', 'Urbanisme', 'Cinema', 'Empresa', 'Medi ambient'}\n",
      "\n",
      "Distribuci√≥n de label1:\n",
      "Societat: 36906 (40.69%)\n",
      "Pol√≠tica: 25568 (28.19%)\n",
      "Economia: 16305 (17.98%)\n",
      "Cultura: 11921 (13.14%)\n",
      "\n",
      "Distribuci√≥n de label2:\n",
      "Partits: 10055 (11.09%)\n",
      "Successos: 7874 (8.68%)\n",
      "Govern: 6506 (7.17%)\n",
      "Judicial: 5789 (6.38%)\n",
      "Policial: 5557 (6.13%)\n",
      "Salut: 5430 (5.99%)\n",
      "Parlament: 4177 (4.61%)\n",
      "Agroalimentaci√≥: 3236 (3.57%)\n",
      "Medi ambient: 3028 (3.34%)\n",
      "M√∫sica: 2872 (3.17%)\n",
      "Educaci√≥: 2757 (3.04%)\n",
      "Empresa: 2699 (2.98%)\n",
      "Pol√≠tica municipal: 2094 (2.31%)\n",
      "Arts: 1989 (2.19%)\n",
      "Infraestructures: 1741 (1.92%)\n",
      "Serveis Socials: 1683 (1.86%)\n",
      "Treball: 1657 (1.83%)\n",
      "Mobilitat: 1625 (1.79%)\n",
      "Cinema: 1560 (1.72%)\n",
      "Teatre: 1493 (1.65%)\n",
      "Turisme: 1232 (1.36%)\n",
      "Equipaments i patrimoni: 1230 (1.36%)\n",
      "Uni√≥ Europea: 1186 (1.31%)\n",
      "Lletres: 1181 (1.30%)\n",
      "Meteorologia: 1080 (1.19%)\n",
      "Govern espanyol: 985 (1.09%)\n",
      "Comer√ß: 984 (1.08%)\n",
      "Festa i cultura popular: 890 (0.98%)\n",
      "Tr√†nsit: 865 (0.95%)\n",
      "Finances: 774 (0.85%)\n",
      "Universitats: 749 (0.83%)\n",
      "Habitatge: 722 (0.80%)\n",
      "Esports: 498 (0.55%)\n",
      "Recerca: 470 (0.52%)\n",
      "Entitats: 432 (0.48%)\n",
      "Noves tecnologies: 428 (0.47%)\n",
      "Energia: 416 (0.46%)\n",
      "Religi√≥: 314 (0.35%)\n",
      "Llengua: 286 (0.32%)\n",
      "Moviments socials: 282 (0.31%)\n",
      "Cooperaci√≥: 250 (0.28%)\n",
      "Ind√∫stria: 233 (0.26%)\n",
      "Castells: 231 (0.25%)\n",
      "Comptes p√∫blics: 206 (0.23%)\n",
      "Immigraci√≥: 189 (0.21%)\n",
      "Gastronomia: 189 (0.21%)\n",
      "Exteriors: 133 (0.15%)\n",
      "Innovaci√≥: 114 (0.13%)\n",
      "Mem√≤ria hist√≤rica: 91 (0.10%)\n",
      "Urbanisme: 86 (0.09%)\n",
      "Log√≠stica: 80 (0.09%)\n",
      "Moda: 53 (0.06%)\n",
      "Hisenda: 19 (0.02%)\n",
      "\n",
      "N√∫mero de clases en label1: 4\n",
      "Clases disponibles en label1: {'Pol√≠tica', 'Societat', 'Economia', 'Cultura'}\n",
      "\n",
      "N√∫mero de clases en label2: 53\n",
      "Clases disponibles en label2: {'Lletres', 'Policial', 'Tr√†nsit', 'Turisme', 'Ind√∫stria', 'Comer√ß', 'Cooperaci√≥', 'Noves tecnologies', 'Innovaci√≥', 'Mem√≤ria hist√≤rica', 'Meteorologia', 'Govern espanyol', 'Festa i cultura popular', 'Partits', 'Infraestructures', 'Judicial', 'M√∫sica', 'Agroalimentaci√≥', 'Govern', 'Universitats', 'Mobilitat', 'Castells', 'Serveis Socials', 'Gastronomia', 'Exteriors', 'Immigraci√≥', 'Salut', 'Moda', 'Habitatge', 'Arts', 'Pol√≠tica municipal', 'Moviments socials', 'Educaci√≥', 'Recerca', 'Treball', 'Successos', 'Log√≠stica', 'Comptes p√∫blics', 'Energia', 'Hisenda', 'Religi√≥', 'Uni√≥ Europea', 'Equipaments i patrimoni', 'Teatre', 'Entitats', 'Parlament', 'Finances', 'Llengua', 'Esports', 'Urbanisme', 'Cinema', 'Empresa', 'Medi ambient'}\n",
      "\n",
      "Distribuci√≥n de label1:\n",
      "Societat: 36906 (40.69%)\n",
      "Pol√≠tica: 25568 (28.19%)\n",
      "Economia: 16305 (17.98%)\n",
      "Cultura: 11921 (13.14%)\n",
      "\n",
      "Distribuci√≥n de label2:\n",
      "Partits: 10055 (11.09%)\n",
      "Successos: 7874 (8.68%)\n",
      "Govern: 6506 (7.17%)\n",
      "Judicial: 5789 (6.38%)\n",
      "Policial: 5557 (6.13%)\n",
      "Salut: 5430 (5.99%)\n",
      "Parlament: 4177 (4.61%)\n",
      "Agroalimentaci√≥: 3236 (3.57%)\n",
      "Medi ambient: 3028 (3.34%)\n",
      "M√∫sica: 2872 (3.17%)\n",
      "Educaci√≥: 2757 (3.04%)\n",
      "Empresa: 2699 (2.98%)\n",
      "Pol√≠tica municipal: 2094 (2.31%)\n",
      "Arts: 1989 (2.19%)\n",
      "Infraestructures: 1741 (1.92%)\n",
      "Serveis Socials: 1683 (1.86%)\n",
      "Treball: 1657 (1.83%)\n",
      "Mobilitat: 1625 (1.79%)\n",
      "Cinema: 1560 (1.72%)\n",
      "Teatre: 1493 (1.65%)\n",
      "Turisme: 1232 (1.36%)\n",
      "Equipaments i patrimoni: 1230 (1.36%)\n",
      "Uni√≥ Europea: 1186 (1.31%)\n",
      "Lletres: 1181 (1.30%)\n",
      "Meteorologia: 1080 (1.19%)\n",
      "Govern espanyol: 985 (1.09%)\n",
      "Comer√ß: 984 (1.08%)\n",
      "Festa i cultura popular: 890 (0.98%)\n",
      "Tr√†nsit: 865 (0.95%)\n",
      "Finances: 774 (0.85%)\n",
      "Universitats: 749 (0.83%)\n",
      "Habitatge: 722 (0.80%)\n",
      "Esports: 498 (0.55%)\n",
      "Recerca: 470 (0.52%)\n",
      "Entitats: 432 (0.48%)\n",
      "Noves tecnologies: 428 (0.47%)\n",
      "Energia: 416 (0.46%)\n",
      "Religi√≥: 314 (0.35%)\n",
      "Llengua: 286 (0.32%)\n",
      "Moviments socials: 282 (0.31%)\n",
      "Cooperaci√≥: 250 (0.28%)\n",
      "Ind√∫stria: 233 (0.26%)\n",
      "Castells: 231 (0.25%)\n",
      "Comptes p√∫blics: 206 (0.23%)\n",
      "Immigraci√≥: 189 (0.21%)\n",
      "Gastronomia: 189 (0.21%)\n",
      "Exteriors: 133 (0.15%)\n",
      "Innovaci√≥: 114 (0.13%)\n",
      "Mem√≤ria hist√≤rica: 91 (0.10%)\n",
      "Urbanisme: 86 (0.09%)\n",
      "Log√≠stica: 80 (0.09%)\n",
      "Moda: 53 (0.06%)\n",
      "Hisenda: 19 (0.02%)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el dataset de projecte-aina/tecla desde Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Cargar el dataset TECLA\n",
    "tecla_dataset = load_dataset(\"projecte-aina/tecla\")\n",
    "\n",
    "# Mostrar informaci√≥n sobre el dataset\n",
    "print(f\"Claves del dataset: {tecla_dataset.keys()}\")\n",
    "print(f\"Tama√±o del conjunto de entrenamiento: {len(tecla_dataset['train'])}\")\n",
    "print(f\"Tama√±o del conjunto de validaci√≥n: {len(tecla_dataset['validation'])}\")\n",
    "print(f\"Tama√±o del conjunto de prueba: {len(tecla_dataset['test'])}\")\n",
    "\n",
    "# Mostrar las columnas disponibles en el dataset\n",
    "print(f\"\\nColumnas disponibles: {tecla_dataset['train'].column_names}\")\n",
    "\n",
    "# Mostrar un ejemplo de los datos\n",
    "print(\"\\nEjemplo de los datos:\")\n",
    "print(tecla_dataset['train'][0])\n",
    "\n",
    "# Explorar las clases/etiquetas disponibles para label1 y label2\n",
    "label1_values = set(tecla_dataset['train']['label1'])\n",
    "label2_values = set(tecla_dataset['train']['label2'])\n",
    "\n",
    "print(f\"\\nN√∫mero de clases en label1: {len(label1_values)}\")\n",
    "print(f\"Clases disponibles en label1: {label1_values}\")\n",
    "\n",
    "print(f\"\\nN√∫mero de clases en label2: {len(label2_values)}\")\n",
    "print(f\"Clases disponibles en label2: {label2_values}\")\n",
    "\n",
    "# Ver la distribuci√≥n de las etiquetas\n",
    "print(\"\\nDistribuci√≥n de label1:\")\n",
    "label1_counts = {}\n",
    "for label in tecla_dataset['train']['label1']:\n",
    "    label1_counts[label] = label1_counts.get(label, 0) + 1\n",
    "for label, count in sorted(label1_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{label}: {count} ({count/len(tecla_dataset['train'])*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nDistribuci√≥n de label2:\")\n",
    "label2_counts = {}\n",
    "for label in tecla_dataset['train']['label2']:\n",
    "    label2_counts[label] = label2_counts.get(label, 0) + 1\n",
    "for label, count in sorted(label2_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{label}: {count} ({count/len(tecla_dataset['train'])*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db16e9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Using model: projecte-aina/roberta-base-ca-v2-cased-sts\n",
      "Classification target: label1\n"
     ]
    }
   ],
   "source": [
    "# Importar las bibliotecas esenciales\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import torch\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "# Verificar si CUDA est√° disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Definir el modelo preentrenado para catal√°n\n",
    "model_name = \"projecte-aina/roberta-base-ca-v2-cased-sts\"\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "# Nos centraremos en label1 para esta tarea de clasificaci√≥n\n",
    "# Puedes cambiar a label2 si lo necesitas\n",
    "target_label = \"label1\"\n",
    "print(f\"Classification target: {target_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c176b3",
   "metadata": {},
   "source": [
    "## Enfoque simplificado con Pipeline\n",
    "\n",
    "En lugar de realizar un proceso completo de entrenamiento manual, podemos utilizar directamente la pipeline de Hugging Face para clasificaci√≥n de texto, lo que simplifica enormemente el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36f99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando modelo para clasificar en 4 categor√≠as de label1:\n",
      "  0: Cultura\n",
      "  1: Economia\n",
      "  2: Pol√≠tica\n",
      "  3: Societat\n",
      "Cargando el modelo base y ajustando para clasificaci√≥n...\n",
      "Cargando el modelo base y ajustando para clasificaci√≥n...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at projecte-aina/roberta-base-ca-v2-cased-sts and are newly initialized because the shapes did not match:\n",
      "- classifier.out_proj.weight: found shape torch.Size([1, 768]) in the checkpoint and torch.Size([4, 768]) in the model instantiated\n",
      "- classifier.out_proj.bias: found shape torch.Size([1]) in the checkpoint and torch.Size([4]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El modelo se ha cargado con 4 etiquetas de salida\n",
      "Error durante el entrenamiento o clasificaci√≥n: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n"
     ]
    }
   ],
   "source": [
    "# Enfoque para clasificaci√≥n de texto usando las categor√≠as espec√≠ficas de TECLA\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, AutoTokenizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# Desactivar TensorFlow para evitar errores\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "try:\n",
    "    # Preparar el dataset para entrenamiento\n",
    "    # Vamos a entrenar un modelo espec√≠fico para label1\n",
    "    label_column = target_label  # Podemos usar 'label1' o 'label2'\n",
    "    \n",
    "    # Obtener etiquetas √∫nicas del dataset\n",
    "    labels = sorted(list(set(tecla_dataset['train'][label_column])))\n",
    "    label2id = {label: i for i, label in enumerate(labels)}\n",
    "    id2label = {i: label for i, label in enumerate(labels)}\n",
    "    num_labels = len(labels)\n",
    "    \n",
    "    print(f\"Entrenando modelo para clasificar en {num_labels} categor√≠as de {label_column}:\")\n",
    "    for i, label in enumerate(labels):\n",
    "        print(f\"  {i}: {label}\")\n",
    "    \n",
    "    # Funci√≥n para tokenizar y preparar el dataset\n",
    "    def tokenize_and_prepare(examples):\n",
    "        # Tokenizar los textos\n",
    "        tokenized = tokenizer(\n",
    "            examples[\"sentence\"], \n",
    "            padding=\"max_length\", \n",
    "            truncation=True, \n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        # A√±adir las etiquetas num√©ricas\n",
    "        tokenized[\"labels\"] = [label2id[label] for label in examples[label_column]]\n",
    "        return tokenized\n",
    "    \n",
    "    # Cargar y preparar los datos\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Aplicar la tokenizaci√≥n a los datasets\n",
    "    tokenized_train = tecla_dataset[\"train\"].map(tokenize_and_prepare, batched=True)\n",
    "    tokenized_val = tecla_dataset[\"validation\"].map(tokenize_and_prepare, batched=True)\n",
    "    \n",
    "    # Funci√≥n para calcular m√©tricas\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        correct = predictions == labels\n",
    "        accuracy = correct.mean()\n",
    "        return {\"accuracy\": accuracy}\n",
    "    \n",
    "    # Cargar modelo pre-entrenado y configurarlo para nuestra tarea\n",
    "    print(\"Cargando el modelo base y ajustando para clasificaci√≥n...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=num_labels,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True  # A√±adido para resolver el error de tama√±o\n",
    "    )\n",
    "    \n",
    "    print(f\"El modelo se ha cargado con {num_labels} etiquetas de salida\")\n",
    "    \n",
    "    # Configurar el entrenamiento con par√°metros compatibles con todas las versiones\n",
    "    # Eliminado 'evaluation_strategy' y adaptado para versiones m√°s antiguas\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        save_steps=500,\n",
    "        # Verificamos si eval_steps est√° disponible en la versi√≥n actual\n",
    "        # Si la siguiente l√≠nea da error, simplemente la versi√≥n es m√°s antigua\n",
    "        eval_steps=500 if hasattr(TrainingArguments, \"eval_steps\") else None,\n",
    "        # Usamos estos par√°metros que deber√≠an funcionar en todas las versiones\n",
    "        do_eval=True,\n",
    "        do_train=True,\n",
    "        save_strategy=\"steps\" if hasattr(TrainingArguments, \"save_strategy\") else None,\n",
    "        evaluation_strategy=\"steps\" if hasattr(TrainingArguments, \"evaluation_strategy\") else None\n",
    "    )\n",
    "    \n",
    "    # Adaptaci√≥n para versiones m√°s antiguas de transformers\n",
    "    try:\n",
    "        # Crear el Trainer\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        # Entrenar el modelo\n",
    "        print(\"Iniciando entrenamiento del modelo...\")\n",
    "        trainer.train()\n",
    "        print(\"Entrenamiento completado!\")\n",
    "        \n",
    "        # Evaluar el modelo\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Resultados de evaluaci√≥n: {eval_results}\")\n",
    "    except TypeError as e:\n",
    "        # Si hay un error por incompatibilidad de versiones, imprimimos informaci√≥n √∫til\n",
    "        print(f\"Error en la configuraci√≥n del Trainer: {e}\")\n",
    "        print(\"Intentando con una configuraci√≥n m√°s simple...\")\n",
    "        \n",
    "        # Versi√≥n simplificada para versiones m√°s antiguas\n",
    "        simple_args = TrainingArguments(\n",
    "            output_dir=\"./results\",\n",
    "            num_train_epochs=3,\n",
    "            per_device_train_batch_size=8,\n",
    "            per_device_eval_batch_size=8,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=\"./logs\"\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=simple_args,\n",
    "            train_dataset=tokenized_train,\n",
    "            eval_dataset=tokenized_val,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "        \n",
    "        print(\"Iniciando entrenamiento con configuraci√≥n simplificada...\")\n",
    "        trainer.train()\n",
    "        print(\"Entrenamiento completado!\")\n",
    "        \n",
    "        # Evaluar el modelo\n",
    "        eval_results = trainer.evaluate()\n",
    "        print(f\"Resultados de evaluaci√≥n: {eval_results}\")\n",
    "    \n",
    "    # Guardar el modelo entrenado\n",
    "    model_save_path = f\"./tecla_{label_column}_classifier\"\n",
    "    model.save_pretrained(model_save_path)\n",
    "    tokenizer.save_pretrained(model_save_path)\n",
    "    print(f\"Modelo guardado en {model_save_path}\")\n",
    "    \n",
    "    # Usar el modelo guardado para clasificaci√≥n\n",
    "    print(\"Creando pipeline con el modelo entrenado...\")\n",
    "    \n",
    "    # Crear una pipeline con el modelo entrenado (usar el modelo guardado)\n",
    "    classifier = pipeline(\n",
    "        \"text-classification\", \n",
    "        model=model,  # Usar el modelo en memoria es m√°s seguro\n",
    "        tokenizer=tokenizer,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    # Ejemplos para probar la clasificaci√≥n\n",
    "    examples = [\n",
    "        \"El govern ha aprovat avui un nou decret llei per regular els preus del lloguer a les grans ciutats.\",\n",
    "        \"La nova exposici√≥ al MNAC presenta m√©s de 100 obres in√®dites del modernisme catal√†.\",\n",
    "        \"L'empresa tecnol√≤gica ha anunciat la creaci√≥ de 200 nous llocs de treball a Barcelona.\",\n",
    "        \"Els estudiants han sortit al carrer per protestar contra les retallades en educaci√≥.\"\n",
    "    ]\n",
    "    \n",
    "    # Clasificar los ejemplos\n",
    "    print(f\"\\nClasificaci√≥n de ejemplos en categor√≠as de {label_column}:\")\n",
    "    for text in examples:\n",
    "        # Asegurarse de manejar correctamente los resultados\n",
    "        try:\n",
    "            result = classifier(text)[0]\n",
    "            # Extraer el n√∫mero de la etiqueta (puede variar seg√∫n la versi√≥n)\n",
    "            label_id = int(result['label'].split('_')[-1]) if '_' in result['label'] else int(result['label'])\n",
    "            predicted_label = id2label[label_id]\n",
    "            print(f\"\\nTexto: {text[:100]}...\")\n",
    "            print(f\"Categor√≠a predicha: {predicted_label}, Confianza: {result['score']:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al clasificar: {e}\")\n",
    "            # Intentar una clasificaci√≥n m√°s directa\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                label_id = torch.argmax(predictions, dim=-1).item()\n",
    "            \n",
    "            predicted_label = id2label[label_id]\n",
    "            confidence = predictions[0, label_id].item()\n",
    "            print(f\"\\nTexto: {text[:100]}...\")\n",
    "            print(f\"Categor√≠a predicha (m√©todo alternativo): {predicted_label}, Confianza: {confidence:.4f}\")\n",
    "    \n",
    "    # Clasificar algunos ejemplos del conjunto de prueba\n",
    "    print(f\"\\nClasificaci√≥n de ejemplos del conjunto de prueba en categor√≠as de {label_column}:\")\n",
    "    for i in range(5):  # Mostrar 5 ejemplos\n",
    "        text = tecla_dataset[\"test\"][i][\"sentence\"]\n",
    "        true_label = tecla_dataset[\"test\"][i][label_column]\n",
    "        \n",
    "        # Usar el enfoque directo con el modelo para evitar problemas con la pipeline\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            label_id = torch.argmax(predictions, dim=-1).item()\n",
    "        \n",
    "        predicted_label = id2label[label_id]\n",
    "        confidence = predictions[0, label_id].item()\n",
    "        \n",
    "        print(f\"\\nEjemplo {i+1}:\")\n",
    "        print(f\"Texto: {text[:100]}...\")\n",
    "        print(f\"Categor√≠a real: {true_label}\")\n",
    "        print(f\"Categor√≠a predicha: {predicted_label}, Confianza: {confidence:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82054ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUACI√ìN COMPLETA DEL CONJUNTO DE VALIDACI√ìN\n",
      "================================================================================\n",
      "Procesando 5669 ejemplos del conjunto de validaci√≥n en lotes de 32...\n",
      "Procesado hasta el ejemplo 32/5669\n",
      "Procesado hasta el ejemplo 32/5669\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Realizar las predicciones\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     32\u001b[0m     logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     33\u001b[0m     batch_predictions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1202\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;124;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1200\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1202\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1208\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1212\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1213\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1214\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:869\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[0;32m    864\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[0;32m    867\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m--> 869\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    882\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:618\u001b[0m, in \u001b[0;36mRobertaEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    607\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[0;32m    608\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[0;32m    609\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    615\u001b[0m         output_attentions,\n\u001b[0;32m    616\u001b[0m     )\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 618\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    628\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:507\u001b[0m, in \u001b[0;36mRobertaLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    496\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    497\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    505\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    506\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 507\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    508\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    509\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:434\u001b[0m, in \u001b[0;36mRobertaAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    425\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    426\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    433\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 434\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    442\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    444\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:325\u001b[0m, in \u001b[0;36mRobertaSdpaSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    323\u001b[0m     key_layer, value_layer \u001b[38;5;241m=\u001b[39m past_key_value\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     key_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    326\u001b[0m     value_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranspose_for_scores(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(current_states))\n\u001b[0;32m    327\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "    # --- NUEVA SECCI√ìN: EVALUACI√ìN COMPLETA DEL CONJUNTO DE VALIDACI√ìN ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EVALUACI√ìN COMPLETA DEL CONJUNTO DE VALIDACI√ìN\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Predecir para todo el conjunto de validaci√≥n\n",
    "    val_texts = tecla_dataset[\"validation\"][\"sentence\"]\n",
    "    val_true_labels = tecla_dataset[\"validation\"][label_column]\n",
    "    \n",
    "    # Convertir las etiquetas reales a IDs num√©ricos\n",
    "    val_true_label_ids = [label2id[label] for label in val_true_labels]\n",
    "    \n",
    "    # Inicializar listas para almacenar predicciones y etiquetas reales\n",
    "    val_predictions = []\n",
    "    \n",
    "    # Procesar el conjunto de validaci√≥n en lotes para mayor eficiencia\n",
    "    batch_size = 32\n",
    "    print(f\"Procesando {len(val_texts)} ejemplos del conjunto de validaci√≥n en lotes de {batch_size}...\")\n",
    "    \n",
    "    for i in range(0, len(val_texts), batch_size):\n",
    "        batch_texts = val_texts[i:i+batch_size]\n",
    "        \n",
    "        # Tokenizar los textos\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        # Mover a la GPU si est√° disponible\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Realizar las predicciones\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        # A√±adir las predicciones a la lista\n",
    "        val_predictions.extend(batch_predictions)\n",
    "        \n",
    "        # Mostrar progreso\n",
    "        if (i // batch_size) % 10 == 0:\n",
    "            print(f\"Procesado hasta el ejemplo {i+len(batch_texts)}/{len(val_texts)}\")\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "    \n",
    "    accuracy = accuracy_score(val_true_label_ids, val_predictions)\n",
    "    print(f\"\\nExactitud (Accuracy) en el conjunto de validaci√≥n: {accuracy:.4f}\")\n",
    "    \n",
    "    # Generar informe de clasificaci√≥n\n",
    "    print(\"\\nInforme de clasificaci√≥n:\")\n",
    "    print(classification_report(val_true_label_ids, val_predictions, target_names=labels))\n",
    "    \n",
    "    # Crear y mostrar matriz de confusi√≥n\n",
    "    conf_matrix = confusion_matrix(val_true_label_ids, val_predictions)\n",
    "    \n",
    "    # Visualizar la matriz de confusi√≥n\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicci√≥n')\n",
    "    plt.ylabel('Etiqueta Real')\n",
    "    plt.title(f'Matriz de Confusi√≥n - Conjunto de Validaci√≥n ({label_column})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analizar errores\n",
    "    print(\"\\nAn√°lisis de ejemplos mal clasificados:\")\n",
    "    \n",
    "    # Encontrar ejemplos mal clasificados\n",
    "    misclassified_indices = [i for i, (true, pred) in enumerate(zip(val_true_label_ids, val_predictions)) if true != pred]\n",
    "    \n",
    "    # Mostrar algunos ejemplos mal clasificados\n",
    "    num_examples = min(5, len(misclassified_indices))\n",
    "    \n",
    "    if num_examples > 0:\n",
    "        for i in range(num_examples):\n",
    "            idx = misclassified_indices[i]\n",
    "            text = val_texts[idx]\n",
    "            true_label = val_true_labels[idx]\n",
    "            pred_label = id2label[val_predictions[idx]]\n",
    "            \n",
    "            print(f\"\\nEjemplo mal clasificado {i+1}:\")\n",
    "            print(f\"Texto: {text[:150]}...\")\n",
    "            print(f\"Etiqueta real: {true_label}\")\n",
    "            print(f\"Etiqueta predicha: {pred_label}\")\n",
    "            print(\"-\"*50)\n",
    "    else:\n",
    "        print(\"¬°No se encontraron ejemplos mal clasificados!\")\n",
    "    \n",
    "    # Guardar m√©tricas y resultados\n",
    "    results = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"confusion_matrix\": conf_matrix.tolist(),\n",
    "        \"num_examples\": len(val_texts),\n",
    "        \"num_misclassified\": len(misclassified_indices)\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open(f\"validation_metrics_{label_column}.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)    \n",
    "    print(f\"\\nLos resultados se han guardado en 'validation_metrics_{label_column}.json'\")\n",
    "    \n",
    "'''except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento o clasificaci√≥n: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9375922d",
   "metadata": {},
   "source": [
    "## Resultados de Evaluaci√≥n\n",
    "\n",
    "Hemos evaluado nuestro modelo entrenado en el conjunto de validaci√≥n completo. Los resultados clave son:\n",
    "\n",
    "1. **Exactitud (Accuracy)**: Porcentaje de ejemplos correctamente clasificados\n",
    "2. **Informe de clasificaci√≥n**: M√©tricas de precisi√≥n, recall y F1-score para cada categor√≠a\n",
    "3. **Matriz de confusi√≥n**: Visualizaci√≥n de predicciones correctas vs incorrectas\n",
    "4. **An√°lisis de errores**: Ejemplos de textos mal clasificados para entender las limitaciones del modelo\n",
    "\n",
    "Estas m√©tricas nos permiten evaluar la calidad y fiabilidad del modelo para la tarea de clasificaci√≥n en catal√°n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b4d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n adicional de resultados\n",
    "\n",
    "# Cargar los resultados si existen\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "result_file = f\"validation_metrics_{target_label}.json\"\n",
    "\n",
    "if os.path.exists(result_file):\n",
    "    with open(result_file, \"r\") as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    # Mostrar un resumen visual de los resultados\n",
    "    print(f\"Resumen de evaluaci√≥n para clasificaci√≥n de {target_label}:\")\n",
    "    print(f\"Exactitud (Accuracy): {results['accuracy']:.4f}\")\n",
    "    print(f\"Total de ejemplos evaluados: {results['num_examples']}\")\n",
    "    print(f\"Ejemplos mal clasificados: {results['num_misclassified']}\")\n",
    "    \n",
    "    # Crear un gr√°fico de barras para la exactitud por clase\n",
    "    # Esto requerir√≠a datos adicionales, pero podemos hacer una visualizaci√≥n simple\n",
    "    if 'confusion_matrix' in results:\n",
    "        conf_matrix = np.array(results['confusion_matrix'])\n",
    "        class_accuracy = np.diag(conf_matrix) / conf_matrix.sum(axis=1)\n",
    "        \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.bar(labels, class_accuracy, color='skyblue')\n",
    "        plt.axhline(y=results['accuracy'], color='r', linestyle='-', label=f'Exactitud global: {results[\"accuracy\"]:.4f}')\n",
    "        plt.xlabel('Categor√≠a')\n",
    "        plt.ylabel('Exactitud')\n",
    "        plt.title(f'Exactitud por Categor√≠a - {target_label}')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.ylim(0, 1.0)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"No se encontr√≥ el archivo de resultados: {result_file}\")\n",
    "    print(\"Ejecuta primero la celda de evaluaci√≥n del conjunto de validaci√≥n.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

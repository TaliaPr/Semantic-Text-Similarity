{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Semantic Text Similarity\n",
    "\n",
    "En este fichero se va a comparar el rendimiento diferentes modelos simples o de regresión de cara a la tarea de clasificación de similitud semántica entre frases en catalán. <sb>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:55:07.143009Z",
     "start_time": "2025-05-23T14:55:05.828592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requisitos\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np\n",
    "import spacy\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from scipy import spatial\n",
    "from transformers import pipeline, AutoTokenizer, AutoModel\n",
    "from scipy.special import logit\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Tipado\n",
    "from typing import Tuple, List, Iterable, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir la base de datos y preprocesar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:55:39.846622Z",
     "start_time": "2025-05-23T14:55:39.839815Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stopwords en Catalan\n",
    "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }\n",
    "# Definir función de pre-procesado\n",
    "def preprocess(sentence: str) -> List[str]:\n",
    "    preprocessed = simple_preprocess(sentence) # Tokenización y normalización, lematización, minúsculas\n",
    "    # Eliminar stopwords\n",
    "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'sentence_1', 'sentence_2', 'label'],\n",
       "    num_rows: 3073\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Text Similarity (STS) dataset (principal per la Pràctica 4)\n",
    "train = load_dataset(\"projecte-aina/sts-ca\", split=\"train\")\n",
    "test = load_dataset(\"projecte-aina/sts-ca\", split=\"test\")\n",
    "val = load_dataset(\"projecte-aina/sts-ca\", split=\"validation\")\n",
    "all_data = load_dataset(\"projecte-aina/sts-ca\", split=\"all\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_corpus(corpus: Any) -> List[Tuple[List[str], List[str], float]]:\n",
    "    # Preprocesa las frases de cada elemento del corpus usando simple_preprocess (tokenización y normalización)\n",
    "    sentences_1_preproc = [simple_preprocess(d[\"sentence_1\"]) for d in corpus]  # Lista de listas: cada oración 1 preprocesada\n",
    "    sentences_2_preproc = [simple_preprocess(d[\"sentence_2\"]) for d in corpus]  # Lista de listas: cada oración 2 preprocesada\n",
    "    scores = [d[\"label\"] for d in corpus]  # Lista de puntuaciones de similitud\n",
    "    # Une las oraciones preprocesadas y su puntuación en una lista de tuplas\n",
    "    sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc, scores))\n",
    "    return sentence_pairs\n",
    "\n",
    "# Aplica la función a los conjuntos de entrenamiento, test y validación\n",
    "train_preproc = map_corpus(train)\n",
    "test_preproc = map_corpus(test)\n",
    "val_preproc = map_corpus(val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding **gensim** pre-entrenado: <sb>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastTextKeyedVectors\n",
    "#cargar como map:\n",
    "wv_model = FastTextKeyedVectors.load('/home/taya/Desktop/cc.ca.gensim.bin', mmap='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos modelos de dimensión reducida, mediante el truncamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_50d = {\n",
    "    word: wv_model[word][:50]\n",
    "    for word in wv_model.index_to_key\n",
    "}\n",
    "\n",
    "wv_model_100d = {\n",
    "    word: wv_model[word][:100]\n",
    "    for word in wv_model.index_to_key\n",
    "}\n",
    "wv_model_150d = {\n",
    "    word: wv_model[word][:150]\n",
    "    for word in wv_model.index_to_key\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construcción del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:25.881487Z",
     "start_time": "2025-05-23T14:58:25.584735Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Preprocesamiento de las oraciones y creación del diccionario\n",
    "sentences_1_preproc = [simple_preprocess(d[\"sentence_1\"]) for d in all_data] #lista de listas que son oraciones lematizadas\n",
    "sentences_2_preproc = [simple_preprocess(d[\"sentence_2\"]) for d in all_data]\n",
    "scores = [d[\"label\"] for d in all_data]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc, scores))#lista de tuplas que son ([palabras or1], [pal or 2], score)\n",
    "# Versión aplanada para poder entrenar el modelo\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc #todas las oraciones juntas\n",
    "diccionario = Dictionary(sentences_pairs_flattened) # diccionario donde cada palabra tiene un indice unico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construcción de la matriz TF-IDF: <sb>\n",
    "\n",
    "Sirve para dar más peso a las palabras importantes y menos peso a las comunes. <sb>\n",
    "- TF (Term Frequency): Qué tan frecuente es una palabra en UN documento.\n",
    "- IDF (Inverse Document Frequency): Qué tan rara es esa palabra en TODA la colección.<sb>\n",
    "\n",
    "Así, el documnto se puede caracterizar por las palabras que contiene y diferenciar de otros por las palabras no comunes que contiene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:28.003478Z",
     "start_time": "2025-05-23T14:58:27.934262Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cálculo de los pesos TF-IDF para las oraciones pre-procesadas\n",
    "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
    "\"\"\"\n",
    "Por ejemplo, si sent es ['hola', 'mundo', 'hola'], el resultado de diccionario.doc2bow(sent) podría ser [(0, 2), (1, 1)], donde 0 es el índice de \"hola\" y 1 es el índice de \"mundo\", indicando que \"hola\" aparece 2 veces y \"mundo\" aparece 1 vez.\n",
    "corpus = El resultado es una lista de representaciones de bolsa de palabras, donde cada elemento corresponde a una oración en el conjunto de datos.\n",
    "\"\"\"\n",
    "modelo_tfidf = TfidfModel(corpus) #transformar el corpus en una representación que refleja la importancia de las palabras en cada documento en relación con el corpus completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aregación: <sb>\n",
    "\n",
    "En esta parte, para poder obtener el embeding de una frase entera se van a promediar los embedings de cada palabra por la que está compuesta la frase, ponderando o no con los pesos TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:59:25.966982Z",
     "start_time": "2025-05-23T14:59:25.958556Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel, model = wv_model) -> Tuple[List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    dada una oracion preprocesada, para cada palabra saca sus pesos TF-IDF y su vector en el embeding\n",
    "    \"\"\"\n",
    "    bow = dictionary.doc2bow(sentence_preproc)#cuenta la frecuencia de cada palabra en la oracion\n",
    "    tf_idf = tf_idf_model[bow] \n",
    "    vectors, weights = [], []\n",
    "    for word_index, weight in tf_idf:\n",
    "        word = dictionary.get(word_index)\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "            weights.append(weight)\n",
    "    return vectors, weights\n",
    "\n",
    "def map_pairs(wv_model2, sentence_pairs: List[Tuple[str, str, float]],dictionary: Dictionary = None, tf_idf_model: TfidfModel = None,) -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    \"\"\"\n",
    "    Mapea los tripletes de oraciones a listas de (x, y), (pares de vectores, score)\n",
    "    :param sentence_pairs: lista de tuplas que son ([palabras or1], [palabras or2], score)\n",
    "    :param dictionary: diccionario donde cada palabra tiene un indice unico\n",
    "    :param tf_idf_model: objeto TfidfModel que da los pesos de las palabras (se puede indexar con un bag of words)\n",
    "    :return: lista de ((vector1, vector2), similitud), donde vector1 y vector2 cambian en funcion de:\n",
    "        si tf_idf_model is not None:\n",
    "                para cada elemento de sentence_pairs devuelve el vector embeding promediado de manera ponderada por los pesos de la matriz TF-IDF de las palabras de las oraciones 1 y 2.\n",
    "        si tf_idf_model is not None\n",
    "            el promedio de los vectores de embeding de las palabras que componen cada una de las oraciones\n",
    "    \"\"\"\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        sentence_1_preproc = preprocess(sentence_1) if isinstance(sentence_1, str) else sentence_1 # se procesa el texto antes de aplicar map_pairs entonces sentence_1 es una lista de tokens y ya nose vuelve a preprocesar\n",
    "        sentence_2_preproc = preprocess(sentence_2) if isinstance(sentence_2, str) else sentence_2\n",
    "        # Si usamos TF-IDF\n",
    "        if tf_idf_model is not None:\n",
    "            # Cálculo del promedio ponderado por TF-IDF de los word embeddings\n",
    "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model,model =  wv_model2, )\n",
    "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, model = wv_model2 )\n",
    "            vector1 = np.average(vectors1, weights=weights1, axis=0, ) #Esta función calcula el promedio de un conjunto de valores. Si se proporciona un argumento weights, el promedio se calcula de manera ponderada, lo que significa que cada valor contribuye al promedio de acuerdo con su peso correspondiente.\n",
    "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
    "        else:\n",
    "            # Cálculo del promedio de los word embeddings\n",
    "            vectors1 = [wv_model2[word] for word in sentence_1_preproc if word in wv_model2]\n",
    "            vectors2 = [wv_model2[word] for word in sentence_2_preproc if word in wv_model2]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "        # Añadir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([-7.22131877e-03, -4.88683421e-03,  2.71017708e-02,  2.32332627e-02,\n",
       "         -9.60097482e-03, -3.16095435e-03,  2.90225599e-02, -1.75413820e-02,\n",
       "          2.93095319e-02, -1.60574403e-02, -6.04936805e-03,  1.49908545e-02,\n",
       "          1.00934507e-02,  1.84449753e-02,  2.16156266e-02,  2.13238810e-02,\n",
       "          8.69774586e-03,  6.13958934e-02,  2.18719114e-02,  1.01426407e-02,\n",
       "          1.16837708e-02,  3.24588305e-03, -1.32856906e-02,  5.77104877e-02,\n",
       "          1.54627187e-02,  2.13443526e-02, -3.82471218e-02,  6.23983637e-03,\n",
       "          7.31161386e-04,  8.99225741e-03, -4.87626204e-03,  1.08773269e-02,\n",
       "          1.30313145e-02, -3.86450925e-03,  7.23370200e-03, -1.75266524e-02,\n",
       "         -9.09778208e-03,  4.43412138e-02, -4.31998409e-04,  6.25044253e-04,\n",
       "         -1.13920750e-02, -1.82465011e-02, -8.11444328e-03, -8.18518457e-03,\n",
       "         -3.54176235e-03, -1.60820262e-01,  7.74505797e-03,  9.80261699e-03,\n",
       "          8.53058034e-03, -1.23878019e-02,  1.24134202e-02, -2.39070017e-03,\n",
       "         -3.17817092e-02, -6.78023514e-03, -1.43296539e-02,  2.57246362e-02,\n",
       "          2.62315431e-02,  1.36448970e-02, -9.16338087e-03,  1.39765626e-02,\n",
       "          4.86506819e-02, -3.05702791e-02, -1.73943639e-02, -2.01844855e-02,\n",
       "          3.91634927e-02,  1.06170025e-03, -2.25761531e-02, -2.53614495e-02,\n",
       "          1.06654209e-02,  3.99356146e-02,  1.90717986e-02,  5.49600371e-03,\n",
       "          2.08872278e-02,  1.34323488e-02,  1.14136353e-02, -2.26947543e-02,\n",
       "         -2.89445889e-02,  3.63149983e-03, -3.17429208e-02, -1.86948783e-02,\n",
       "          7.58524843e-03, -2.51519190e-02,  1.59076956e-02, -1.57653595e-02,\n",
       "         -1.32853988e-02,  1.84268937e-02,  1.35733292e-02, -1.35745752e-02,\n",
       "         -2.62841402e-02, -1.71081547e-03, -6.57123376e-02,  1.80123245e-02,\n",
       "          2.18237573e-02,  4.92439771e-03, -4.24249468e-03,  1.44695216e-02,\n",
       "          1.61562532e-02, -5.86160831e-03,  1.07315954e-02,  2.03850600e-02,\n",
       "         -9.68246967e-03, -9.98037382e-03,  6.31454679e-07, -2.24527261e-02,\n",
       "          5.15347347e-03, -1.18663279e-02, -4.56879624e-03,  4.84564375e-02,\n",
       "          5.10890407e-03, -1.10297034e-03, -5.12788351e-03, -1.27865401e-02,\n",
       "          1.69212758e-02, -2.37873653e-04,  7.97397964e-03, -1.12930849e-02,\n",
       "         -4.02350222e-04, -1.98382137e-02,  2.10519730e-03, -1.94884122e-03,\n",
       "         -1.41923877e-02,  1.53408167e-02, -3.14154498e-02,  5.16255789e-03,\n",
       "          6.93068941e-03,  1.27301166e-02,  2.95631792e-03, -1.99976450e-02,\n",
       "         -1.29251593e-02,  2.83612074e-02, -1.00102755e-02, -3.59831313e-03,\n",
       "          9.12615437e-03,  3.19957247e-02, -2.39536587e-03, -1.88541424e-03,\n",
       "          1.71505670e-02,  6.10816907e-02, -2.02739033e-02,  3.70995447e-04,\n",
       "         -1.79623004e-02, -9.18574379e-04,  2.46088662e-02, -5.23656944e-03,\n",
       "         -3.28713545e-02, -1.66103211e-02, -2.97392304e-02,  1.05556641e-02,\n",
       "          1.15840674e-02,  5.76075530e-02,  5.02314960e-02,  2.17064867e-02,\n",
       "          8.58374305e-03, -6.60827014e-02, -4.82426792e-03, -1.56691531e-02,\n",
       "          2.12149921e-02, -2.74767225e-02, -1.23504192e-02,  1.28560904e-02,\n",
       "          2.30588782e-02, -1.09581482e-02,  6.07751199e-03,  1.72556543e-02,\n",
       "         -1.97553865e-03,  1.22152828e-02,  1.04813740e-02,  1.40103719e-02,\n",
       "          5.37980764e-03, -5.76685295e-02,  1.36694746e-02, -2.36321303e-02,\n",
       "         -8.50384182e-03,  3.60939922e-02, -2.06882135e-03,  1.76951758e-02,\n",
       "          1.57728072e-02,  2.88058438e-02, -8.86882738e-03,  6.88743752e-02,\n",
       "          6.25128593e-03,  3.75693638e-03,  3.87385404e-05, -1.95502007e-02,\n",
       "         -5.08330865e-03, -6.56666336e-03, -4.60735302e-03, -1.69378997e-02,\n",
       "          2.03075495e-02, -4.75446082e-02, -2.89494102e-02, -7.66059492e-03,\n",
       "         -2.24331713e-03,  8.62236897e-02, -2.80728569e-02, -4.31060661e-03,\n",
       "          1.31037543e-02,  6.93664039e-03,  8.79472834e-03,  5.53005787e-02,\n",
       "         -3.09062304e-03, -1.42629341e-02, -2.72252156e-02,  1.37947659e-02,\n",
       "         -3.86245701e-03, -1.68961022e-03,  1.90346599e-02, -1.98370433e-02,\n",
       "          2.21129468e-04,  8.48248098e-03,  1.53173742e-02, -4.47935110e-02,\n",
       "          5.90692373e-02,  4.12743244e-03,  2.69058790e-02, -7.84230922e-03,\n",
       "         -1.00485311e-02,  3.48163964e-03, -7.07877696e-03, -8.69120567e-03,\n",
       "         -1.09471734e-02, -5.26084500e-03, -8.29154383e-03,  1.45321145e-02,\n",
       "          1.17931868e-02,  1.51625271e-03,  5.06379921e-05,  3.64201031e-02,\n",
       "         -5.98440757e-03,  4.41127321e-03, -2.08663335e-02, -1.70784310e-02,\n",
       "         -2.18255851e-02,  6.00788341e-03,  5.07507834e-03, -1.38918141e-02,\n",
       "         -6.91033048e-03,  1.14552128e-02,  6.34593222e-02, -1.32782786e-02,\n",
       "         -1.52115628e-02,  4.98272376e-02,  1.72577717e-02, -1.29923328e-02,\n",
       "         -1.60627377e-02,  4.10427215e-02, -2.45433983e-03,  5.57846760e-03,\n",
       "         -2.11911409e-02, -2.09040772e-03,  4.27414873e-02,  2.59594736e-02,\n",
       "         -2.21033701e-02, -9.22074085e-03,  6.56417001e-03, -6.15493147e-03,\n",
       "         -6.36646366e-03, -2.16802575e-02,  2.19316476e-02, -1.32581929e-03,\n",
       "         -8.58274602e-03,  1.45549130e-03, -1.33940025e-02, -4.33783476e-03,\n",
       "          9.55339827e-03, -2.29094632e-02, -3.11402989e-03, -1.97023859e-02,\n",
       "          2.13536615e-03, -1.45499117e-02,  2.32157588e-02, -2.51983588e-02,\n",
       "         -2.26461076e-02,  9.16354896e-03,  2.66498964e-02, -1.95218307e-02,\n",
       "         -5.17713434e-02, -2.78031737e-02, -2.49143180e-02,  5.64633386e-03,\n",
       "         -1.70476468e-02,  1.34696997e-02, -5.37980768e-03, -2.86228522e-02,\n",
       "          1.15971552e-02,  1.62302861e-02, -1.50279653e-02, -3.47484244e-02,\n",
       "         -2.68725549e-02, -6.37603429e-03,  1.05356869e-04, -1.25608841e-02,\n",
       "         -4.91036526e-03, -5.21338575e-04,  4.98603459e-02, -1.67891928e-03,\n",
       "         -2.89849270e-03, -2.52519604e-02,  1.43659082e-02, -1.06123175e-02]),\n",
       "  array([-6.83419957e-03, -1.18342274e-02,  2.81851265e-02,  1.56639266e-02,\n",
       "         -8.56577435e-04, -8.51332926e-04,  1.97378555e-02, -1.29571395e-02,\n",
       "          4.93545554e-02, -2.41754346e-02, -6.15790577e-03,  2.05138671e-02,\n",
       "          1.93670358e-02,  2.11652259e-02,  2.54499821e-02,  1.63368237e-02,\n",
       "          4.16049481e-03,  3.83112881e-02,  2.22273047e-02,  4.62837957e-03,\n",
       "          8.80235256e-03, -2.01674838e-03, -1.16806213e-02,  4.86109168e-02,\n",
       "          1.98380204e-02, -1.89877654e-03, -4.72341870e-02, -4.54726101e-03,\n",
       "         -7.67185251e-03, -5.59219061e-05, -9.20897783e-03,  7.59127691e-03,\n",
       "          7.42895688e-04, -6.14518295e-03,  1.84238488e-02, -2.73708592e-02,\n",
       "          3.67920039e-03,  5.15129104e-02,  4.80286734e-03, -9.51228594e-03,\n",
       "         -1.11621336e-02, -9.42123322e-03, -4.36669167e-03, -1.52258050e-02,\n",
       "          9.90805424e-04, -1.54695765e-01,  1.73456723e-02,  4.71676183e-03,\n",
       "          4.02265375e-03, -2.22887292e-02, -4.60747433e-03,  3.95980001e-03,\n",
       "         -4.49414413e-02, -1.66012033e-02, -1.11012129e-02,  2.86654471e-02,\n",
       "          2.77660979e-02,  2.29358544e-02, -9.27368574e-03,  1.76946555e-02,\n",
       "          3.86702123e-02, -2.49444389e-02,  8.16866338e-04, -1.31154861e-02,\n",
       "          4.54887142e-02,  1.82431527e-03, -9.26950632e-03, -1.15554111e-02,\n",
       "          2.80840740e-02,  1.72604205e-02,  5.92232558e-03,  1.80038366e-02,\n",
       "          4.91635839e-02,  2.28532993e-02,  6.92838741e-03, -3.29340996e-02,\n",
       "         -2.60844809e-02, -3.53558118e-03, -2.37629223e-02, -1.33396815e-02,\n",
       "          3.39014677e-02, -2.58249188e-02,  1.58621179e-02, -1.38280199e-02,\n",
       "         -1.94001082e-02,  3.23780085e-02,  8.73536972e-03, -1.06491400e-02,\n",
       "         -2.06113311e-02, -1.00204539e-02, -5.92414899e-02,  2.21567742e-02,\n",
       "          1.96703621e-02,  4.83981773e-03,  4.64754394e-03,  1.72329994e-02,\n",
       "          3.77878697e-03, -7.31476285e-03,  2.15761497e-02,  2.36946578e-02,\n",
       "         -3.94739185e-04, -3.15822189e-03,  6.27627253e-03, -1.63456528e-02,\n",
       "         -2.86408123e-03, -1.34084593e-02, -2.05908982e-02,  3.79488378e-02,\n",
       "         -2.59135821e-03, -5.99367373e-03, -5.48361672e-03, -2.25192984e-02,\n",
       "          1.53839963e-02, -1.96333146e-03,  1.04385230e-02, -9.95364947e-03,\n",
       "         -9.15884628e-04, -5.44193423e-03, -2.63560402e-03, -1.20995468e-02,\n",
       "         -1.94892631e-02,  5.10339449e-03, -3.99716956e-02,  6.63766081e-03,\n",
       "          2.83341507e-02,  1.61479897e-02, -6.24960469e-03, -2.01853115e-02,\n",
       "         -2.30493463e-02,  2.41974686e-02, -1.18435264e-02,  3.91196668e-03,\n",
       "          1.57427261e-02,  3.39905907e-02,  1.78401215e-02, -5.43773888e-03,\n",
       "          1.96007225e-02,  8.70553736e-02, -2.09045670e-02,  5.44575620e-03,\n",
       "         -1.74754817e-02, -6.31366838e-03,  4.67595758e-03,  1.15769349e-03,\n",
       "         -2.48051513e-02, -1.49467794e-02,  8.50755240e-03,  3.35060067e-02,\n",
       "          3.74447485e-03,  4.09239615e-02,  3.32012232e-02,  3.43022553e-02,\n",
       "          1.95873175e-03, -4.23116064e-02, -1.78572984e-03, -2.38531207e-02,\n",
       "          2.10662950e-03, -2.19738587e-02, -1.20522812e-02, -3.03089888e-03,\n",
       "          2.11344370e-02, -8.71930295e-03,  7.29551984e-03,  1.66129700e-02,\n",
       "         -1.27604342e-02,  9.56745217e-03,  6.98299118e-03,  2.53704170e-02,\n",
       "         -4.42790063e-03, -3.90480910e-02,  8.54313270e-03, -2.50870639e-02,\n",
       "         -3.21222553e-05,  1.70423389e-02,  1.84731972e-02,  2.96567194e-03,\n",
       "          2.61907966e-02,  1.49996948e-02, -2.33189960e-02,  6.46570743e-02,\n",
       "          3.11559877e-03,  2.37721322e-02,  1.93601322e-02, -2.57546712e-02,\n",
       "         -1.83501803e-02, -1.41467092e-02, -3.99285842e-03, -1.79875420e-02,\n",
       "          1.90183532e-02, -2.44178147e-02, -1.57051930e-02, -2.09854681e-02,\n",
       "          1.37791918e-03,  7.44763341e-02, -1.74727091e-02,  3.67230073e-03,\n",
       "          1.56129086e-02,  6.37680905e-04,  1.01134477e-02,  2.87668182e-02,\n",
       "          2.03147982e-02, -1.17364094e-02, -1.45116558e-02,  6.01164431e-03,\n",
       "         -7.51482784e-03, -1.18620916e-02,  1.71493723e-02, -1.25271817e-02,\n",
       "         -1.03429024e-02, -4.95697306e-03,  3.03381939e-02, -3.41038112e-02,\n",
       "          4.47540520e-02,  1.99627772e-02,  1.54856961e-02, -1.28739820e-02,\n",
       "         -1.37375703e-02,  7.22146110e-03, -1.12781470e-02, -3.54985917e-03,\n",
       "         -1.46177715e-02,  1.68767010e-02, -1.02305389e-03,  4.97828967e-03,\n",
       "         -4.69090611e-03, -7.10695948e-03,  5.32717242e-03,  2.88679476e-02,\n",
       "         -2.48944984e-03, -3.63596450e-04, -1.58664227e-02, -2.27195900e-02,\n",
       "         -1.06893757e-02, -1.09062554e-03,  1.24194070e-02,  1.34343369e-02,\n",
       "          2.25927407e-03,  5.54371125e-03,  6.09565856e-02, -9.14932218e-03,\n",
       "         -1.07793950e-02,  4.92963797e-02,  1.36362660e-02, -1.14051405e-02,\n",
       "         -1.74310632e-02,  5.70965599e-02, -1.19615038e-02,  1.05203062e-02,\n",
       "         -4.02353531e-03,  5.68556388e-03,  5.59104681e-02,  1.28396213e-02,\n",
       "         -1.06977573e-02, -1.08159224e-02, -1.46696972e-02, -9.03286637e-03,\n",
       "         -3.02932333e-04, -5.26778925e-04,  2.86689934e-02,  2.36106037e-03,\n",
       "         -4.98861461e-03,  6.93722282e-03, -7.73450376e-03, -8.31161353e-03,\n",
       "          7.77818995e-03, -6.43278433e-03, -1.27221542e-02,  1.05815064e-02,\n",
       "          2.84699066e-03, -2.10621337e-02, -3.27117528e-03, -2.31504872e-02,\n",
       "         -4.80825505e-03, -1.02471190e-02,  6.89615496e-03, -2.43042988e-02,\n",
       "         -1.73822640e-02, -2.59426005e-02, -2.67280864e-02,  1.26821842e-02,\n",
       "          1.60994342e-03,  3.71996485e-03, -5.48653625e-03, -1.88920388e-02,\n",
       "          1.97826725e-02,  4.47507107e-03, -1.87621528e-02, -2.91604958e-02,\n",
       "         -1.39914642e-02, -1.30793378e-02, -9.50955897e-04, -3.02010468e-02,\n",
       "          4.11583971e-03, -7.42575856e-03,  3.74844929e-02, -1.16436960e-02,\n",
       "         -1.95666894e-02, -1.67785357e-02,  2.49638147e-02, -8.38376810e-03])),\n",
       " 3.5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_no_tfidf = map_pairs(wv_model, train_preproc, tf_idf_model=None, dictionary=diccionario, )\n",
    "mapped_train_tfidf = map_pairs(wv_model,train_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "\n",
    "mapped_val_no_tfidf = map_pairs(wv_model,val_preproc, tf_idf_model=None, dictionary=diccionario, )\n",
    "mapped_val_tfidf = map_pairs(wv_model,val_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "\n",
    "mapped_train_tfidf[0]# Imprimir los pares de vectores y la puntuación de similitud asociada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De dimensión reducida: <sb>\n",
    "\n",
    "- sí usando pesos TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_50 = map_pairs(wv_model_50d, train_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_100 = map_pairs(wv_model_100d, train_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_150 = map_pairs(wv_model_150d, train_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "\n",
    "\n",
    "mapped_val_50 = map_pairs(wv_model_50d, val_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_val_100 = map_pairs(wv_model_100d, val_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_val_150 = map_pairs(wv_model_150d, val_preproc, tf_idf_model=modelo_tfidf, dictionary=diccionario, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n",
      "(50,)\n",
      "(100,)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# Imprimir las dimensiones de los vectores resultantes\n",
    "print(mapped_no_tfidf[0][0][0].shape)\n",
    "print(mapped_train_tfidf[0][0][0].shape)\n",
    "print(mapped_50[0][0][0].shape)\n",
    "print(mapped_100[0][0][0].shape)\n",
    "print(mapped_150[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definición de diferentes modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:59:33.852866Z",
     "start_time": "2025-05-23T14:59:29.286142Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir el Modelo\n",
    "\n",
    "def build_and_compile_model(hidden_size: int = 128, embedding_size: int = 300, learning_rate: float = 0.001) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Esto crea una red neuronal de manera que al entrenarla las distancias coseno cuadren con la etiqueta real\n",
    "    hidden_size: Tamaño de capas ocultas (no se usa en este código)\n",
    "    embedding_size: Dimensión de los vectores de entrada (300)\n",
    "    learning_rate: Tasa de aprendizaje para el optimizador\n",
    "    \"\"\"\n",
    "    # Capa de entrada para los pares de vectores\n",
    "    input_1 = tf.keras.Input(shape=(embedding_size,)) #los pares de vectores a comparar\n",
    "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
    "\n",
    "    # Capa oculta, con funcion de activacion lineal, tiene como objetivo proyectar los vectores de entrada en un nuevo espacio.\n",
    "    \"\"\"\n",
    "    La capa oculta (en este caso, la capa densa) tiene pesos que se ajustan durante el entrenamiento.\n",
    "    Estos pesos son los que transforman los vectores de entrada en los vectores proyectados\n",
    "    \"\"\"\n",
    "    first_projection = tf.keras.layers.Dense(\n",
    "        embedding_size,\n",
    "        # activation='tanh',\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),# inicializa los pesos de la capa como una matriz identidad\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "    )\n",
    "    #aplica la capa de proyeccion a los dos vectores de entrada\n",
    "    projected_1 = first_projection(input_1)\n",
    "    projected_2 = first_projection(input_2)\n",
    "\n",
    "    normalize = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))\n",
    "    projected_1 = normalize(projected_1)\n",
    "    projected_2 = normalize(projected_2)\n",
    "    output = tf.keras.layers.Lambda(lambda tensors: 2.5 * (1.0 + tf.reduce_sum(tensors[0] * tensors[1], axis=1)))([projected_1, projected_2])\n",
    "    # Definir el modelo con las capas de entrada y salida\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output) #Durante el entrenamiento, Keras ajusta los pesos de la capa oculta para minimizar la función de pérdida definida (en este caso, el error absoluto medio).\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**build_and_compile_model**: Proyección lineal + Similitud Coseno \n",
    "\n",
    "Arquitectura:\n",
    "\n",
    "- Aplica una capa densa compartida (misma proyección) a cada vector por separado. Inicialmente es una matriz identidad.\n",
    "- Los normaliza a magnitud 1.\n",
    "- Calcula la similitud coseno como cos(θ) = dot(product).\n",
    "- La salida es 2.5 * (1 + similitud_coseno), lo cual transforma el rango [-1, 1] a [0, 5].\n",
    "- Se entrena con MAE (Mean Absolute Error).\n",
    "\n",
    "Objetivo:\n",
    "- Aprender una proyección donde la similitud coseno refleje la puntuación deseada (por ejemplo, cuán similares son dos textos o frases).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_aggregated(embedding_dim: int, hidden_size: int = 128, dropout_rate: float = 0.3) -> tf.keras.Model:\n",
    "    input_1 = tf.keras.Input(shape=(embedding_dim,), name=\"input_vector_1\")\n",
    "    input_2 = tf.keras.Input(shape=(embedding_dim,), name=\"input_vector_2\")\n",
    "    concatenated = tf.keras.layers.Concatenate(axis=-1)([input_1, input_2])\n",
    "    x = tf.keras.layers.BatchNormalization()(concatenated)\n",
    "    x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x) # Activació lineal per a regressió\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics=['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "#model_agg.fit([X1_train, X2_train], Y_train, epochs=..., batch_size=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo **build_model_aggregated**: Concatenación + Red Neuronal Densa\n",
    "Arquitectura:\n",
    "- Toma dos vectores de entrada (input_vector_1, input_vector_2).\n",
    "- Los concatena (Concatenate), por lo que la dimensión del vector combinado es el doble del embedding_dim.\n",
    "- Aplica:\n",
    "    - BatchNormalization\n",
    "    - Dense con ReLU\n",
    "    - Otro BatchNormalization\n",
    "    - Dropout\n",
    "    - Una capa de salida densa sin activación (regresión lineal).\n",
    "    - Se entrena con MSE (Mean Squared Error).\n",
    "\n",
    "Objetivo:\n",
    "- Aprender una función no lineal entre los vectores concatenados y la puntuación objetivo (p. ej., similitud STS, afinidad, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_and_compile_model2(embedding_size: int = 300, learning_rate: float = 0.001) -> tf.keras.Model:\n",
    "    # Input layer\n",
    "    input_1 = tf.keras.Input(shape=(embedding_size,), name=\"input_vector_1\")\n",
    "    input_2 = tf.keras.Input(shape=(embedding_size,), name=\"input_vector_2\")\n",
    "\n",
    "    # hidden layer\n",
    "    first_projection_layer = tf.keras.layers.Dense(\n",
    "        embedding_size,\n",
    "        activation='tanh',\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "        name=\"projection_layer\"\n",
    "    )\n",
    "    dropout = tf.keras.layers.Dropout(0.3, name=\"projection_dropout\")\n",
    "    projected_1_dense = dropout(first_projection_layer(input_1))\n",
    "    projected_2_dense = dropout(first_projection_layer(input_2))\n",
    "\n",
    "    # Normalize the projected vectors using Lambda layers\n",
    "    normalized_1 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_1\"\n",
    "    )(projected_1_dense)\n",
    "    normalized_2 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_2\"\n",
    "    )(projected_2_dense)\n",
    "\n",
    "    # Compute the custom similarity score using a Lambda layer\n",
    "    similarity_sum = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True), name=\"similarity_sum\"\n",
    "    )([normalized_1, normalized_2])\n",
    "\n",
    "    output = tf.keras.layers.Lambda(\n",
    "        lambda x: 0.5 * (1.0 + x), name=\"output_scaling\" #cambiar 0.5 por 2.5 para que este entre 0 y 5 \n",
    "    )(similarity_sum)\n",
    "\n",
    "    # Definir el modelo con las capas de entrada y salida\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output, name=\"similarity_model\")\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo **build_and_compile_model2**.\n",
    "\n",
    "Deferencias clave con build_and_compile_model: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspecto                         | build_and_compile_model                  | `build_and_compile_model2`                              |\n",
    "| ------------------------------- | ----------------------------------------- | ------------------------------------------------------ |\n",
    "| **Activación en la proyección** | Ninguna (lineal)                          | `tanh` (no lineal)                                     |\n",
    "| **Regularización**              | No hay                                    | Sí, con `Dropout`                                      |\n",
    "| **Normalización**               | Directamente con `tf.linalg.l2_normalize` | Igual, pero encapsulada en `Lambda` layers con nombres |\n",
    "| **Similitud coseno**            | `2.5 * (1.0 + coseno)`                    | `0.5 * (1.0 + coseno)`                                 |\n",
    "| **Escalado de salida**          | Rango `[0, 5]`                            | Rango `[0, 1]`                                         |\n",
    "| **Perdida**                     | `mean_absolute_error`                     | `mean_squared_error`                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training y evaluación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir constantes de entrenamiento\n",
    "batch_size: int = 64\n",
    "num_epochs: int = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    model,\n",
    "    X1_test: np.ndarray,\n",
    "    X2_test: np.ndarray,\n",
    "    Y_test: np.ndarray,\n",
    "    name: str = \"\"\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Evalúa un modelo de similitud semántica sobre un conjunto de test y muestra varias métricas.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo entrenado (por ejemplo, un modelo de Keras).\n",
    "        X1_test: Array de vectores de entrada 1 (frase 1).\n",
    "        X2_test: Array de vectores de entrada 2 (frase 2).\n",
    "        Y_test: Array de etiquetas verdaderas (similitud real).\n",
    "        name: Nombre del modelo (opcional, para mostrar en los resultados).\n",
    "\n",
    "    Returns:\n",
    "        Diccionario con las métricas calculadas: MSE, RMSE, MAE, Pearson y Spearman.\n",
    "    \"\"\"\n",
    "    # Realiza la predicción del modelo\n",
    "    y_pred = model.predict([X1_test, X2_test]).squeeze()\n",
    "    y_true = Y_test.squeeze()\n",
    "\n",
    "    # Calcula las métricas de evaluación\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    pearson, _ = pearsonr(y_true, y_pred)\n",
    "    spearman, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "    # Muestra los resultados por pantalla\n",
    "    print(f\"\\n🔎 Resultados para el modelo '{name}':\")\n",
    "    print(f\"MSE:      {mse:.4f}\")\n",
    "    print(f\"RMSE:     {rmse:.4f}\")\n",
    "    print(f\"MAE:      {mae:.4f}\")\n",
    "    print(f\"Pearson:  {pearson:.4f}\")\n",
    "    print(f\"Spearman: {spearman:.4f}\")\n",
    "\n",
    "    # Devuelve las métricas en un diccionario\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"pearson\": pearson,\n",
    "        \"spearman\": spearman\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtener Train y Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Obtiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parejas de vectores de oraciones - Listas de (d, d, 1)\n",
    "    :param pair_list: lista que devuelve map_pairs(), lista de ((vector1, vector2), similitud), sonde vector1 y 2 son vectores agregados\n",
    "    :return:\n",
    "    transforma una lista de pares de vectores y puntuaciones en el formato adecuado para alimentar a un modelo de aprendizaje automático.\n",
    "    \"\"\"\n",
    "    _x, _y = zip(*pair_list) #_x: lista de tuplas (embedding_1, embedding_2), _y: lista de etiquetas\n",
    "    _x_1, _x_2 = zip(*_x)#_x_1: todos los embedding_1,  _x_2: todos los embedding_2\n",
    "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, ) / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las listas de train y test USANDO TF-IDF\n",
    "x_train, y_train = pair_list_to_x_y(mapped_train_tfidf)\n",
    "x_val, y_val = pair_list_to_x_y(mapped_val_tfidf)\n",
    "\n",
    "\n",
    "# Obtener las listas de train y test SIN USAR TF-IDF\n",
    "x_train_normal, y_train_normal = pair_list_to_x_y(mapped_no_tfidf)\n",
    "x_val_normal, y_val_normal = pair_list_to_x_y(mapped_val_no_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#De dimension reducida CON TF-IDF\n",
    "x_train_50, y_train_50 = pair_list_to_x_y(mapped_50)\n",
    "x_val_50, y_val_50 = pair_list_to_x_y(mapped_val_50)\n",
    "\n",
    "x_train_100, y_train_100 = pair_list_to_x_y(mapped_100)\n",
    "x_val_100, y_val_100 = pair_list_to_x_y(mapped_val_100)\n",
    "\n",
    "x_train_150, y_train_150 = pair_list_to_x_y(mapped_150)\n",
    "x_val_150, y_val_150 = pair_list_to_x_y(mapped_val_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-06 21:40:54.666095: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Preparar los conjuntos en forma de tensor\n",
    "#Con TF-IDF\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "\n",
    "\n",
    "#Sin TF-IDF\n",
    "train_dataset_normal = tf.data.Dataset.from_tensor_slices((x_train_normal, y_train_normal))\n",
    "train_dataset_normal = train_dataset_normal.shuffle(buffer_size=len(x_train_normal)).batch(batch_size)\n",
    "\n",
    "val_dataset_normal = tf.data.Dataset.from_tensor_slices((x_val_normal, y_val_normal))\n",
    "val_dataset_normal = val_dataset_normal.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensión 50\n",
    "train_dataset_50 = tf.data.Dataset.from_tensor_slices((x_train_50, y_train_50))\n",
    "train_dataset_50 = train_dataset_50.shuffle(buffer_size=len(x_train_50)).batch(batch_size)\n",
    "\n",
    "val_dataset_50 = tf.data.Dataset.from_tensor_slices((x_val_50, y_val_50))\n",
    "val_dataset_50 = val_dataset_50.batch(batch_size)\n",
    "\n",
    "#Dimensión 100\n",
    "train_dataset_100 = tf.data.Dataset.from_tensor_slices((x_train_100, y_train_100))\n",
    "train_dataset_100 = train_dataset_100.shuffle(buffer_size=len(x_train_100)).batch(batch_size)\n",
    "\n",
    "val_dataset_100 = tf.data.Dataset.from_tensor_slices((x_val_100, y_val_100))\n",
    "val_dataset_100 = val_dataset_100.batch(batch_size)\n",
    "\n",
    "\n",
    "#Dimensión 150\n",
    "train_dataset_150 = tf.data.Dataset.from_tensor_slices((x_train_150, y_train_150))\n",
    "train_dataset_150 = train_dataset_150.shuffle(buffer_size=len(x_train_150)).batch(batch_size)\n",
    "\n",
    "val_dataset_150 = tf.data.Dataset.from_tensor_slices((x_val_150, y_val_150))\n",
    "val_dataset_150 = val_dataset_150.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo COS:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos no entrenar ningún modelo si sólo utilizamos COS similarity. El rendiminto dependerá completamente del Word Embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando media clásica la correlación de Pearson es: 0.4023026679065351\n",
      "Usando media ponderada con TF-IDF la correlación de Pearson es: 0.24358562692308788\n",
      "Usando media ponderada con TF-IDF y dimensión 50, la correlación de Pearson es: 0.3632097710213902\n",
      "Usando media ponderada con TF-IDF y dimensión 100, la correlación de Pearson es: 0.38166246524470904\n",
      "Usando media ponderada con TF-IDF y dimensión 150, la correlación de Pearson es: 0.39754565403044134\n"
     ]
    }
   ],
   "source": [
    "y_pred_baseline = []\n",
    "y_pred_normal = []\n",
    "y_pred_50 = []\n",
    "y_pred_100 = []\n",
    "y_pred_150 = []\n",
    "for j in range(len(mapped_val_tfidf)):\n",
    "    i = mapped_val_tfidf[j]\n",
    "    v1= i[0][0] \n",
    "    v2 = i[0][1]\n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_baseline.append(d)\n",
    "\n",
    "    k = mapped_val_no_tfidf[j]\n",
    "    v1 = k[0][0]\n",
    "    v2 = k[0][1] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_normal.append(d)\n",
    "\n",
    "    m = mapped_val_50[j]\n",
    "    v1 = m[0][0] \n",
    "    v2 = m[0][1]\n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_50.append(d)\n",
    "\n",
    "    l = mapped_val_100[j]\n",
    "    v1 = l[0][0]\n",
    "    v2 = l[0][1] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_100.append(d)\n",
    "\n",
    "    e = mapped_val_150[j]\n",
    "    v1 = e[0][0]\n",
    "    v2 = e[0][1] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_150.append(d)\n",
    "\n",
    "# Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(np.array(y_pred_baseline), y_val.flatten())\n",
    "correlation_normal, _ = pearsonr(np.array(y_pred_normal), y_val_normal.flatten())\n",
    "correlation_50, _ = pearsonr(np.array(y_pred_50), y_val_50.flatten())\n",
    "correlation_100, _ = pearsonr(np.array(y_pred_100), y_val_100.flatten())\n",
    "correlation_150, _ = pearsonr(np.array(y_pred_150), y_val_150.flatten())\n",
    "\n",
    "# Imprimir el coeficiente de correlación de Pearson\n",
    "print(f\"Usando media clásica la correlación de Pearson es: {correlation}\")\n",
    "print(f\"Usando media ponderada con TF-IDF la correlación de Pearson es: {correlation_normal}\")\n",
    "print(f\"Usando media ponderada con TF-IDF y dimensión 50, la correlación de Pearson es: {correlation_50}\")\n",
    "print(f\"Usando media ponderada con TF-IDF y dimensión 100, la correlación de Pearson es: {correlation_100}\")\n",
    "print(f\"Usando media ponderada con TF-IDF y dimensión 150, la correlación de Pearson es: {correlation_150}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vemos que, en este caso, la ponderación con los pesos TF-IDF empeora considerablemente el rendimiento. Si dos frases tienen pocas o ninguna palabra en común, el coseno será bajo, aunque el significado sea parecido.\n",
    "- La reducción de la dimensión también afecta negativamente al modelo. Suponemos que es debido a la pérdida considerable de información.\n",
    "- Nos gustaría poder modificar los pesos durante el cálculo de la distancia coseno."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de Regresión 1, prueba del embeding pre-entrenado:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sin distancia coseno**, modelo simple de regresión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_vector_1', 'input_vector_2']\n",
      "Received: inputs=(('Tensor(shape=(None, 300))', 'Tensor(shape=(None, 300))'),)\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 2.5055 - mae: 1.2171 - root_mean_squared_error: 1.5797\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4504 - mae: 0.9192 - root_mean_squared_error: 1.2035\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9373 - mae: 0.7495 - root_mean_squared_error: 0.9679\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7207 - mae: 0.6446 - root_mean_squared_error: 0.8488\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5769 - mae: 0.5778 - root_mean_squared_error: 0.7591\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4608 - mae: 0.5098 - root_mean_squared_error: 0.6782\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3841 - mae: 0.4838 - root_mean_squared_error: 0.6198\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3248 - mae: 0.4401 - root_mean_squared_error: 0.5695\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2571 - mae: 0.3875 - root_mean_squared_error: 0.5068\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2379 - mae: 0.3707 - root_mean_squared_error: 0.4876\n",
      "Epoch 11/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1926 - mae: 0.3262 - root_mean_squared_error: 0.4387\n",
      "Epoch 12/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1742 - mae: 0.3210 - root_mean_squared_error: 0.4172\n",
      "Epoch 13/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1473 - mae: 0.2871 - root_mean_squared_error: 0.3835\n",
      "Epoch 14/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1215 - mae: 0.2612 - root_mean_squared_error: 0.3486\n",
      "Epoch 15/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1017 - mae: 0.2432 - root_mean_squared_error: 0.3188\n",
      "Epoch 16/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0901 - mae: 0.2286 - root_mean_squared_error: 0.3000\n",
      "Epoch 17/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0751 - mae: 0.2081 - root_mean_squared_error: 0.2740\n",
      "Epoch 18/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0643 - mae: 0.1959 - root_mean_squared_error: 0.2535\n",
      "Epoch 19/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0660 - mae: 0.1967 - root_mean_squared_error: 0.2566\n",
      "Epoch 20/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0569 - mae: 0.1796 - root_mean_squared_error: 0.2385\n",
      "Epoch 21/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0473 - mae: 0.1663 - root_mean_squared_error: 0.2175\n",
      "Epoch 22/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0428 - mae: 0.1590 - root_mean_squared_error: 0.2068\n",
      "Epoch 23/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0421 - mae: 0.1558 - root_mean_squared_error: 0.2051\n",
      "Epoch 24/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0414 - mae: 0.1531 - root_mean_squared_error: 0.2034\n",
      "Epoch 25/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0354 - mae: 0.1477 - root_mean_squared_error: 0.1879\n",
      "Epoch 26/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0373 - mae: 0.1439 - root_mean_squared_error: 0.1920\n",
      "Epoch 27/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0296 - mae: 0.1336 - root_mean_squared_error: 0.1719\n",
      "Epoch 28/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0288 - mae: 0.1322 - root_mean_squared_error: 0.1696\n",
      "Epoch 29/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0286 - mae: 0.1306 - root_mean_squared_error: 0.1691\n",
      "Epoch 30/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0264 - mae: 0.1258 - root_mean_squared_error: 0.1623\n",
      "Epoch 31/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0252 - mae: 0.1230 - root_mean_squared_error: 0.1588\n",
      "Epoch 32/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0247 - mae: 0.1211 - root_mean_squared_error: 0.1572\n",
      "Epoch 33/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0253 - mae: 0.1221 - root_mean_squared_error: 0.1589\n",
      "Epoch 34/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0242 - mae: 0.1208 - root_mean_squared_error: 0.1557\n",
      "Epoch 35/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0218 - mae: 0.1143 - root_mean_squared_error: 0.1476\n",
      "Epoch 36/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0216 - mae: 0.1126 - root_mean_squared_error: 0.1468\n",
      "Epoch 37/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0208 - mae: 0.1123 - root_mean_squared_error: 0.1440\n",
      "Epoch 38/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - mae: 0.1111 - root_mean_squared_error: 0.1441\n",
      "Epoch 39/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0215 - mae: 0.1147 - root_mean_squared_error: 0.1467\n",
      "Epoch 40/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0190 - mae: 0.1055 - root_mean_squared_error: 0.1375\n",
      "Epoch 41/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0192 - mae: 0.1089 - root_mean_squared_error: 0.1385\n",
      "Epoch 42/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0181 - mae: 0.1029 - root_mean_squared_error: 0.1344\n",
      "Epoch 43/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0183 - mae: 0.1041 - root_mean_squared_error: 0.1352\n",
      "Epoch 44/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0177 - mae: 0.1038 - root_mean_squared_error: 0.1331\n",
      "Epoch 45/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0167 - mae: 0.1009 - root_mean_squared_error: 0.1291\n",
      "Epoch 46/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0165 - mae: 0.1003 - root_mean_squared_error: 0.1285\n",
      "Epoch 47/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0163 - mae: 0.0988 - root_mean_squared_error: 0.1276\n",
      "Epoch 48/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0154 - mae: 0.0970 - root_mean_squared_error: 0.1236\n",
      "Epoch 49/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0154 - mae: 0.0974 - root_mean_squared_error: 0.1241\n",
      "Epoch 50/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - mae: 0.0958 - root_mean_squared_error: 0.1229\n",
      "Epoch 51/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0145 - mae: 0.0939 - root_mean_squared_error: 0.1204\n",
      "Epoch 52/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0156 - mae: 0.0957 - root_mean_squared_error: 0.1250\n",
      "Epoch 53/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - mae: 0.0952 - root_mean_squared_error: 0.1230\n",
      "Epoch 54/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0146 - mae: 0.0918 - root_mean_squared_error: 0.1207\n",
      "Epoch 55/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0139 - mae: 0.0910 - root_mean_squared_error: 0.1179\n",
      "Epoch 56/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0139 - mae: 0.0911 - root_mean_squared_error: 0.1178\n",
      "Epoch 57/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0133 - mae: 0.0906 - root_mean_squared_error: 0.1155\n",
      "Epoch 58/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0140 - mae: 0.0922 - root_mean_squared_error: 0.1181\n",
      "Epoch 59/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0132 - mae: 0.0893 - root_mean_squared_error: 0.1148\n",
      "Epoch 60/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0132 - mae: 0.0882 - root_mean_squared_error: 0.1146\n",
      "Epoch 61/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0121 - mae: 0.0845 - root_mean_squared_error: 0.1099\n",
      "Epoch 62/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0126 - mae: 0.0883 - root_mean_squared_error: 0.1124\n",
      "Epoch 63/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0120 - mae: 0.0862 - root_mean_squared_error: 0.1096\n",
      "Epoch 64/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0122 - mae: 0.0862 - root_mean_squared_error: 0.1105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x787a1155ef60>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Con TF-IDF\n",
    "model_no_cos = build_model_aggregated(embedding_dim=300)\n",
    "model_no_cos.fit([x_train], y_train, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "🔎 Resultados para el modelo 'Linealmodel no cos()':\n",
      "MSE:      0.0308\n",
      "RMSE:     0.1756\n",
      "MAE:      0.1344\n",
      "Pearson:  0.2468\n",
      "Spearman: 0.2546\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val\n",
    "D_agragated_tfidf = evaluate_model(model_no_cos, X1, X2, y_val, name=\"Linealmodel no cos()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0169 - mae: 0.1002 - root_mean_squared_error: 0.1301\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0141 - mae: 0.0925 - root_mean_squared_error: 0.1185\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0147 - mae: 0.0947 - root_mean_squared_error: 0.1213\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0153 - mae: 0.0952 - root_mean_squared_error: 0.1236\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0146 - mae: 0.0938 - root_mean_squared_error: 0.1210\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0136 - mae: 0.0896 - root_mean_squared_error: 0.1165\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0126 - mae: 0.0892 - root_mean_squared_error: 0.1123\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0124 - mae: 0.0861 - root_mean_squared_error: 0.1116\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0116 - mae: 0.0854 - root_mean_squared_error: 0.1079\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0117 - mae: 0.0850 - root_mean_squared_error: 0.1083\n",
      "Epoch 11/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0117 - mae: 0.0843 - root_mean_squared_error: 0.1083\n",
      "Epoch 12/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0118 - mae: 0.0847 - root_mean_squared_error: 0.1086\n",
      "Epoch 13/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0121 - mae: 0.0864 - root_mean_squared_error: 0.1099\n",
      "Epoch 14/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0101 - mae: 0.0794 - root_mean_squared_error: 0.1004\n",
      "Epoch 15/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0112 - mae: 0.0815 - root_mean_squared_error: 0.1060\n",
      "Epoch 16/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0101 - mae: 0.0789 - root_mean_squared_error: 0.1005\n",
      "Epoch 17/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0108 - mae: 0.0796 - root_mean_squared_error: 0.1037\n",
      "Epoch 18/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0107 - mae: 0.0792 - root_mean_squared_error: 0.1035\n",
      "Epoch 19/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0108 - mae: 0.0804 - root_mean_squared_error: 0.1040\n",
      "Epoch 20/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0115 - mae: 0.0821 - root_mean_squared_error: 0.1071\n",
      "Epoch 21/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0098 - mae: 0.0778 - root_mean_squared_error: 0.0987\n",
      "Epoch 22/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0105 - mae: 0.0795 - root_mean_squared_error: 0.1026\n",
      "Epoch 23/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0098 - mae: 0.0766 - root_mean_squared_error: 0.0988\n",
      "Epoch 24/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0098 - mae: 0.0777 - root_mean_squared_error: 0.0988\n",
      "Epoch 25/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0093 - mae: 0.0758 - root_mean_squared_error: 0.0966\n",
      "Epoch 26/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0100 - mae: 0.0786 - root_mean_squared_error: 0.1001\n",
      "Epoch 27/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0097 - mae: 0.0759 - root_mean_squared_error: 0.0983\n",
      "Epoch 28/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0093 - mae: 0.0745 - root_mean_squared_error: 0.0964\n",
      "Epoch 29/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0095 - mae: 0.0762 - root_mean_squared_error: 0.0975\n",
      "Epoch 30/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0095 - mae: 0.0756 - root_mean_squared_error: 0.0973\n",
      "Epoch 31/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0092 - mae: 0.0737 - root_mean_squared_error: 0.0958\n",
      "Epoch 32/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0092 - mae: 0.0745 - root_mean_squared_error: 0.0958\n",
      "Epoch 33/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0092 - mae: 0.0744 - root_mean_squared_error: 0.0961\n",
      "Epoch 34/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0086 - mae: 0.0725 - root_mean_squared_error: 0.0928\n",
      "Epoch 35/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0089 - mae: 0.0735 - root_mean_squared_error: 0.0943\n",
      "Epoch 36/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0093 - mae: 0.0741 - root_mean_squared_error: 0.0962\n",
      "Epoch 37/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0083 - mae: 0.0708 - root_mean_squared_error: 0.0909\n",
      "Epoch 38/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0089 - mae: 0.0734 - root_mean_squared_error: 0.0941\n",
      "Epoch 39/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0088 - mae: 0.0724 - root_mean_squared_error: 0.0936\n",
      "Epoch 40/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0085 - mae: 0.0715 - root_mean_squared_error: 0.0923\n",
      "Epoch 41/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0085 - mae: 0.0715 - root_mean_squared_error: 0.0922\n",
      "Epoch 42/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0080 - mae: 0.0694 - root_mean_squared_error: 0.0896\n",
      "Epoch 43/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0087 - mae: 0.0720 - root_mean_squared_error: 0.0930\n",
      "Epoch 44/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0084 - mae: 0.0714 - root_mean_squared_error: 0.0917\n",
      "Epoch 45/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0089 - mae: 0.0740 - root_mean_squared_error: 0.0945\n",
      "Epoch 46/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0080 - mae: 0.0700 - root_mean_squared_error: 0.0892\n",
      "Epoch 47/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0079 - mae: 0.0686 - root_mean_squared_error: 0.0888\n",
      "Epoch 48/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0082 - mae: 0.0704 - root_mean_squared_error: 0.0907\n",
      "Epoch 49/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0080 - mae: 0.0691 - root_mean_squared_error: 0.0894\n",
      "Epoch 50/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0075 - mae: 0.0671 - root_mean_squared_error: 0.0868\n",
      "Epoch 51/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0078 - mae: 0.0690 - root_mean_squared_error: 0.0883\n",
      "Epoch 52/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0080 - mae: 0.0697 - root_mean_squared_error: 0.0895\n",
      "Epoch 53/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0083 - mae: 0.0714 - root_mean_squared_error: 0.0909\n",
      "Epoch 54/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0079 - mae: 0.0681 - root_mean_squared_error: 0.0890\n",
      "Epoch 55/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0077 - mae: 0.0685 - root_mean_squared_error: 0.0877\n",
      "Epoch 56/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0073 - mae: 0.0673 - root_mean_squared_error: 0.0852\n",
      "Epoch 57/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0071 - mae: 0.0653 - root_mean_squared_error: 0.0841\n",
      "Epoch 58/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0073 - mae: 0.0655 - root_mean_squared_error: 0.0853\n",
      "Epoch 59/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0079 - mae: 0.0674 - root_mean_squared_error: 0.0889\n",
      "Epoch 60/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0077 - mae: 0.0680 - root_mean_squared_error: 0.0878\n",
      "Epoch 61/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0073 - mae: 0.0663 - root_mean_squared_error: 0.0854\n",
      "Epoch 62/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0079 - mae: 0.0684 - root_mean_squared_error: 0.0888\n",
      "Epoch 63/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0077 - mae: 0.0680 - root_mean_squared_error: 0.0876\n",
      "Epoch 64/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0075 - mae: 0.0671 - root_mean_squared_error: 0.0865\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x787a11cc5d00>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sin TF-IDF\n",
    "model_no_cos_no_tfidf = build_model_aggregated(embedding_dim=300)\n",
    "model_no_cos.fit([x_train_normal], y_train_normal, epochs=num_epochs, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "🔎 Resultados para el modelo 'Linealmodel no cos()':\n",
      "MSE:      0.0314\n",
      "RMSE:     0.1771\n",
      "MAE:      0.1336\n",
      "Pearson:  0.2577\n",
      "Spearman: 0.2713\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val_normal\n",
    "D_agragated_no_tfidf = evaluate_model(model_no_cos, X1, X2, y_val_normal, name=\"Linealmodel no cos()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Este modelo no tiene mucho sentido ya que no incorpora la distancia coseno en el etrenamiento.\n",
    "- TF-IDF no mejora el rendimiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo con coseno y activación lineal:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3.9945 - val_loss: 3.7056\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.6515 - val_loss: 3.6114\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5301 - val_loss: 3.5616\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4431 - val_loss: 3.5285\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3736 - val_loss: 3.5051\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3150 - val_loss: 3.4873\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2658 - val_loss: 3.4731\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2222 - val_loss: 3.4616\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1845 - val_loss: 3.4523\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1504 - val_loss: 3.4442\n",
      "Epoch 11/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1203 - val_loss: 3.4382\n",
      "Epoch 12/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0945 - val_loss: 3.4321\n",
      "Epoch 13/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0659 - val_loss: 3.4259\n",
      "Epoch 14/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0443 - val_loss: 3.4209\n",
      "Epoch 15/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0237 - val_loss: 3.4212\n",
      "Epoch 16/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0064 - val_loss: 3.4163\n",
      "Epoch 17/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9916 - val_loss: 3.4113\n",
      "Epoch 18/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9654 - val_loss: 3.4094\n",
      "Epoch 19/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9524 - val_loss: 3.4086\n",
      "Epoch 20/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9458 - val_loss: 3.4032\n",
      "Epoch 21/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9251 - val_loss: 3.4048\n",
      "Epoch 22/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9131 - val_loss: 3.4020\n",
      "Epoch 23/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9056 - val_loss: 3.3984\n",
      "Epoch 24/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8859 - val_loss: 3.4009\n",
      "Epoch 25/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8778 - val_loss: 3.3985\n",
      "Epoch 26/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8601 - val_loss: 3.3935\n",
      "Epoch 27/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8550 - val_loss: 3.3945\n",
      "Epoch 28/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8422 - val_loss: 3.3896\n",
      "Epoch 29/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8310 - val_loss: 3.3928\n",
      "Epoch 30/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8329 - val_loss: 3.3882\n",
      "Epoch 31/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8126 - val_loss: 3.3889\n",
      "Epoch 32/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8039 - val_loss: 3.3847\n",
      "Epoch 33/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8008 - val_loss: 3.3835\n",
      "Epoch 34/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7868 - val_loss: 3.3809\n",
      "Epoch 35/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7791 - val_loss: 3.3857\n",
      "Epoch 36/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7747 - val_loss: 3.3763\n",
      "Epoch 37/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7584 - val_loss: 3.3758\n",
      "Epoch 38/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7541 - val_loss: 3.3794\n",
      "Epoch 39/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7460 - val_loss: 3.3720\n",
      "Epoch 40/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7293 - val_loss: 3.3734\n",
      "Epoch 41/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7257 - val_loss: 3.3781\n",
      "Epoch 42/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7268 - val_loss: 3.3668\n",
      "Epoch 43/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7059 - val_loss: 3.3663\n",
      "Epoch 44/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6971 - val_loss: 3.3667\n",
      "Epoch 45/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6917 - val_loss: 3.3664\n",
      "Epoch 46/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6877 - val_loss: 3.3641\n",
      "Epoch 47/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6779 - val_loss: 3.3617\n",
      "Epoch 48/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6703 - val_loss: 3.3623\n",
      "Epoch 49/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6637 - val_loss: 3.3563\n",
      "Epoch 50/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6568 - val_loss: 3.3633\n",
      "Epoch 51/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6633 - val_loss: 3.3573\n",
      "Epoch 52/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6457 - val_loss: 3.3486\n",
      "Epoch 53/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6400 - val_loss: 3.3491\n",
      "Epoch 54/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6288 - val_loss: 3.3510\n",
      "Epoch 55/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6255 - val_loss: 3.3454\n",
      "Epoch 56/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6160 - val_loss: 3.3435\n",
      "Epoch 57/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6051 - val_loss: 3.3443\n",
      "Epoch 58/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6046 - val_loss: 3.3423\n",
      "Epoch 59/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5962 - val_loss: 3.3375\n",
      "Epoch 60/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5871 - val_loss: 3.3327\n",
      "Epoch 61/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5813 - val_loss: 3.3409\n",
      "Epoch 62/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5849 - val_loss: 3.3296\n",
      "Epoch 63/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.5716 - val_loss: 3.3295\n",
      "Epoch 64/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5638 - val_loss: 3.3310\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x787a0f7ef560>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lin = build_and_compile_model()\n",
    "model_lin.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🔎 Resultados para el modelo '':\n",
      "MSE:      12.1662\n",
      "RMSE:     3.4880\n",
      "MAE:      3.3310\n",
      "Pearson:  0.2451\n",
      "Spearman: 0.2986\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val\n",
    "D_lin_tfidf = evaluate_model(model_lin, X1, X2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 4.0661 - val_loss: 4.1375\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5981 - val_loss: 4.2438\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.3850 - val_loss: 4.2717\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2886 - val_loss: 4.2721\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2246 - val_loss: 4.2670\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1787 - val_loss: 4.2609\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1404 - val_loss: 4.2538\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1077 - val_loss: 4.2501\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0731 - val_loss: 4.2463\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0413 - val_loss: 4.2446\n",
      "Epoch 11/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0123 - val_loss: 4.2423\n",
      "Epoch 12/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9877 - val_loss: 4.2406\n",
      "Epoch 13/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.9648 - val_loss: 4.2375\n",
      "Epoch 14/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9434 - val_loss: 4.2345\n",
      "Epoch 15/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9260 - val_loss: 4.2333\n",
      "Epoch 16/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9104 - val_loss: 4.2307\n",
      "Epoch 17/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8924 - val_loss: 4.2300\n",
      "Epoch 18/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8799 - val_loss: 4.2302\n",
      "Epoch 19/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8637 - val_loss: 4.2302\n",
      "Epoch 20/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8512 - val_loss: 4.2257\n",
      "Epoch 21/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8403 - val_loss: 4.2216\n",
      "Epoch 22/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8269 - val_loss: 4.2218\n",
      "Epoch 23/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8153 - val_loss: 4.2160\n",
      "Epoch 24/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8065 - val_loss: 4.2179\n",
      "Epoch 25/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.7963 - val_loss: 4.2159\n",
      "Epoch 26/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7893 - val_loss: 4.2196\n",
      "Epoch 27/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7806 - val_loss: 4.2205\n",
      "Epoch 28/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7839 - val_loss: 4.2121\n",
      "Epoch 29/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7719 - val_loss: 4.2060\n",
      "Epoch 30/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7615 - val_loss: 4.1988\n",
      "Epoch 31/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7594 - val_loss: 4.1933\n",
      "Epoch 32/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7500 - val_loss: 4.1903\n",
      "Epoch 33/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7388 - val_loss: 4.1900\n",
      "Epoch 34/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7316 - val_loss: 4.1919\n",
      "Epoch 35/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7267 - val_loss: 4.1924\n",
      "Epoch 36/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7209 - val_loss: 4.1946\n",
      "Epoch 37/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7171 - val_loss: 4.1976\n",
      "Epoch 38/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7106 - val_loss: 4.1963\n",
      "Epoch 39/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7194 - val_loss: 4.1903\n",
      "Epoch 40/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7062 - val_loss: 4.1817\n",
      "Epoch 41/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7041 - val_loss: 4.1758\n",
      "Epoch 42/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6964 - val_loss: 4.1685\n",
      "Epoch 43/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6965 - val_loss: 4.1638\n",
      "Epoch 44/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7002 - val_loss: 4.1619\n",
      "Epoch 45/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6985 - val_loss: 4.1663\n",
      "Epoch 46/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6892 - val_loss: 4.1670\n",
      "Epoch 47/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6803 - val_loss: 4.1599\n",
      "Epoch 48/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6784 - val_loss: 4.1603\n",
      "Epoch 49/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6744 - val_loss: 4.1616\n",
      "Epoch 50/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6754 - val_loss: 4.1569\n",
      "Epoch 51/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6674 - val_loss: 4.1569\n",
      "Epoch 52/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6655 - val_loss: 4.1611\n",
      "Epoch 53/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6663 - val_loss: 4.1542\n",
      "Epoch 54/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6578 - val_loss: 4.1587\n",
      "Epoch 55/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6590 - val_loss: 4.1570\n",
      "Epoch 56/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6567 - val_loss: 4.1590\n",
      "Epoch 57/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6520 - val_loss: 4.1534\n",
      "Epoch 58/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6506 - val_loss: 4.1477\n",
      "Epoch 59/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6434 - val_loss: 4.1475\n",
      "Epoch 60/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6412 - val_loss: 4.1451\n",
      "Epoch 61/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6405 - val_loss: 4.1459\n",
      "Epoch 62/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6348 - val_loss: 4.1470\n",
      "Epoch 63/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6363 - val_loss: 4.1479\n",
      "Epoch 64/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6344 - val_loss: 4.1513\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x787a0f7e1490>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lin_no_tfidf = build_and_compile_model()\n",
    "model_lin_no_tfidf.fit(train_dataset_normal, epochs=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "\n",
      "🔎 Resultados para el modelo '':\n",
      "MSE:      13.4533\n",
      "RMSE:     3.6679\n",
      "MAE:      3.3347\n",
      "Pearson:  0.1075\n",
      "Spearman: 0.1922\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val_normal\n",
    "D_lin_no_itfidf = evaluate_model(model_lin_no_tfidf, X1, X2, y_val_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vemos que resultados no meoran incorporando la distancia coseno en el entrenamiento.\n",
    "- La ponderación TF-IDF, en este caso, sí que mejora el rendimineto del modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo con coeno y activación no lineal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.1045 - val_loss: 0.1450\n",
      "Epoch 2/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0820 - val_loss: 0.1353\n",
      "Epoch 3/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0760 - val_loss: 0.1305\n",
      "Epoch 4/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0729 - val_loss: 0.1264\n",
      "Epoch 5/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0698 - val_loss: 0.1231\n",
      "Epoch 6/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0659 - val_loss: 0.1206\n",
      "Epoch 7/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0634 - val_loss: 0.1185\n",
      "Epoch 8/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0607 - val_loss: 0.1165\n",
      "Epoch 9/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0583 - val_loss: 0.1153\n",
      "Epoch 10/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0566 - val_loss: 0.1145\n",
      "Epoch 11/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0549 - val_loss: 0.1135\n",
      "Epoch 12/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0534 - val_loss: 0.1124\n",
      "Epoch 13/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0513 - val_loss: 0.1120\n",
      "Epoch 14/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0507 - val_loss: 0.1105\n",
      "Epoch 15/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.1100\n",
      "Epoch 16/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0481 - val_loss: 0.1100\n",
      "Epoch 17/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0470 - val_loss: 0.1089\n",
      "Epoch 18/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0459 - val_loss: 0.1089\n",
      "Epoch 19/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0453 - val_loss: 0.1081\n",
      "Epoch 20/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0438 - val_loss: 0.1077\n",
      "Epoch 21/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0433 - val_loss: 0.1076\n",
      "Epoch 22/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0427 - val_loss: 0.1075\n",
      "Epoch 23/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0416 - val_loss: 0.1070\n",
      "Epoch 24/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0410 - val_loss: 0.1068\n",
      "Epoch 25/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.1064\n",
      "Epoch 26/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.1059\n",
      "Epoch 27/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.1056\n",
      "Epoch 28/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.1056\n",
      "Epoch 29/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.1056\n",
      "Epoch 30/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.1053\n",
      "Epoch 31/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0370 - val_loss: 0.1053\n",
      "Epoch 32/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.1054\n",
      "Epoch 33/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.1049\n",
      "Epoch 34/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.1050\n",
      "Epoch 35/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.1048\n",
      "Epoch 36/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0351 - val_loss: 0.1043\n",
      "Epoch 37/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0348 - val_loss: 0.1045\n",
      "Epoch 38/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0338 - val_loss: 0.1044\n",
      "Epoch 39/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0340 - val_loss: 0.1044\n",
      "Epoch 40/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0336 - val_loss: 0.1042\n",
      "Epoch 41/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0332 - val_loss: 0.1038\n",
      "Epoch 42/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0324 - val_loss: 0.1039\n",
      "Epoch 43/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0325 - val_loss: 0.1041\n",
      "Epoch 44/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0324 - val_loss: 0.1042\n",
      "Epoch 45/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0323 - val_loss: 0.1040\n",
      "Epoch 46/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0314 - val_loss: 0.1037\n",
      "Epoch 47/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0312 - val_loss: 0.1038\n",
      "Epoch 48/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0305 - val_loss: 0.1034\n",
      "Epoch 49/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0308 - val_loss: 0.1037\n",
      "Epoch 50/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0304 - val_loss: 0.1040\n",
      "Epoch 51/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0301 - val_loss: 0.1038\n",
      "Epoch 52/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0299 - val_loss: 0.1034\n",
      "Epoch 53/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0296 - val_loss: 0.1035\n",
      "Epoch 54/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0297 - val_loss: 0.1035\n",
      "Epoch 55/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0290 - val_loss: 0.1034\n",
      "Epoch 56/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0289 - val_loss: 0.1034\n",
      "Epoch 57/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0290 - val_loss: 0.1034\n",
      "Epoch 58/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0283 - val_loss: 0.1035\n",
      "Epoch 59/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0285 - val_loss: 0.1034\n",
      "Epoch 60/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0283 - val_loss: 0.1032\n",
      "Epoch 61/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0284 - val_loss: 0.1034\n",
      "Epoch 62/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0283 - val_loss: 0.1033\n",
      "Epoch 63/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0280 - val_loss: 0.1035\n",
      "Epoch 64/64\n",
      "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0279 - val_loss: 0.1032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x787a0ed8cd70>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_no_lin = build_and_compile_model2()\n",
    "# Entrenar el modelo\n",
    "model_no_lin.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🔎 Resultados para el modelo '':\n",
      "MSE:      0.1032\n",
      "RMSE:     0.3212\n",
      "MAE:      0.2868\n",
      "Pearson:  0.4717\n",
      "Spearman: 0.4755\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val\n",
    "D_no_lin_tfidf = evaluate_model(model_no_lin, X1, X2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_lin_no_tfidf = build_and_compile_model2()\n",
    "model_no_lin_no_tfidf.fit(train_dataset_normal, epochs=num_epochs, validation_data=val_dataset_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🔎 Resultados para el modelo '':\n",
      "MSE:      0.1050\n",
      "RMSE:     0.3240\n",
      "MAE:      0.2848\n",
      "Pearson:  0.3791\n",
      "Spearman: 0.4134\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val_normal\n",
    "D_no_lin_no_tfidf = evaluate_model(model_no_lin_no_tfidf, X1, X2, y_val_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- El uso de TF-IDF mejora los resultados. <sb>\n",
    "- La activación no lineal y la incorporación de la distancia coseno dentro del enrenamiento mejora el rendimiento.\n",
    "\n",
    "### El impacto de la dimensionalidad:\n",
    "\n",
    "Para la evaluación del impacto de la dimensionalidad se va a usar el mejor modelo hasta el momento, que es `build_and_compile_model2()`. Usando la ponderación TF-IDF ya que, como hemos visto, permite mejorar el rendimineto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_lin_50 = build_and_compile_model2(embedding_size = 50)\n",
    "model_no_lin_50.fit(train_dataset_50, epochs=num_epochs, validation_data=val_dataset_50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🔎 Resultados para el modelo 'De dimansión 50':\n",
      "MSE:      0.1203\n",
      "RMSE:     0.3469\n",
      "MAE:      0.3119\n",
      "Pearson:  0.4211\n",
      "Spearman: 0.4397\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val_50\n",
    "D_50 = evaluate_model(model_no_lin_50, X1, X2, y_val_50, \"De dimansión 50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_lin_100 = build_and_compile_model2(embedding_size = 100)\n",
    "model_no_lin_100.fit(train_dataset_100, epochs=num_epochs, validation_data=val_dataset_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🔎 Resultados para el modelo 'De dimansión 100':\n",
      "MSE:      0.1104\n",
      "RMSE:     0.3323\n",
      "MAE:      0.2986\n",
      "Pearson:  0.4619\n",
      "Spearman: 0.4844\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val_100\n",
    "D_100 = evaluate_model(model_no_lin_100, X1, X2, y_val_100, \"De dimansión 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_no_lin_150 = build_and_compile_model2(embedding_size = 150)\n",
    "model_no_lin_150.fit(train_dataset_150, epochs=num_epochs, validation_data=val_dataset_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "\n",
      "🔎 Resultados para el modelo '':\n",
      "MSE:      0.1075\n",
      "RMSE:     0.3279\n",
      "MAE:      0.2941\n",
      "Pearson:  0.4702\n",
      "Spearman: 0.4877\n"
     ]
    }
   ],
   "source": [
    "X1, X2 = x_val_150\n",
    "D_150 = evaluate_model(model_no_lin_150, X1, X2, y_val_150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que un tamaño de embeding 100 da los mejores resultados. Parece que una dimensión menor permite generalizar un poco mejor al modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado evaluaremos la mejor arquitectura encontrada hasta ahora (build_and_compile_model2) utilizando el embeding de Spacy, que ya devuelve el vector de la oración promediado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ca_core_news_md\") #previamente hay que descargar: python -m spacy download ca_core_news_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_spacy(model, train: Iterable[dict]) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Procesa los datos de entrenamiento para extraer los embeddings de las frases y las puntuaciones.\n",
    "    Args:\n",
    "        model: Un modelo de lenguaje spaCy cargado.\n",
    "        train: Iterable de diccionarios con las claves 'sentence_1', 'sentence_2' y 'label'.\n",
    "        \n",
    "    Devuelve tres arrays: vectores de sentence_1, sentence_2 y etiquetas.\n",
    "    \"\"\"\n",
    "    e1_list = []\n",
    "    e2_list = []\n",
    "    scores = []\n",
    "    for i in train:\n",
    "        s1 = model(i[\"sentence_1\"])\n",
    "        s2 = model(i[\"sentence_2\"])\n",
    "        e1_list.append(s1.vector)\n",
    "        e2_list.append(s2.vector)\n",
    "        scores.append(i[\"label\"]/ 5.0)\n",
    "    return np.array(e1_list), np.array(e2_list), np.array(scores, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_train_s, x2_train_s, y_train_s = prepare_spacy(nlp, train)\n",
    "x1_val_s, x2_val_s, y_val_s = prepare_spacy(nlp, val)\n",
    "\n",
    "train_dataset_s = tf.data.Dataset.from_tensor_slices(((x1_train_s, x2_train_s), y_train_s))\n",
    "train_dataset_s = train_dataset_s.shuffle(buffer_size=len(x1_train_s)).batch(batch_size)\n",
    "\n",
    "val_dataset_s = tf.data.Dataset.from_tensor_slices(((x1_val_s, x2_val_s), y_val_s))\n",
    "val_dataset_s = val_dataset_s.batch(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar\n",
    "model_no_lin_con_s = build_and_compile_model2(embedding_size=300)\n",
    "model_no_lin_con_s.fit(train_dataset_s, epochs=num_epochs, validation_data=val_dataset_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\n",
      "🔎 Resultados para el modelo 'Spacy Embedding':\n",
      "MSE:      0.0975\n",
      "RMSE:     0.3122\n",
      "MAE:      0.2658\n",
      "Pearson:  0.2863\n",
      "Spearman: 0.2975\n"
     ]
    }
   ],
   "source": [
    "# Evaluar\n",
    "D_s = evaluate_model(model_no_lin_con_s, x1_val_s, x2_val_s, y_val_s, \"Spacy Embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenemos uno de los peores resultados. Podemos inferir que el modelo de FastText es mucho más potente. <sb>\n",
    "\n",
    "Además podríamos decir que, en general, el promediado de vectores de palabras:\n",
    "- Pierde el orden de las palabras.\n",
    "- No modela interacciones sintácticas ni semánticas complejas.\n",
    "- Se ve afectado mucho si hay muchas palabras comunes o sin información."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RoBERTa fine-tuned por STS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo está pre-entrenado para maximizar la correlacion de Pearson y Searman média. Si antes las palabras tienían el mismo embedding en diferentes contextos, ahora se entrena un modelo específico para captar similitudes entre oraciones del conjunto de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs_ = [(item['sentence_1'], item['sentence_2']) for item in val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se modifica el codigo propuesto en HuggingFase por causa de problemas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def version_corregida_huggingface(sentence_pairs):\n",
    "    model = 'projecte-aina/roberta-base-ca-v2-cased-sts'\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "    pipe = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "    \n",
    "    # Preparar pares usando el tokenizer correctamente\n",
    "    texts = [f\"{s1} [SEP] {s2}\" for s1, s2 in sentence_pairs]\n",
    "    predictions = pipe(texts)\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "predicted_scores = version_corregida_huggingface(sentence_pairs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_scores = [item['score'] for item in predicted_scores]\n",
    "true_scores = [item['label'] for item in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3242\n",
      "MAE: 0.4224\n",
      "Pearson correlation: 0.7496\n",
      "Spearman correlation: 0.7304\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "# Calcular errores\n",
    "mse = mean_squared_error(true_scores, predicted_scores)\n",
    "mae = mean_absolute_error(true_scores, predicted_scores)\n",
    "\n",
    "# Calcular correlaciones\n",
    "pearson_corr, _ = pearsonr(true_scores, predicted_scores)\n",
    "spearman_corr, _ = spearmanr(true_scores, predicted_scores)\n",
    "\n",
    "# Mostrar resultados\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"Pearson correlation: {pearson_corr:.4f}\")\n",
    "print(f\"Spearman correlation: {spearman_corr:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El fine-tunnig mejora muy significativamente el rendimiento del modelo. Este es el modelo con el mejor rendimiento que obtenemos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RoBERTa reentrenado con coseno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante el entrenamiento del modelo RoBERTa podemos añadir la similitud de coseno como la predicción de similitud de las frases. Esto permite crear un embeding donde la dirección representa el significado de la frase, y la mejor forma de medirlo es con el coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs_train = [(item['sentence_1'], item['sentence_2']) for item in train]\n",
    "true_scores_train = [item['label']/5 for item in train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equilibramos el dataset de entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.hist(true_scores_train, bins=20)\n",
    "plt.title(\"Distribución de etiquetas reales\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a numpy\n",
    "scores = np.array(true_scores_train)\n",
    "pairs = np.array(sentence_pairs_train)\n",
    "\n",
    "# Seleccionar ejemplos por rangos\n",
    "mask_low = (scores <= 0.3)\n",
    "mask_mid = (scores > 0.3) & (scores <= 0.7)\n",
    "mask_high = (scores > 0.7)\n",
    "\n",
    "# Balancear\n",
    "n = min(mask_low.sum(), mask_mid.sum(), mask_high.sum())  # Tamaño mínimo de los grupos\n",
    "\n",
    "# Tomar n ejemplos de cada grupo\n",
    "balanced_pairs = np.concatenate([\n",
    "    pairs[mask_low][:n],\n",
    "    pairs[mask_mid][:n],\n",
    "    pairs[mask_high][:n]\n",
    "])\n",
    "balanced_scores = np.concatenate([\n",
    "    scores[mask_low][:n],\n",
    "    scores[mask_mid][:n],\n",
    "    scores[mask_high][:n]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at projecte-aina/roberta-base-ca-v2-cased-sts and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Dataset\n",
    "\n",
    "class SimilarityDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para pares de frases y sus similitudes.\n",
    "    \"\"\"\n",
    "    def __init__(self, pairs: List[Tuple[str, str]], similarities: List[float], tokenizer: Any, max_len: int = 128):\n",
    "        self.pairs = pairs\n",
    "        self.similarities = similarities\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> dict:\n",
    "        sent1, sent2 = self.pairs[idx]\n",
    "        similarity = self.similarities[idx]\n",
    "\n",
    "        encoded1 = self.tokenizer(sent1, padding='max_length', truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors='pt')\n",
    "        encoded2 = self.tokenizer(sent2, padding='max_length', truncation=True,\n",
    "                                  max_length=self.max_len, return_tensors='pt')\n",
    "\n",
    "        return {\n",
    "            'input_ids1': encoded1['input_ids'].squeeze(0),\n",
    "            'attention_mask1': encoded1['attention_mask'].squeeze(0),\n",
    "            'input_ids2': encoded2['input_ids'].squeeze(0),\n",
    "            'attention_mask2': encoded2['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(similarity, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# 2. Modelo con pooling + comparación\n",
    "class SimilarityModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo de similitud basado en un encoder de Transformers.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    def forward(self, input_ids1: torch.Tensor, attention_mask1: torch.Tensor,\n",
    "                input_ids2: torch.Tensor, attention_mask2: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        out1 = self.encoder(input_ids=input_ids1, attention_mask=attention_mask1)\n",
    "        out2 = self.encoder(input_ids=input_ids2, attention_mask=attention_mask2)\n",
    "\n",
    "        # Usamos el embedding del token [CLS]\n",
    "        emb1 = out1.last_hidden_state[:, 0, :]  # (batch_size, hidden_size)\n",
    "        emb2 = out2.last_hidden_state[:, 0, :]\n",
    "\n",
    "        return emb1, emb2\n",
    "\n",
    "# 3. Función de pérdida: Cosine Similarity Loss\n",
    "class CosineSimilarityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Función de pérdida basada en la similitud coseno y MSE.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.cos_sim = nn.CosineSimilarity(dim=1)\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, emb1: torch.Tensor, emb2: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n",
    "        sim = self.cos_sim(emb1, emb2)  # predicción\n",
    "        loss = self.mse(sim, labels)    # MSE entre predicción y similitud real\n",
    "        return loss\n",
    "\n",
    "# Preparación de tokenizer, dataset, dataloader, modelo, función de pérdida y optimizador\n",
    "tokenizer: Any = AutoTokenizer.from_pretrained('projecte-aina/roberta-base-ca-v2-cased-sts')\n",
    "train_dataset: SimilarityDataset = SimilarityDataset(balanced_pairs, balanced_scores, tokenizer)\n",
    "dataloader: DataLoader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "model: SimilarityModel = SimilarityModel('projecte-aina/roberta-base-ca-v2-cased-sts')\n",
    "loss_fn: CosineSimilarityLoss = CosineSimilarityLoss()\n",
    "optimizer: AdamW = AdamW(model.parameters(), lr=2e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 3.2327\n",
      "Epoch 2 - Loss: 2.0414\n",
      "Epoch 3 - Loss: 1.4310\n",
      "Epoch 4 - Loss: 1.1652\n",
      "Epoch 5 - Loss: 0.9914\n",
      "Epoch 6 - Loss: 0.6520\n",
      "Epoch 7 - Loss: 0.5043\n",
      "Epoch 8 - Loss: 0.4765\n",
      "Epoch 9 - Loss: 0.4092\n",
      "Epoch 10 - Loss: 0.3856\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "epochs = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        input_ids1 = batch['input_ids1'].to(device)\n",
    "        attention_mask1 = batch['attention_mask1'].to(device)\n",
    "        input_ids2 = batch['input_ids2'].to(device)\n",
    "        attention_mask2 = batch['attention_mask2'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        emb1, emb2 = model(input_ids1, attention_mask1, input_ids2, attention_mask2)\n",
    "        loss = loss_fn(emb1, emb2, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "\n",
    "# Guardar todo el modelo entrenado\n",
    "torch.save(model.state_dict(), './modelo_entrenado3.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con 10 epoch la loss sigue siendo grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_scores_val = [item['label']/5  for item in val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predecir:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at projecte-aina/roberta-base-ca-v2-cased-sts and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 1. Obtener las similitudes predichas\n",
    "model = SimilarityModel('projecte-aina/roberta-base-ca-v2-cased-sts')\n",
    "model.load_state_dict(torch.load('./modelo_entrenado3.pt', map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu')))\n",
    "model.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "predicciones = []\n",
    "for s1, s2 in sentence_pairs_:\n",
    "    encoded1 = tokenizer(s1, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "    encoded2 = tokenizer(s2, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "    for k in encoded1:\n",
    "        encoded1[k] = encoded1[k].to(device)\n",
    "        encoded2[k] = encoded2[k].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        emb1, emb2 = model(\n",
    "            input_ids1=encoded1['input_ids'],\n",
    "            attention_mask1=encoded1['attention_mask'],\n",
    "            input_ids2=encoded2['input_ids'],\n",
    "            attention_mask2=encoded2['attention_mask']\n",
    "        )\n",
    "        sim = F.cosine_similarity(emb1, emb2).item()\n",
    "        predicciones.append(sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 Correlación de Pearson: 0.2597\n",
      "📊 Valor-p: 3.7768e-09\n"
     ]
    }
   ],
   "source": [
    "correlacion, p_valor = pearsonr(predicciones, true_scores_val)\n",
    "print(f\"\\n📈 Correlación de Pearson: {correlacion:.4f}\")\n",
    "print(f\"📊 Valor-p: {p_valor:.4e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos encontramos con un problema de capacidad computacional. Lo mejor sería entrenar más el modelo, para obtener mejores resultados. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

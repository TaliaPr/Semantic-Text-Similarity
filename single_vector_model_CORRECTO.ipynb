{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Semantic Text Similarity\n",
    "Este modelo utiliza gensim para convertir pares de vectores + puntuaciones en vectores (word embeddings).\n",
    "Dado un dataset, infiere la puntuaci√≥n de similitud entre ambas frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:55:07.143009Z",
     "start_time": "2025-05-23T14:55:05.828592Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Requisitos\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:55:08.481500Z",
     "start_time": "2025-05-23T14:55:08.479325Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tipado\n",
    "from typing import Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:55:10.982179Z",
     "start_time": "2025-05-23T14:55:10.979566Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cargar stopwords en Catalan\n",
    "# STOPWORDS_CA = {\"a\", \"abans\", \"ac√≠\", \"ah\", \"aix√≠\", \"aix√≤\", \"al\", \"aleshores\", \"algun\", \"alguna\", \"algunes\", \"alguns\", \"alhora\", \"all√†\", \"all√≠\", \"all√≤\", \"als\", \"altra\", \"altre\", \"altres\", \"amb\", \"ambdues\", \"ambd√≥s\", \"anar\", \"ans\", \"apa\", \"aquell\", \"aquella\", \"aquelles\", \"aquells\", \"aquest\", \"aquesta\", \"aquestes\", \"aquests\", \"aqu√≠\", \"baix\", \"bastant\", \"b√©\", \"cada\", \"cadascuna\", \"cadascunes\", \"cadascuns\", \"cadasc√∫\", \"com\", \"consegueixo\", \"conseguim\", \"conseguir\", \"consigueix\", \"consigueixen\", \"consigueixes\", \"contra\", \"d'un\", \"d'una\", \"d'unes\", \"d'uns\", \"dalt\", \"de\", \"del\", \"dels\", \"des\", \"des de\", \"despr√©s\", \"dins\", \"dintre\", \"donat\", \"doncs\", \"durant\", \"e\", \"eh\", \"el\", \"elles\", \"ells\", \"els\", \"em\", \"en\", \"encara\", \"ens\", \"entre\", \"era\", \"erem\", \"eren\", \"eres\", \"es\", \"esta\", \"estan\", \"estat\", \"estava\", \"estaven\", \"estem\", \"esteu\", \"estic\", \"est√†\", \"est√†vem\", \"est√†veu\", \"et\", \"etc\", \"ets\", \"fa\", \"faig\", \"fan\", \"fas\", \"fem\", \"fer\", \"feu\", \"fi\", \"fins\", \"fora\", \"gaireb√©\", \"ha\", \"han\", \"has\", \"haver\", \"havia\", \"he\", \"hem\", \"heu\", \"hi\", \"ho\", \"i\", \"igual\", \"iguals\", \"incl√≤s\", \"ja\", \"jo\", \"l'hi\", \"la\", \"les\", \"li\", \"li'n\", \"llarg\", \"llavors\", \"m'he\", \"ma\", \"mal\", \"malgrat\", \"mateix\", \"mateixa\", \"mateixes\", \"mateixos\", \"me\", \"mentre\", \"meu\", \"meus\", \"meva\", \"meves\", \"mode\", \"molt\", \"molta\", \"moltes\", \"molts\", \"mon\", \"mons\", \"m√©s\", \"n'he\", \"n'hi\", \"ne\", \"ni\", \"no\", \"nogensmenys\", \"nom√©s\", \"nosaltres\", \"nostra\", \"nostre\", \"nostres\", \"o\", \"oh\", \"oi\", \"on\", \"pas\", \"pel\", \"pels\", \"per\", \"per que\", \"perqu√®\", \"per√≤\", \"poc\", \"poca\", \"pocs\", \"podem\", \"poden\", \"poder\", \"podeu\", \"poques\", \"potser\", \"primer\", \"propi\", \"puc\", \"qual\", \"quals\", \"quan\", \"quant\", \"que\", \"quelcom\", \"qui\", \"quin\", \"quina\", \"quines\", \"quins\", \"qu√®\", \"s'ha\", \"s'han\", \"sa\", \"sabem\", \"saben\", \"saber\", \"sabeu\", \"sap\", \"saps\", \"semblant\", \"semblants\", \"sense\", \"ser\", \"ses\", \"seu\", \"seus\", \"seva\", \"seves\", \"si\", \"sobre\", \"sobretot\", \"soc\", \"solament\", \"sols\", \"som\", \"son\", \"sons\", \"sota\", \"sou\", \"s√≥c\", \"s√≥n\", \"t'ha\", \"t'han\", \"t'he\", \"ta\", \"tal\", \"tamb√©\", \"tampoc\", \"tan\", \"tant\", \"tanta\", \"tantes\", \"te\", \"tene\", \"tenim\", \"tenir\", \"teniu\", \"teu\", \"teus\", \"teva\", \"teves\", \"tinc\", \"ton\", \"tons\", \"tot\", \"tota\", \"totes\", \"tots\", \"un\", \"una\", \"unes\", \"uns\", \"us\", \"va\", \"vaig\", \"vam\", \"van\", \"vas\", \"veu\", \"vosaltres\", \"vostra\", \"vostre\", \"vostres\", \"√©rem\", \"√©reu\", \"√©s\", \"√©ssent\", \"√∫ltim\", \"√∫s\"}\n",
    "STOPWORDS_CA = {\"a\", \"al\", \"el\", \"la\", \"els\", \"les\", \"de\", \"un\", \"una\", \"algun\", \"alguna\", }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:55:39.846622Z",
     "start_time": "2025-05-23T14:55:39.839815Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir funci√≥n de pre-procesado\n",
    "def preprocess(sentence: str) -> List[str]:\n",
    "    preprocessed = simple_preprocess(sentence) # Tokenizaci√≥n y normalizaci√≥n, lematizaci√≥n, min√∫sculas\n",
    "    # Eliminar stopwords\n",
    "    preprocessed = [token for token in preprocessed if token not in STOPWORDS_CA]\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:04.254349Z",
     "start_time": "2025-05-23T14:55:40.303605Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Modelos pre-entrenados\\n# WV_MODEL_PATH = \"/Users/salva/Downloads/cc.ca.300.bin.gz\"\\nWV_MODEL_PATH = \\'/Users/salva/Downloads/cc.ca.300.vec.gz\\'\\nimport gensim\\nwv_model =  gensim.models.KeyedVectors.load_word2vec_format(WV_MODEL_PATH, binary=False)\\nwv_model\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Modelos pre-entrenados\n",
    "# WV_MODEL_PATH = \"/Users/salva/Downloads/cc.ca.300.bin.gz\"\n",
    "WV_MODEL_PATH = '/Users/salva/Downloads/cc.ca.300.vec.gz'\n",
    "import gensim\n",
    "wv_model =  gensim.models.KeyedVectors.load_word2vec_format(WV_MODEL_PATH, binary=False)\n",
    "wv_model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.fasttext import FastTextKeyedVectors\n",
    "#cargar como map:\n",
    "wv_model = FastTextKeyedVectors.load('/home/taya/Desktop/cc.ca.gensim.bin', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:16.013377Z",
     "start_time": "2025-05-23T14:58:16.007686Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Ejemplo de 10 pares de oraciones con puntuaci√≥n de similitud asociada\\n#No se usa nunca en el codigo\\ninput_pairs = [\\n    ('M'agrada el futbol', 'Disfruto veient partits de futbol', 4),\\n    ('El cel est√† despejat', 'Fa un dia bonic', 4.5),\\n    ('M'encanta viatjar', 'Explorar nous llocs √©s una passi√≥', 3.5),\\n    ('Prefereixo l'estiu', 'No m'agrada el fred de l'hivern', 2.5),\\n    ('Tinc gana', 'Qu√® hi ha per sopar?', 2),\\n    ('La m√∫sica em relaxa', 'Escoltar m√∫sica √©s una ter√†pia', 3),\\n    ('El llibre √©s emocionant', 'No puc deixar de llegir-lo', 4),\\n    ('M'agrada la pizza', '√âs el meu menjar preferit', 4.5),\\n    ('Estic cansat', 'Necessito fer una migdiada', 1.5),\\n    ('Avui fa molta calor', '√âs un dia sofocant', 3.5)\\n    ]\\n\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Ejemplo de 10 pares de oraciones con puntuaci√≥n de similitud asociada\n",
    "#No se usa nunca en el codigo\n",
    "input_pairs = [\n",
    "    ('M\\'agrada el futbol', 'Disfruto veient partits de futbol', 4),\n",
    "    ('El cel est√† despejat', 'Fa un dia bonic', 4.5),\n",
    "    ('M\\'encanta viatjar', 'Explorar nous llocs √©s una passi√≥', 3.5),\n",
    "    ('Prefereixo l\\'estiu', 'No m\\'agrada el fred de l\\'hivern', 2.5),\n",
    "    ('Tinc gana', 'Qu√® hi ha per sopar?', 2),\n",
    "    ('La m√∫sica em relaxa', 'Escoltar m√∫sica √©s una ter√†pia', 3),\n",
    "    ('El llibre √©s emocionant', 'No puc deixar de llegir-lo', 4),\n",
    "    ('M\\'agrada la pizza', '√âs el meu menjar preferit', 4.5),\n",
    "    ('Estic cansat', 'Necessito fer una migdiada', 1.5),\n",
    "    ('Avui fa molta calor', '√âs un dia sofocant', 3.5)\n",
    "    ]\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:24.542260Z",
     "start_time": "2025-05-23T14:58:16.595919Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'sentence_1', 'sentence_2', 'label'],\n",
       "    num_rows: 3073\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Text Similarity (STS) dataset (principal per la Pr√†ctica 4)\n",
    "train = load_dataset(\"projecte-aina/sts-ca\", split=\"train\")\n",
    "test = load_dataset(\"projecte-aina/sts-ca\", split=\"test\")\n",
    "val = load_dataset(\"projecte-aina/sts-ca\", split=\"validation\")\n",
    "all_data = load_dataset(\"projecte-aina/sts-ca\", split=\"all\")\n",
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento y construccion del diccionario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:25.881487Z",
     "start_time": "2025-05-23T14:58:25.584735Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x7dd2186c7ec0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocesamiento de las oraciones y creaci√≥n del diccionario\n",
    "sentences_1_preproc = [simple_preprocess(d[\"sentence_1\"]) for d in all_data] #lista de listas que son oraciones lematizadas\n",
    "sentences_2_preproc = [simple_preprocess(d[\"sentence_2\"]) for d in all_data]\n",
    "scores = [d[\"label\"] for d in all_data]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc, scores))#lista de tuplas que son ([palabras or1], [pal or 2], score)\n",
    "# Versi√≥n aplanada para poder entrenar el modelo\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc #todas las oraciones juntas\n",
    "diccionario = Dictionary(sentences_pairs_flattened) # diccionario donde cada palabra tiene un indice unico\n",
    "diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:26.908268Z",
     "start_time": "2025-05-23T14:58:26.906394Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['atorga', 'per', 'primer', 'cop', 'les', 'mencions', 'encarna', 'sanahuja', 'la', 'inclusi√≥', 'de', 'la', 'perspectiva', 'de', 'g√®nere', 'en', 'doc√®ncia', 'universit√†ria'], ['creen', 'la', 'menci√≥', 'encarna', 'sanahuja', 'la', 'inclusi√≥', 'de', 'la', 'perspectiva', 'de', 'g√®nere', 'en', 'doc√®ncia', 'universit√†ria'], 3.5)\n"
     ]
    }
   ],
   "source": [
    "print(sentence_pairs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "construccion de la metriz TF-IDF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:58:28.003478Z",
     "start_time": "2025-05-23T14:58:27.934262Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# C√°lculo de los pesos TF-IDF para las oraciones pre-procesadas\n",
    "corpus = [diccionario.doc2bow(sent) for sent in sentences_pairs_flattened]\n",
    "\"\"\"\n",
    "Por ejemplo, si sent es ['hola', 'mundo', 'hola'], el resultado de diccionario.doc2bow(sent) podr√≠a ser [(0, 2), (1, 1)], donde 0 es el √≠ndice de \"hola\" y 1 es el √≠ndice de \"mundo\", indicando que \"hola\" aparece 2 veces y \"mundo\" aparece 1 vez.\n",
    "corpus = El resultado es una lista de representaciones de bolsa de palabras, donde cada elemento corresponde a una oraci√≥n en el conjunto de datos.\n",
    "\"\"\"\n",
    "modelo_tfidf = TfidfModel(corpus) #transformar el corpus en una representaci√≥n que refleja la importancia de las palabras en cada documento en relaci√≥n con el corpus completo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelos de dimensi√≥n reducida:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_model_50d = {\n",
    "    word: wv_model[word][:50]\n",
    "    for word in wv_model.index_to_key\n",
    "}\n",
    "\n",
    "wv_model_100d = {\n",
    "    word: wv_model[word][:100]\n",
    "    for word in wv_model.index_to_key\n",
    "}\n",
    "wv_model_150d = {\n",
    "    word: wv_model[word][:150]\n",
    "    for word in wv_model.index_to_key\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aregaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:59:25.966982Z",
     "start_time": "2025-05-23T14:59:25.958556Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def map_tf_idf(sentence_preproc: List[str], dictionary: Dictionary, tf_idf_model: TfidfModel, model = wv_model) -> Tuple[List[np.ndarray], List[float]]:\n",
    "    \"\"\"\n",
    "    lo que hace es que coge una oracion preprocesada, para cada palabra saca sus pesos TF-IDF y su vector en el embeding\n",
    "    \"\"\"\n",
    "    bow = dictionary.doc2bow(sentence_preproc)#cuenta la frecuencia de cada palabra en la oracion\n",
    "    tf_idf = tf_idf_model[bow] \n",
    "    vectors, weights = [], []\n",
    "    for word_index, weight in tf_idf:\n",
    "        word = dictionary.get(word_index)\n",
    "        if word in model:\n",
    "            vectors.append(model[word])\n",
    "            weights.append(weight)\n",
    "    return vectors, weights\n",
    "\n",
    "def map_pairs(wv_model2, sentence_pairs: List[Tuple[str, str, float]],dictionary: Dictionary = None, tf_idf_model: TfidfModel = None,) -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    \"\"\"\n",
    "    Mapea los tripletes de oraciones a listas de (x, y), (pares de vectores, score)\n",
    "    :param sentence_pairs: lista de tuplas que son ([palabras or1], [palabras or2], score)\n",
    "    :param dictionary: diccionario donde cada palabra tiene un indice unico\n",
    "    :param tf_idf_model: objeto TfidfModel que da los pesos de las palabras (se puede indexar con un bag of words)\n",
    "    :return: lista de ((vector1, vector2), similitud), donde vector1 y vector2 cambian en funcion de:\n",
    "        si tf_idf_model is not None:\n",
    "                para cada elemento de sentence_pairs devuelve el vector embeding promediado de manera ponderada por los pesos de la matriz TF-IDF de las palabras de las oraciones 1 y 2.\n",
    "        si tf_idf_model is not None\n",
    "            el promedio de los vectores de embeding de las palabras que componen cada una de las oraciones\n",
    "    \"\"\"\n",
    "    # Mapeo de los pares de oraciones a pares de vectores\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        sentence_1_preproc = preprocess(sentence_1) if isinstance(sentence_1, str) else sentence_1 # se procesa el texto antes de aplicar map_pairs entonces sentence_1 es una lista de tokens y ya nose vuelve a preprocesar\n",
    "        sentence_2_preproc = preprocess(sentence_2) if isinstance(sentence_2, str) else sentence_2\n",
    "        # Si usamos TF-IDF\n",
    "        if tf_idf_model is not None:\n",
    "            # C√°lculo del promedio ponderado por TF-IDF de los word embeddings\n",
    "            vectors1, weights1 = map_tf_idf(sentence_1_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model,model =  wv_model2, )\n",
    "            vectors2, weights2 = map_tf_idf(sentence_2_preproc, dictionary=dictionary, tf_idf_model=tf_idf_model, model = wv_model2 )\n",
    "            vector1 = np.average(vectors1, weights=weights1, axis=0, ) #Esta funci√≥n calcula el promedio de un conjunto de valores. Si se proporciona un argumento weights, el promedio se calcula de manera ponderada, lo que significa que cada valor contribuye al promedio de acuerdo con su peso correspondiente.\n",
    "            vector2 = np.average(vectors2, weights=weights2, axis=0, )\n",
    "        else:\n",
    "            # C√°lculo del promedio de los word embeddings\n",
    "            vectors1 = [wv_model2[word] for word in sentence_1_preproc if word in wv_model2]\n",
    "            vectors2 = [wv_model2[word] for word in sentence_2_preproc if word in wv_model2]\n",
    "            vector1 = np.mean(vectors1, axis=0)\n",
    "            vector2 = np.mean(vectors2, axis=0)\n",
    "        # A√±adir a la lista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([-7.22131877e-03, -4.88683421e-03,  2.71017708e-02,  2.32332627e-02,\n",
       "         -9.60097482e-03, -3.16095435e-03,  2.90225599e-02, -1.75413820e-02,\n",
       "          2.93095319e-02, -1.60574403e-02, -6.04936805e-03,  1.49908545e-02,\n",
       "          1.00934507e-02,  1.84449753e-02,  2.16156266e-02,  2.13238810e-02,\n",
       "          8.69774586e-03,  6.13958934e-02,  2.18719114e-02,  1.01426407e-02,\n",
       "          1.16837708e-02,  3.24588305e-03, -1.32856906e-02,  5.77104877e-02,\n",
       "          1.54627187e-02,  2.13443526e-02, -3.82471218e-02,  6.23983637e-03,\n",
       "          7.31161386e-04,  8.99225741e-03, -4.87626204e-03,  1.08773269e-02,\n",
       "          1.30313145e-02, -3.86450925e-03,  7.23370200e-03, -1.75266524e-02,\n",
       "         -9.09778208e-03,  4.43412138e-02, -4.31998409e-04,  6.25044253e-04,\n",
       "         -1.13920750e-02, -1.82465011e-02, -8.11444328e-03, -8.18518457e-03,\n",
       "         -3.54176235e-03, -1.60820262e-01,  7.74505797e-03,  9.80261699e-03,\n",
       "          8.53058034e-03, -1.23878019e-02,  1.24134202e-02, -2.39070017e-03,\n",
       "         -3.17817092e-02, -6.78023514e-03, -1.43296539e-02,  2.57246362e-02,\n",
       "          2.62315431e-02,  1.36448970e-02, -9.16338087e-03,  1.39765626e-02,\n",
       "          4.86506819e-02, -3.05702791e-02, -1.73943639e-02, -2.01844855e-02,\n",
       "          3.91634927e-02,  1.06170025e-03, -2.25761531e-02, -2.53614495e-02,\n",
       "          1.06654209e-02,  3.99356146e-02,  1.90717986e-02,  5.49600371e-03,\n",
       "          2.08872278e-02,  1.34323488e-02,  1.14136353e-02, -2.26947543e-02,\n",
       "         -2.89445889e-02,  3.63149983e-03, -3.17429208e-02, -1.86948783e-02,\n",
       "          7.58524843e-03, -2.51519190e-02,  1.59076956e-02, -1.57653595e-02,\n",
       "         -1.32853988e-02,  1.84268937e-02,  1.35733292e-02, -1.35745752e-02,\n",
       "         -2.62841402e-02, -1.71081547e-03, -6.57123376e-02,  1.80123245e-02,\n",
       "          2.18237573e-02,  4.92439771e-03, -4.24249468e-03,  1.44695216e-02,\n",
       "          1.61562532e-02, -5.86160831e-03,  1.07315954e-02,  2.03850600e-02,\n",
       "         -9.68246967e-03, -9.98037382e-03,  6.31454679e-07, -2.24527261e-02,\n",
       "          5.15347347e-03, -1.18663279e-02, -4.56879624e-03,  4.84564375e-02,\n",
       "          5.10890407e-03, -1.10297034e-03, -5.12788351e-03, -1.27865401e-02,\n",
       "          1.69212758e-02, -2.37873653e-04,  7.97397964e-03, -1.12930849e-02,\n",
       "         -4.02350222e-04, -1.98382137e-02,  2.10519730e-03, -1.94884122e-03,\n",
       "         -1.41923877e-02,  1.53408167e-02, -3.14154498e-02,  5.16255789e-03,\n",
       "          6.93068941e-03,  1.27301166e-02,  2.95631792e-03, -1.99976450e-02,\n",
       "         -1.29251593e-02,  2.83612074e-02, -1.00102755e-02, -3.59831313e-03,\n",
       "          9.12615437e-03,  3.19957247e-02, -2.39536587e-03, -1.88541424e-03,\n",
       "          1.71505670e-02,  6.10816907e-02, -2.02739033e-02,  3.70995447e-04,\n",
       "         -1.79623004e-02, -9.18574379e-04,  2.46088662e-02, -5.23656944e-03,\n",
       "         -3.28713545e-02, -1.66103211e-02, -2.97392304e-02,  1.05556641e-02,\n",
       "          1.15840674e-02,  5.76075530e-02,  5.02314960e-02,  2.17064867e-02,\n",
       "          8.58374305e-03, -6.60827014e-02, -4.82426792e-03, -1.56691531e-02,\n",
       "          2.12149921e-02, -2.74767225e-02, -1.23504192e-02,  1.28560904e-02,\n",
       "          2.30588782e-02, -1.09581482e-02,  6.07751199e-03,  1.72556543e-02,\n",
       "         -1.97553865e-03,  1.22152828e-02,  1.04813740e-02,  1.40103719e-02,\n",
       "          5.37980764e-03, -5.76685295e-02,  1.36694746e-02, -2.36321303e-02,\n",
       "         -8.50384182e-03,  3.60939922e-02, -2.06882135e-03,  1.76951758e-02,\n",
       "          1.57728072e-02,  2.88058438e-02, -8.86882738e-03,  6.88743752e-02,\n",
       "          6.25128593e-03,  3.75693638e-03,  3.87385404e-05, -1.95502007e-02,\n",
       "         -5.08330865e-03, -6.56666336e-03, -4.60735302e-03, -1.69378997e-02,\n",
       "          2.03075495e-02, -4.75446082e-02, -2.89494102e-02, -7.66059492e-03,\n",
       "         -2.24331713e-03,  8.62236897e-02, -2.80728569e-02, -4.31060661e-03,\n",
       "          1.31037543e-02,  6.93664039e-03,  8.79472834e-03,  5.53005787e-02,\n",
       "         -3.09062304e-03, -1.42629341e-02, -2.72252156e-02,  1.37947659e-02,\n",
       "         -3.86245701e-03, -1.68961022e-03,  1.90346599e-02, -1.98370433e-02,\n",
       "          2.21129468e-04,  8.48248098e-03,  1.53173742e-02, -4.47935110e-02,\n",
       "          5.90692373e-02,  4.12743244e-03,  2.69058790e-02, -7.84230922e-03,\n",
       "         -1.00485311e-02,  3.48163964e-03, -7.07877696e-03, -8.69120567e-03,\n",
       "         -1.09471734e-02, -5.26084500e-03, -8.29154383e-03,  1.45321145e-02,\n",
       "          1.17931868e-02,  1.51625271e-03,  5.06379921e-05,  3.64201031e-02,\n",
       "         -5.98440757e-03,  4.41127321e-03, -2.08663335e-02, -1.70784310e-02,\n",
       "         -2.18255851e-02,  6.00788341e-03,  5.07507834e-03, -1.38918141e-02,\n",
       "         -6.91033048e-03,  1.14552128e-02,  6.34593222e-02, -1.32782786e-02,\n",
       "         -1.52115628e-02,  4.98272376e-02,  1.72577717e-02, -1.29923328e-02,\n",
       "         -1.60627377e-02,  4.10427215e-02, -2.45433983e-03,  5.57846760e-03,\n",
       "         -2.11911409e-02, -2.09040772e-03,  4.27414873e-02,  2.59594736e-02,\n",
       "         -2.21033701e-02, -9.22074085e-03,  6.56417001e-03, -6.15493147e-03,\n",
       "         -6.36646366e-03, -2.16802575e-02,  2.19316476e-02, -1.32581929e-03,\n",
       "         -8.58274602e-03,  1.45549130e-03, -1.33940025e-02, -4.33783476e-03,\n",
       "          9.55339827e-03, -2.29094632e-02, -3.11402989e-03, -1.97023859e-02,\n",
       "          2.13536615e-03, -1.45499117e-02,  2.32157588e-02, -2.51983588e-02,\n",
       "         -2.26461076e-02,  9.16354896e-03,  2.66498964e-02, -1.95218307e-02,\n",
       "         -5.17713434e-02, -2.78031737e-02, -2.49143180e-02,  5.64633386e-03,\n",
       "         -1.70476468e-02,  1.34696997e-02, -5.37980768e-03, -2.86228522e-02,\n",
       "          1.15971552e-02,  1.62302861e-02, -1.50279653e-02, -3.47484244e-02,\n",
       "         -2.68725549e-02, -6.37603429e-03,  1.05356869e-04, -1.25608841e-02,\n",
       "         -4.91036526e-03, -5.21338575e-04,  4.98603459e-02, -1.67891928e-03,\n",
       "         -2.89849270e-03, -2.52519604e-02,  1.43659082e-02, -1.06123175e-02]),\n",
       "  array([-6.83419957e-03, -1.18342274e-02,  2.81851265e-02,  1.56639266e-02,\n",
       "         -8.56577435e-04, -8.51332926e-04,  1.97378555e-02, -1.29571395e-02,\n",
       "          4.93545554e-02, -2.41754346e-02, -6.15790577e-03,  2.05138671e-02,\n",
       "          1.93670358e-02,  2.11652259e-02,  2.54499821e-02,  1.63368237e-02,\n",
       "          4.16049481e-03,  3.83112881e-02,  2.22273047e-02,  4.62837957e-03,\n",
       "          8.80235256e-03, -2.01674838e-03, -1.16806213e-02,  4.86109168e-02,\n",
       "          1.98380204e-02, -1.89877654e-03, -4.72341870e-02, -4.54726101e-03,\n",
       "         -7.67185251e-03, -5.59219061e-05, -9.20897783e-03,  7.59127691e-03,\n",
       "          7.42895688e-04, -6.14518295e-03,  1.84238488e-02, -2.73708592e-02,\n",
       "          3.67920039e-03,  5.15129104e-02,  4.80286734e-03, -9.51228594e-03,\n",
       "         -1.11621336e-02, -9.42123322e-03, -4.36669167e-03, -1.52258050e-02,\n",
       "          9.90805424e-04, -1.54695765e-01,  1.73456723e-02,  4.71676183e-03,\n",
       "          4.02265375e-03, -2.22887292e-02, -4.60747433e-03,  3.95980001e-03,\n",
       "         -4.49414413e-02, -1.66012033e-02, -1.11012129e-02,  2.86654471e-02,\n",
       "          2.77660979e-02,  2.29358544e-02, -9.27368574e-03,  1.76946555e-02,\n",
       "          3.86702123e-02, -2.49444389e-02,  8.16866338e-04, -1.31154861e-02,\n",
       "          4.54887142e-02,  1.82431527e-03, -9.26950632e-03, -1.15554111e-02,\n",
       "          2.80840740e-02,  1.72604205e-02,  5.92232558e-03,  1.80038366e-02,\n",
       "          4.91635839e-02,  2.28532993e-02,  6.92838741e-03, -3.29340996e-02,\n",
       "         -2.60844809e-02, -3.53558118e-03, -2.37629223e-02, -1.33396815e-02,\n",
       "          3.39014677e-02, -2.58249188e-02,  1.58621179e-02, -1.38280199e-02,\n",
       "         -1.94001082e-02,  3.23780085e-02,  8.73536972e-03, -1.06491400e-02,\n",
       "         -2.06113311e-02, -1.00204539e-02, -5.92414899e-02,  2.21567742e-02,\n",
       "          1.96703621e-02,  4.83981773e-03,  4.64754394e-03,  1.72329994e-02,\n",
       "          3.77878697e-03, -7.31476285e-03,  2.15761497e-02,  2.36946578e-02,\n",
       "         -3.94739185e-04, -3.15822189e-03,  6.27627253e-03, -1.63456528e-02,\n",
       "         -2.86408123e-03, -1.34084593e-02, -2.05908982e-02,  3.79488378e-02,\n",
       "         -2.59135821e-03, -5.99367373e-03, -5.48361672e-03, -2.25192984e-02,\n",
       "          1.53839963e-02, -1.96333146e-03,  1.04385230e-02, -9.95364947e-03,\n",
       "         -9.15884628e-04, -5.44193423e-03, -2.63560402e-03, -1.20995468e-02,\n",
       "         -1.94892631e-02,  5.10339449e-03, -3.99716956e-02,  6.63766081e-03,\n",
       "          2.83341507e-02,  1.61479897e-02, -6.24960469e-03, -2.01853115e-02,\n",
       "         -2.30493463e-02,  2.41974686e-02, -1.18435264e-02,  3.91196668e-03,\n",
       "          1.57427261e-02,  3.39905907e-02,  1.78401215e-02, -5.43773888e-03,\n",
       "          1.96007225e-02,  8.70553736e-02, -2.09045670e-02,  5.44575620e-03,\n",
       "         -1.74754817e-02, -6.31366838e-03,  4.67595758e-03,  1.15769349e-03,\n",
       "         -2.48051513e-02, -1.49467794e-02,  8.50755240e-03,  3.35060067e-02,\n",
       "          3.74447485e-03,  4.09239615e-02,  3.32012232e-02,  3.43022553e-02,\n",
       "          1.95873175e-03, -4.23116064e-02, -1.78572984e-03, -2.38531207e-02,\n",
       "          2.10662950e-03, -2.19738587e-02, -1.20522812e-02, -3.03089888e-03,\n",
       "          2.11344370e-02, -8.71930295e-03,  7.29551984e-03,  1.66129700e-02,\n",
       "         -1.27604342e-02,  9.56745217e-03,  6.98299118e-03,  2.53704170e-02,\n",
       "         -4.42790063e-03, -3.90480910e-02,  8.54313270e-03, -2.50870639e-02,\n",
       "         -3.21222553e-05,  1.70423389e-02,  1.84731972e-02,  2.96567194e-03,\n",
       "          2.61907966e-02,  1.49996948e-02, -2.33189960e-02,  6.46570743e-02,\n",
       "          3.11559877e-03,  2.37721322e-02,  1.93601322e-02, -2.57546712e-02,\n",
       "         -1.83501803e-02, -1.41467092e-02, -3.99285842e-03, -1.79875420e-02,\n",
       "          1.90183532e-02, -2.44178147e-02, -1.57051930e-02, -2.09854681e-02,\n",
       "          1.37791918e-03,  7.44763341e-02, -1.74727091e-02,  3.67230073e-03,\n",
       "          1.56129086e-02,  6.37680905e-04,  1.01134477e-02,  2.87668182e-02,\n",
       "          2.03147982e-02, -1.17364094e-02, -1.45116558e-02,  6.01164431e-03,\n",
       "         -7.51482784e-03, -1.18620916e-02,  1.71493723e-02, -1.25271817e-02,\n",
       "         -1.03429024e-02, -4.95697306e-03,  3.03381939e-02, -3.41038112e-02,\n",
       "          4.47540520e-02,  1.99627772e-02,  1.54856961e-02, -1.28739820e-02,\n",
       "         -1.37375703e-02,  7.22146110e-03, -1.12781470e-02, -3.54985917e-03,\n",
       "         -1.46177715e-02,  1.68767010e-02, -1.02305389e-03,  4.97828967e-03,\n",
       "         -4.69090611e-03, -7.10695948e-03,  5.32717242e-03,  2.88679476e-02,\n",
       "         -2.48944984e-03, -3.63596450e-04, -1.58664227e-02, -2.27195900e-02,\n",
       "         -1.06893757e-02, -1.09062554e-03,  1.24194070e-02,  1.34343369e-02,\n",
       "          2.25927407e-03,  5.54371125e-03,  6.09565856e-02, -9.14932218e-03,\n",
       "         -1.07793950e-02,  4.92963797e-02,  1.36362660e-02, -1.14051405e-02,\n",
       "         -1.74310632e-02,  5.70965599e-02, -1.19615038e-02,  1.05203062e-02,\n",
       "         -4.02353531e-03,  5.68556388e-03,  5.59104681e-02,  1.28396213e-02,\n",
       "         -1.06977573e-02, -1.08159224e-02, -1.46696972e-02, -9.03286637e-03,\n",
       "         -3.02932333e-04, -5.26778925e-04,  2.86689934e-02,  2.36106037e-03,\n",
       "         -4.98861461e-03,  6.93722282e-03, -7.73450376e-03, -8.31161353e-03,\n",
       "          7.77818995e-03, -6.43278433e-03, -1.27221542e-02,  1.05815064e-02,\n",
       "          2.84699066e-03, -2.10621337e-02, -3.27117528e-03, -2.31504872e-02,\n",
       "         -4.80825505e-03, -1.02471190e-02,  6.89615496e-03, -2.43042988e-02,\n",
       "         -1.73822640e-02, -2.59426005e-02, -2.67280864e-02,  1.26821842e-02,\n",
       "          1.60994342e-03,  3.71996485e-03, -5.48653625e-03, -1.88920388e-02,\n",
       "          1.97826725e-02,  4.47507107e-03, -1.87621528e-02, -2.91604958e-02,\n",
       "         -1.39914642e-02, -1.30793378e-02, -9.50955897e-04, -3.02010468e-02,\n",
       "          4.11583971e-03, -7.42575856e-03,  3.74844929e-02, -1.16436960e-02,\n",
       "         -1.95666894e-02, -1.67785357e-02,  2.49638147e-02, -8.38376810e-03])),\n",
       " 3.5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imprimir los pares de vectores y la puntuaci√≥n de similitud asociada\n",
    "mapped_no_tfidf = map_pairs(wv_model, sentence_pairs, tf_idf_model=None, dictionary=diccionario, )\n",
    "mapped = map_pairs(wv_model,sentence_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De dimensi√≥n reducida: <sb>\n",
    "\n",
    "s√≠ usando pesos TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_50 = map_pairs(wv_model_50d,sentence_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_100 = map_pairs(wv_model_100d,sentence_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario, )\n",
    "mapped_150 = map_pairs(wv_model_150d,sentence_pairs, tf_idf_model=modelo_tfidf, dictionary=diccionario, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300,)\n",
      "(300,)\n",
      "(50,)\n",
      "(100,)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(mapped_no_tfidf[0][0][0].shape)\n",
    "print(mapped[0][0][0].shape)\n",
    "print(mapped_50[0][0][0].shape)\n",
    "print(mapped_100[0][0][0].shape)\n",
    "print(mapped_150[0][0][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diferentes modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:59:33.852866Z",
     "start_time": "2025-05-23T14:59:29.286142Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir el Modelo\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_and_compile_model(hidden_size: int = 128, embedding_size: int = 300, learning_rate: float = 0.001) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Esto crea una red neuronal de manera que al entrenarla las distancias coseno cuadren con la etiqueta real\n",
    "    hidden_size: Tama√±o de capas ocultas (no se usa en este c√≥digo)\n",
    "    embedding_size: Dimensi√≥n de los vectores de entrada (300)\n",
    "    learning_rate: Tasa de aprendizaje para el optimizador\n",
    "    \"\"\"\n",
    "    # Capa de entrada para los pares de vectores\n",
    "    input_1 = tf.keras.Input(shape=(embedding_size,)) #los pares de vectores a comparar\n",
    "    input_2 = tf.keras.Input(shape=(embedding_size,))\n",
    "\n",
    "    # Capa oculta, con funcion de activacion lineal, tiene como objetivo proyectar los vectores de entrada en un nuevo espacio.\n",
    "    \"\"\"\n",
    "    La capa oculta (en este caso, la capa densa) tiene pesos que se ajustan durante el entrenamiento.\n",
    "    Estos pesos son los que transforman los vectores de entrada en los vectores proyectados\n",
    "    \"\"\"\n",
    "    first_projection = tf.keras.layers.Dense(\n",
    "        embedding_size,\n",
    "        # activation='tanh',\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),# inicializa los pesos de la capa como una matriz identidad\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "    )\n",
    "    #aplica la capa de proyeccion a los dos vectores de entrada\n",
    "    projected_1 = first_projection(input_1)\n",
    "    projected_2 = first_projection(input_2)\n",
    "    \"\"\"\n",
    "    # Compute the cosine distance\n",
    "    projected_1 = tf.linalg.l2_normalize(projected_1, axis=1, ) #Normaliza ambos vectores para que tengan magnitud 1, necesario para el c√°lculo de similitud coseno\n",
    "    projected_2 = tf.linalg.l2_normalize(projected_2, axis=1, )\n",
    "    output = 2.5 * (1.0 + tf.reduce_sum(projected_1 * projected_2, axis=1, ))\n",
    "    \"\"\" \n",
    "    #lo comentado es del profe y no va. Esto es del Chat############################################################################################\n",
    "    normalize = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis=1))\n",
    "    projected_1 = normalize(projected_1)\n",
    "    projected_2 = normalize(projected_2)\n",
    "    output = tf.keras.layers.Lambda(lambda tensors: 2.5 * (1.0 + tf.reduce_sum(tensors[0] * tensors[1], axis=1)))([projected_1, projected_2])\n",
    "    ############################################################################################################################################\n",
    "    # Definir el modelo con las capas de entrada y salida\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output) #Durante el entrenamiento, Keras ajusta los pesos de la capa oculta para minimizar la funci√≥n de p√©rdida definida (en este caso, el error absoluto medio).\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(loss='mean_absolute_error',\n",
    "                  optimizer=tf.keras.optimizers.Adam(learning_rate))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ValueError: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.ops`). You are likely doing something like:\n",
    "\n",
    "```\n",
    "x = Input(...)\n",
    "...\n",
    "tf_fn(x)  # Invalid.\n",
    "```\n",
    "\n",
    "What you should do instead is wrap `tf_fn` in a layer:\n",
    "\n",
    "```\n",
    "class MyLayer(Layer):\n",
    "    def call(self, x):\n",
    "        return tf_fn(x)\n",
    "\n",
    "x = MyLayer()(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En las transparencias para el modelo 1 pone este codigo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_aggregated(embedding_dim: int, hidden_size: int = 128, dropout_rate: float = 0.3) -> tf.keras.Model:\n",
    "    input_1 = tf.keras.Input(shape=(embedding_dim,), name=\"input_vector_1\")\n",
    "    input_2 = tf.keras.Input(shape=(embedding_dim,), name=\"input_vector_2\")\n",
    "    concatenated = tf.keras.layers.Concatenate(axis=-1)([input_1, input_2])\n",
    "    x = tf.keras.layers.BatchNormalization()(concatenated)\n",
    "    x = tf.keras.layers.Dense(hidden_size, activation='relu')(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    output = tf.keras.layers.Dense(1)(x) # Activaci√≥ lineal per a regressi√≥\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output)\n",
    "    model.compile(loss='mean_squared_error', optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "                  metrics=['mae', tf.keras.metrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "#model_agg.fit([X1_train, X2_train], Y_train, epochs=..., batch_size=...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Modelo build_model_aggregated: Concatenaci√≥n + Red Neuronal Densa\n",
    "üèóÔ∏è Arquitectura:\n",
    "\n",
    "    Toma dos vectores de entrada (input_vector_1, input_vector_2).\n",
    "\n",
    "    Los concatena (Concatenate), por lo que la dimensi√≥n del vector combinado es el doble del embedding_dim.\n",
    "\n",
    "    Aplica:\n",
    "\n",
    "        BatchNormalization\n",
    "\n",
    "        Dense con ReLU\n",
    "\n",
    "        Otro BatchNormalization\n",
    "\n",
    "        Dropout\n",
    "\n",
    "        Una capa de salida densa sin activaci√≥n (regresi√≥n lineal).\n",
    "\n",
    "    Se entrena con MSE (Mean Squared Error).\n",
    "\n",
    "üßæ Objetivo impl√≠cito:\n",
    "\n",
    "    Aprender una funci√≥n no lineal entre los vectores concatenados y la puntuaci√≥n objetivo (p. ej., similitud STS, afinidad, etc.).\n",
    "\n",
    "    Aprende una transformaci√≥n compleja basada en composici√≥n conjunta de los dos vectores.\n",
    "\n",
    "üìå Ventajas:\n",
    "\n",
    "    Flexibilidad para aprender patrones complejos.\n",
    "\n",
    "    Permite capturar interacciones no lineales entre los dos vectores.\n",
    "\n",
    "‚ùó Consideraciones:\n",
    "\n",
    "    Puede sobreajustarse si el dataset es peque√±o.\n",
    "\n",
    "    Requiere m√°s par√°metros, por lo tanto m√°s datos para entrenar bien.\n",
    "\n",
    "üß† build_and_compile_model: Proyecci√≥n + Similitud Coseno\n",
    "üèóÔ∏è Arquitectura:\n",
    "\n",
    "    Aplica una capa densa compartida (misma proyecci√≥n) a cada vector por separado. Inicialmente es una matriz identidad.\n",
    "\n",
    "    Los normaliza a magnitud 1.\n",
    "\n",
    "    Calcula la similitud coseno como cos(Œ∏) = dot(product).\n",
    "\n",
    "    La salida es 2.5 * (1 + similitud_coseno), lo cual transforma el rango [-1, 1] a [0, 5].\n",
    "\n",
    "    Se entrena con MAE (Mean Absolute Error).\n",
    "\n",
    "üßæ Objetivo impl√≠cito:\n",
    "\n",
    "    Aprender una proyecci√≥n donde la similitud coseno refleje la puntuaci√≥n deseada (por ejemplo, cu√°n similares son dos textos o frases).\n",
    "\n",
    "    Optimiza directamente sobre una funci√≥n interpretable (similitud coseno), √∫til para tareas tipo semantic textual similarity (STS).\n",
    "\n",
    "üìå Ventajas:\n",
    "\n",
    "    Muy interpretativo.\n",
    "\n",
    "    M√°s simple y menos propenso a sobreajuste.\n",
    "\n",
    "    Funciona bien cuando la relaci√≥n entre embeddings y similitud es principalmente angular (coseno).\n",
    "\n",
    "‚ùó Consideraciones:\n",
    "\n",
    "    Menor capacidad expresiva que el Modelo 1.\n",
    "\n",
    "    Asume que la similitud se puede modelar bien con una proyecci√≥n lineal + coseno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_and_compile_model2(embedding_size: int = 300, learning_rate: float = 0.001) -> tf.keras.Model:\n",
    "    # Input layer\n",
    "    input_1 = tf.keras.Input(shape=(embedding_size,), name=\"input_vector_1\")\n",
    "    input_2 = tf.keras.Input(shape=(embedding_size,), name=\"input_vector_2\")\n",
    "\n",
    "    # hidden layer\n",
    "    first_projection_layer = tf.keras.layers.Dense(\n",
    "        embedding_size,\n",
    "        activation='tanh',\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "        name=\"projection_layer\"\n",
    "    )\n",
    "    dropout = tf.keras.layers.Dropout(0.3, name=\"projection_dropout\")\n",
    "    projected_1_dense = dropout(first_projection_layer(input_1))\n",
    "    projected_2_dense = dropout(first_projection_layer(input_2))\n",
    "\n",
    "    # Normalize the projected vectors using Lambda layers\n",
    "    normalized_1 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_1\"\n",
    "    )(projected_1_dense)\n",
    "    normalized_2 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_2\"\n",
    "    )(projected_2_dense)\n",
    "\n",
    "    # Compute the custom similarity score using a Lambda layer\n",
    "    similarity_sum = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True), name=\"similarity_sum\"\n",
    "    )([normalized_1, normalized_2])\n",
    "\n",
    "    output = tf.keras.layers.Lambda(\n",
    "        lambda x: 0.5 * (1.0 + x), name=\"output_scaling\" #cambiar 0.5 por 2.5 para que este entre 0 y 5 \n",
    "    )(similarity_sum)\n",
    "\n",
    "    # Definir el modelo con las capas de entrada y salida\n",
    "    model = tf.keras.Model(inputs=[input_1, input_2], outputs=output, name=\"similarity_model\")\n",
    "\n",
    "    # Compilar el modelo\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    )\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Caracter√≠stica             | Modelo 1: Cosine (`build_and_compile_model2`) | Modelo 2: MLP (`build_model_aggregated`) |\n",
    "| -------------------------- | -------------------------------------------- | ---------------------------------------- |\n",
    "| Tipo de entrada            | 2 vectores                                   | 2 vectores concatenados                  |\n",
    "| Proyecci√≥n                 | Capa densa compartida                        | Dense normal (no compartida)             |\n",
    "| Normalizaci√≥n L2           | ‚úÖ s√≠                                         | ‚ùå no                                     |\n",
    "| M√©trica impl√≠cita          | Cosine similarity                            | No definida; aprende desde los datos     |\n",
    "| Salida                     | Escalado de coseno (rango 0-1 o 0-5)         | Escalar libre (regresi√≥n lineal)         |\n",
    "| Capacidad expresiva        | Limitada (coseno + proyecci√≥n)               | Alta (MLP)                               |\n",
    "| Interpretabilidad          | Alta                                         | Media                                    |\n",
    "| Velocidad de entrenamiento | M√°s r√°pido                                   | M√°s lento                                |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Aspecto                         | build_and_compile_model                  | `build_and_compile_model2`                              |\n",
    "| ------------------------------- | ----------------------------------------- | ------------------------------------------------------ |\n",
    "| **Activaci√≥n en la proyecci√≥n** | Ninguna (lineal)                          | `tanh` (no lineal)                                     |\n",
    "| **Regularizaci√≥n**              | No hay                                    | S√≠, con `Dropout`                                      |\n",
    "| **Normalizaci√≥n**               | Directamente con `tf.linalg.l2_normalize` | Igual, pero encapsulada en `Lambda` layers con nombres |\n",
    "| **Similitud coseno**            | `2.5 * (1.0 + coseno)`                    | `0.5 * (1.0 + coseno)`                                 |\n",
    "| **Escalado de salida**          | Rango `[0, 5]`                            | Rango `[0, 1]`                                         |\n",
    "| **Perdida**                     | `mean_absolute_error`                     | `mean_squared_error`                                   |\n",
    "| **Estilo**                      | M√°s directo, menos modular                | M√°s claro, modular, con nombres de capas               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training y evaluaci√≥n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "def evaluate_model(model, X1_test, X2_test, Y_test, name=\"\"):\n",
    "    y_pred = model.predict([X1_test, X2_test]).squeeze()\n",
    "    y_true = Y_test.squeeze()\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    pearson, _ = pearsonr(y_true, y_pred)\n",
    "    spearman, _ = spearmanr(y_true, y_pred)\n",
    "\n",
    "    print(f\"\\nüîé Resultados para el modelo '{name}':\")\n",
    "    print(f\"MSE:      {mse:.4f}\")\n",
    "    print(f\"RMSE:     {rmse:.4f}\")\n",
    "    print(f\"MAE:      {mae:.4f}\")\n",
    "    print(f\"Pearson:  {pearson:.4f}\")\n",
    "    print(f\"Spearman: {spearman:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"mse\": mse,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"pearson\": pearson,\n",
    "        \"spearman\": spearman\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir constantes de entrenamiento\n",
    "batch_size: int = 64\n",
    "num_epochs: int = 64\n",
    "train_val_split: float = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener x_train e y_train\n",
    "train_slice: int = int(len(mapped) * train_val_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2458"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Otiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parejas de vectores de oraciones - Listas de (d, d, 1)\n",
    "    :param pair_list: lista que devuelve map_pairs(), lista de ((vector1, vector2), similitud), sonde vector1 y 2 son vectores agregados\n",
    "    :return:\n",
    "    transforma una lista de pares de vectores y puntuaciones (como los que se usan en tareas de similaridad sem√°ntica tipo STS) en el formato adecuado para alimentar a un modelo de aprendizaje autom√°tico.\n",
    "    \"\"\"\n",
    "    _x, _y = zip(*pair_list) #_x: lista de tuplas (embedding_1, embedding_2), _y: lista de etiquetas\n",
    "    _x_1, _x_2 = zip(*_x)#_x_1: todos los embedding_1,  _x_2: todos los embedding_2\n",
    "    return (np.array(_x_1), np.array(_x_2)), np.array(_y, dtype=np.float32, ) / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las listas de train y test USANDO TF-IDF\n",
    "x_train, y_train = pair_list_to_x_y(mapped[:train_slice])\n",
    "x_val, y_val = pair_list_to_x_y(mapped[train_slice:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las listas de train y test SIN USAR TF-IDF\n",
    "x_train_normal, y_train_normal = pair_list_to_x_y(mapped[:train_slice])\n",
    "x_val_normal, y_val_normal = pair_list_to_x_y(mapped[train_slice:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_50, y_train_50 = pair_list_to_x_y(mapped_50[:train_slice])\n",
    "x_val_50, y_val_50 = pair_list_to_x_y(mapped_50[train_slice:])\n",
    "\n",
    "x_train_100, y_train_100 = pair_list_to_x_y(mapped_100[:train_slice])\n",
    "x_val_100, y_val_100 = pair_list_to_x_y(mapped_100[train_slice:])\n",
    "\n",
    "x_train_150, y_train_150 = pair_list_to_x_y(mapped_150[:train_slice])\n",
    "x_val_150, y_val_150 = pair_list_to_x_y(mapped_150[train_slice:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-25 17:07:34.108625: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Preparar los conjuntos de datos de entrenamiento y validaci√≥n\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar los conjuntos de datos de entrenamiento y validaci√≥n\n",
    "train_dataset_normal = tf.data.Dataset.from_tensor_slices((x_train_normal, y_train_normal))\n",
    "train_dataset_normal = train_dataset_normal.shuffle(buffer_size=len(x_train_normal)).batch(batch_size)\n",
    "\n",
    "val_dataset_normal = tf.data.Dataset.from_tensor_slices((x_val_normal, y_val_normal))\n",
    "val_dataset_normal = val_dataset_normal.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distancia COS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos evaluar el modelo si s√≥lo utilizamos COS similarity. (Depende completamente de los Word Embeddings)\n",
    "from scipy.stats import pearsonr\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_normal = mapped_no_tfidf[train_slice:]\n",
    "val = mapped[train_slice:]\n",
    "val_50 = mapped_50[train_slice:]\n",
    "val_100 = mapped_100[train_slice:]\n",
    "val_150 = mapped_150[train_slice:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando media cl√°sica la correlaci√≥n de Pearson es: 0.43930761675943547\n",
      "Usando media ponderada con TF-IDF la correlaci√≥n de Pearson es: 0.3689169387565914\n",
      "Usando media ponderada con TF-IDF y dimensi√≥n 50, la correlaci√≥n de Pearson es: 0.41854519762908476\n",
      "Usando media ponderada con TF-IDF y dimensi√≥n 100, la correlaci√≥n de Pearson es: 0.4308664019477864\n",
      "Usando media ponderada con TF-IDF y dimensi√≥n 150, la correlaci√≥n de Pearson es: 0.4422013561274339\n"
     ]
    }
   ],
   "source": [
    "y_pred_baseline = []\n",
    "y_pred_normal = []\n",
    "y_pred_50 = []\n",
    "y_pred_100 = []\n",
    "y_pred_150 = []\n",
    "for j in range(len(val)):\n",
    "    i = val[j]\n",
    "    v1, v2 = i[0] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_baseline.append(d)\n",
    "\n",
    "    k = val_normal[j]\n",
    "    v1, v2 = k[0] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_normal.append(d)\n",
    "\n",
    "    m = val_50[j]\n",
    "    v1, v2 = m[0] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_50.append(d)\n",
    "\n",
    "    l = val_100[j]\n",
    "    v1, v2 = l[0] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_100.append(d)\n",
    "\n",
    "    e = val_150[j]\n",
    "    v1, v2 = e[0] \n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_150.append(d)\n",
    "\n",
    "# Calcular la correlaci√≥n de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(np.array(y_pred_baseline), y_val.flatten())\n",
    "correlation_normal, _ = pearsonr(np.array(y_pred_normal), y_val_normal.flatten())\n",
    "correlation_50, _ = pearsonr(np.array(y_pred_50), y_val_50.flatten())\n",
    "correlation_100, _ = pearsonr(np.array(y_pred_100), y_val_100.flatten())\n",
    "correlation_150, _ = pearsonr(np.array(y_pred_150), y_val_150.flatten())\n",
    "\n",
    "# Imprimir el coeficiente de correlaci√≥n de Pearson\n",
    "print(f\"Usando media cl√°sica la correlaci√≥n de Pearson es: {correlation}\")\n",
    "print(f\"Usando media ponderada con TF-IDF la correlaci√≥n de Pearson es: {correlation_normal}\")\n",
    "print(f\"Usando media ponderada con TF-IDF y dimensi√≥n 50, la correlaci√≥n de Pearson es: {correlation_50}\")\n",
    "print(f\"Usando media ponderada con TF-IDF y dimensi√≥n 100, la correlaci√≥n de Pearson es: {correlation_100}\")\n",
    "print(f\"Usando media ponderada con TF-IDF y dimensi√≥n 150, la correlaci√≥n de Pearson es: {correlation_150}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:27:56.120397Z",
     "start_time": "2025-05-23T21:27:56.109750Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#la del profe: (la de arriba es la misma pero la he modificado para ver las dos correlaciones a la vez)\\nval = mapped[train_slice:]\\ny_pred_baseline = []\\ny_pred_normal = []\\nfor (v1, v2), dist in val:\\n    d = 1.0 - spatial.distance.cosine(v1, v2)\\n    y_pred_baseline.append(d)\\n# Calcular la correlaci√≥n de Pearson entre las predicciones y los datos de prueba\\ncorrelation, _ = pearsonr(np.array(y_pred_baseline), y_val.flatten())\\n# Imprimir el coeficiente de correlaci√≥n de Pearson\\nprint(f\"Correlaci√≥n de Pearson: {correlation}\")\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#la del profe: (la de arriba es la misma pero la he modificado para ver las dos correlaciones a la vez)\n",
    "val = mapped[train_slice:]\n",
    "y_pred_baseline = []\n",
    "y_pred_normal = []\n",
    "for (v1, v2), dist in val:\n",
    "    d = 1.0 - spatial.distance.cosine(v1, v2)\n",
    "    y_pred_baseline.append(d)\n",
    "# Calcular la correlaci√≥n de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(np.array(y_pred_baseline), y_val.flatten())\n",
    "# Imprimir el coeficiente de correlaci√≥n de Pearson\n",
    "print(f\"Correlaci√≥n de Pearson: {correlation}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prueba del embeding aprendido:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_vector_1', 'input_vector_2']\n",
      "Received: inputs=(('Tensor(shape=(None, 300))', 'Tensor(shape=(None, 300))'),)\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 2.3664 - mae: 1.1879 - root_mean_squared_error: 1.5340\n",
      "Epoch 2/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.9782 - mae: 0.7739 - root_mean_squared_error: 0.9889\n",
      "Epoch 3/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.7932 - mae: 0.6947 - root_mean_squared_error: 0.8904\n",
      "Epoch 4/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.5493 - mae: 0.5778 - root_mean_squared_error: 0.7409\n",
      "Epoch 5/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.4561 - mae: 0.5207 - root_mean_squared_error: 0.6745\n",
      "Epoch 6/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.3417 - mae: 0.4474 - root_mean_squared_error: 0.5845\n",
      "Epoch 7/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.3004 - mae: 0.4108 - root_mean_squared_error: 0.5479\n",
      "Epoch 8/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.2426 - mae: 0.3695 - root_mean_squared_error: 0.4917\n",
      "Epoch 9/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1774 - mae: 0.3216 - root_mean_squared_error: 0.4211\n",
      "Epoch 10/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1438 - mae: 0.2856 - root_mean_squared_error: 0.3791\n",
      "Epoch 11/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.1229 - mae: 0.2702 - root_mean_squared_error: 0.3505\n",
      "Epoch 12/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0983 - mae: 0.2376 - root_mean_squared_error: 0.3135\n",
      "Epoch 13/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0905 - mae: 0.2269 - root_mean_squared_error: 0.3006\n",
      "Epoch 14/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0795 - mae: 0.2082 - root_mean_squared_error: 0.2814\n",
      "Epoch 15/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0615 - mae: 0.1881 - root_mean_squared_error: 0.2479\n",
      "Epoch 16/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0541 - mae: 0.1772 - root_mean_squared_error: 0.2325\n",
      "Epoch 17/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0496 - mae: 0.1684 - root_mean_squared_error: 0.2226\n",
      "Epoch 18/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0456 - mae: 0.1633 - root_mean_squared_error: 0.2134\n",
      "Epoch 19/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0383 - mae: 0.1520 - root_mean_squared_error: 0.1955\n",
      "Epoch 20/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0376 - mae: 0.1480 - root_mean_squared_error: 0.1937\n",
      "Epoch 21/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0364 - mae: 0.1498 - root_mean_squared_error: 0.1907\n",
      "Epoch 22/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0290 - mae: 0.1332 - root_mean_squared_error: 0.1703\n",
      "Epoch 23/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0296 - mae: 0.1358 - root_mean_squared_error: 0.1720\n",
      "Epoch 24/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0257 - mae: 0.1231 - root_mean_squared_error: 0.1603\n",
      "Epoch 25/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0281 - mae: 0.1291 - root_mean_squared_error: 0.1677\n",
      "Epoch 26/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0255 - mae: 0.1224 - root_mean_squared_error: 0.1597\n",
      "Epoch 27/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0232 - mae: 0.1184 - root_mean_squared_error: 0.1523\n",
      "Epoch 28/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0245 - mae: 0.1192 - root_mean_squared_error: 0.1566\n",
      "Epoch 29/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0230 - mae: 0.1184 - root_mean_squared_error: 0.1515\n",
      "Epoch 30/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0232 - mae: 0.1186 - root_mean_squared_error: 0.1521\n",
      "Epoch 31/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0223 - mae: 0.1135 - root_mean_squared_error: 0.1492\n",
      "Epoch 32/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0214 - mae: 0.1137 - root_mean_squared_error: 0.1460\n",
      "Epoch 33/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0206 - mae: 0.1120 - root_mean_squared_error: 0.1434\n",
      "Epoch 34/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0197 - mae: 0.1093 - root_mean_squared_error: 0.1402\n",
      "Epoch 35/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0190 - mae: 0.1059 - root_mean_squared_error: 0.1378\n",
      "Epoch 36/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0187 - mae: 0.1070 - root_mean_squared_error: 0.1369\n",
      "Epoch 37/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0186 - mae: 0.1065 - root_mean_squared_error: 0.1362\n",
      "Epoch 38/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0171 - mae: 0.1031 - root_mean_squared_error: 0.1308\n",
      "Epoch 39/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0184 - mae: 0.1054 - root_mean_squared_error: 0.1357\n",
      "Epoch 40/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0170 - mae: 0.1026 - root_mean_squared_error: 0.1304\n",
      "Epoch 41/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0175 - mae: 0.1020 - root_mean_squared_error: 0.1323\n",
      "Epoch 42/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0167 - mae: 0.1015 - root_mean_squared_error: 0.1293\n",
      "Epoch 43/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0162 - mae: 0.0983 - root_mean_squared_error: 0.1273\n",
      "Epoch 44/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0160 - mae: 0.0977 - root_mean_squared_error: 0.1266\n",
      "Epoch 45/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0158 - mae: 0.0977 - root_mean_squared_error: 0.1255\n",
      "Epoch 46/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0152 - mae: 0.0944 - root_mean_squared_error: 0.1231\n",
      "Epoch 47/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 0.0157 - mae: 0.0978 - root_mean_squared_error: 0.1254\n",
      "Epoch 48/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0136 - mae: 0.0907 - root_mean_squared_error: 0.1166\n",
      "Epoch 49/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0137 - mae: 0.0909 - root_mean_squared_error: 0.1170\n",
      "Epoch 50/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0132 - mae: 0.0904 - root_mean_squared_error: 0.1147\n",
      "Epoch 51/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0147 - mae: 0.0943 - root_mean_squared_error: 0.1213\n",
      "Epoch 52/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0149 - mae: 0.0949 - root_mean_squared_error: 0.1221\n",
      "Epoch 53/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0139 - mae: 0.0916 - root_mean_squared_error: 0.1178\n",
      "Epoch 54/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0132 - mae: 0.0902 - root_mean_squared_error: 0.1151\n",
      "Epoch 55/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0132 - mae: 0.0904 - root_mean_squared_error: 0.1149\n",
      "Epoch 56/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0128 - mae: 0.0869 - root_mean_squared_error: 0.1133\n",
      "Epoch 57/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0114 - mae: 0.0848 - root_mean_squared_error: 0.1068\n",
      "Epoch 58/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0119 - mae: 0.0852 - root_mean_squared_error: 0.1091\n",
      "Epoch 59/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0117 - mae: 0.0852 - root_mean_squared_error: 0.1083\n",
      "Epoch 60/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0121 - mae: 0.0867 - root_mean_squared_error: 0.1098\n",
      "Epoch 61/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0118 - mae: 0.0838 - root_mean_squared_error: 0.1087\n",
      "Epoch 62/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0121 - mae: 0.0857 - root_mean_squared_error: 0.1098\n",
      "Epoch 63/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 0.0114 - mae: 0.0843 - root_mean_squared_error: 0.1069\n",
      "Epoch 64/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0112 - mae: 0.0817 - root_mean_squared_error: 0.1059\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7dd103c96cf0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_no_cos = build_model_aggregated(embedding_dim=300)\n",
    "model_no_cos.fit([x_train], y_train, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "\n",
      "üîé Resultados para el modelo 'Linealmodel no cos()':\n",
      "MSE:      0.0279\n",
      "RMSE:     0.1672\n",
      "MAE:      0.1280\n",
      "Pearson:  0.3419\n",
      "Spearman: 0.3337\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': 'Linealmodel no cos()',\n",
       " 'mse': 0.027944477,\n",
       " 'rmse': 0.16716602,\n",
       " 'mae': 0.12803978,\n",
       " 'pearson': 0.3418520310011242,\n",
       " 'spearman': 0.33370564078195586}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X1, X2 = x_val\n",
    "evaluate_model(model_no_cos, X1, X2, y_val, name=\"Linealmodel no cos()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.utils.plot_model(model_no_cos, show_shapes=True, show_layer_activations=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/models/functional.py:241: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: ['input_vector_1', 'input_vector_2']\n",
      "Received: inputs=(('Tensor(shape=(None, None))', 'Tensor(shape=(None, None))'),)\n",
      "  warnings.warn(msg)\n",
      "2025-05-25 17:33:02.090854: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INVALID_ARGUMENT: Incompatible shapes: [64,100] vs. [600]\n",
      "\t [[{{function_node __inference_one_step_on_data_55655}}{{node gradient_tape/functional_1/batch_normalization_1/batchnorm/mul_1/BroadcastGradientArgs}}]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node gradient_tape/functional_1/batch_normalization_1/batchnorm/mul_1/BroadcastGradientArgs defined at (most recent call last):\n  File \"/usr/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/usr/lib/python3.12/runpy.py\", line 88, in _run_code\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_26870/2176028764.py\", line 2, in <module>\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 78, in train_step\n\nIncompatible shapes: [64,100] vs. [600]\n\t [[{{node gradient_tape/functional_1/batch_normalization_1/batchnorm/mul_1/BroadcastGradientArgs}}]] [Op:__inference_multi_step_on_iterator_55735]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_no_cos_50 \u001b[38;5;241m=\u001b[39m build_model_aggregated(embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel_no_cos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx_train_50\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_50\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/Desktop/myenv/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node gradient_tape/functional_1/batch_normalization_1/batchnorm/mul_1/BroadcastGradientArgs defined at (most recent call last):\n  File \"/usr/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n\n  File \"/usr/lib/python3.12/runpy.py\", line 88, in _run_code\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_26870/2176028764.py\", line 2, in <module>\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 377, in fit\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 220, in function\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 133, in multi_step_on_iterator\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 114, in one_step_on_data\n\n  File \"/home/taya/Desktop/myenv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 78, in train_step\n\nIncompatible shapes: [64,100] vs. [600]\n\t [[{{node gradient_tape/functional_1/batch_normalization_1/batchnorm/mul_1/BroadcastGradientArgs}}]] [Op:__inference_multi_step_on_iterator_55735]"
     ]
    }
   ],
   "source": [
    "#model_no_cos_50 = build_model_aggregated(embedding_dim=50)\n",
    "#model_no_cos.fit([x_train_50], y_train_50, epochs=num_epochs, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 3.9752 - val_loss: 3.7108\n",
      "Epoch 2/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.6279 - val_loss: 3.6226\n",
      "Epoch 3/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.5095 - val_loss: 3.5766\n",
      "Epoch 4/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.4266 - val_loss: 3.5462\n",
      "Epoch 5/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 3.3611 - val_loss: 3.5241\n",
      "Epoch 6/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 3.3055 - val_loss: 3.5071\n",
      "Epoch 7/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 3.2580 - val_loss: 3.4937\n",
      "Epoch 8/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2178 - val_loss: 3.4825\n",
      "Epoch 9/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.1824 - val_loss: 3.4730\n",
      "Epoch 10/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.1502 - val_loss: 3.4649\n",
      "Epoch 11/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 3.1230 - val_loss: 3.4580\n",
      "Epoch 12/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.0975 - val_loss: 3.4527\n",
      "Epoch 13/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0756 - val_loss: 3.4471\n",
      "Epoch 14/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 3.0526 - val_loss: 3.4422\n",
      "Epoch 15/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 3.0324 - val_loss: 3.4383\n",
      "Epoch 16/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 3.0152 - val_loss: 3.4357\n",
      "Epoch 17/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9999 - val_loss: 3.4318\n",
      "Epoch 18/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9813 - val_loss: 3.4288\n",
      "Epoch 19/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.9653 - val_loss: 3.4270\n",
      "Epoch 20/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.9531 - val_loss: 3.4237\n",
      "Epoch 21/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.9360 - val_loss: 3.4216\n",
      "Epoch 22/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.9251 - val_loss: 3.4194\n",
      "Epoch 23/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.9120 - val_loss: 3.4183\n",
      "Epoch 24/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.9028 - val_loss: 3.4157\n",
      "Epoch 25/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8857 - val_loss: 3.4134\n",
      "Epoch 26/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8746 - val_loss: 3.4117\n",
      "Epoch 27/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.8677 - val_loss: 3.4091\n",
      "Epoch 28/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.8564 - val_loss: 3.4069\n",
      "Epoch 29/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.8430 - val_loss: 3.4052\n",
      "Epoch 30/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.8368 - val_loss: 3.4019\n",
      "Epoch 31/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.8264 - val_loss: 3.3998\n",
      "Epoch 32/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 2.8166 - val_loss: 3.3994\n",
      "Epoch 33/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.8069 - val_loss: 3.3999\n",
      "Epoch 34/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.8007 - val_loss: 3.3930\n",
      "Epoch 35/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7896 - val_loss: 3.3937\n",
      "Epoch 36/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.7826 - val_loss: 3.3898\n",
      "Epoch 37/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.7690 - val_loss: 3.3867\n",
      "Epoch 38/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7636 - val_loss: 3.3887\n",
      "Epoch 39/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7606 - val_loss: 3.3840\n",
      "Epoch 40/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7473 - val_loss: 3.3846\n",
      "Epoch 41/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.7436 - val_loss: 3.3851\n",
      "Epoch 42/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7390 - val_loss: 3.3772\n",
      "Epoch 43/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7286 - val_loss: 3.3753\n",
      "Epoch 44/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2.7243 - val_loss: 3.3736\n",
      "Epoch 45/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.7184 - val_loss: 3.3677\n",
      "Epoch 46/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.7081 - val_loss: 3.3672\n",
      "Epoch 47/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.7028 - val_loss: 3.3649\n",
      "Epoch 48/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6940 - val_loss: 3.3631\n",
      "Epoch 49/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6880 - val_loss: 3.3598\n",
      "Epoch 50/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6797 - val_loss: 3.3559\n",
      "Epoch 51/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6723 - val_loss: 3.3573\n",
      "Epoch 52/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6706 - val_loss: 3.3539\n",
      "Epoch 53/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6589 - val_loss: 3.3491\n",
      "Epoch 54/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6504 - val_loss: 3.3470\n",
      "Epoch 55/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.6426 - val_loss: 3.3469\n",
      "Epoch 56/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6408 - val_loss: 3.3421\n",
      "Epoch 57/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6307 - val_loss: 3.3410\n",
      "Epoch 58/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2.6239 - val_loss: 3.3379\n",
      "Epoch 59/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.6150 - val_loss: 3.3391\n",
      "Epoch 60/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6122 - val_loss: 3.3361\n",
      "Epoch 61/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.6086 - val_loss: 3.3350\n",
      "Epoch 62/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.5975 - val_loss: 3.3330\n",
      "Epoch 63/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 2.5956 - val_loss: 3.3314\n",
      "Epoch 64/64\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5901 - val_loss: 3.3332\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7dd0fa174e00>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lin = build_and_compile_model()\n",
    "model_lin.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "\n",
      "üîé Resultados para el modelo '':\n",
      "MSE:      12.2433\n",
      "RMSE:     3.4990\n",
      "MAE:      3.3332\n",
      "Pearson:  0.3417\n",
      "Spearman: 0.3970\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'name': '',\n",
       " 'mse': 12.243323,\n",
       " 'rmse': 3.499046,\n",
       " 'mae': 3.3331766,\n",
       " 'pearson': 0.341672256901909,\n",
       " 'spearman': 0.397042571697594}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(model_lin, X1, X2, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Correlaci√≥n de Pearson: 0.341672256901909\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "y_pred: tf.RaggedTensor = model_lin.predict(x_val)\n",
    "# Calcular la correlaci√≥n de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
    "# Imprimir el coeficiente de correlaci√≥n de Pearson\n",
    "print(f\"Correlaci√≥n de Pearson: {correlation}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y compilar el modelo\n",
    "model_no_lin = build_and_compile_model2()\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True, show_layer_activations=True, )\n",
    "#print(model.summary())\n",
    "# Entrenar el modelo\n",
    "model_no_lin.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:27:56.239203Z",
     "start_time": "2025-05-23T21:27:56.137048Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "Correlaci√≥n de Pearson: 0.5301249838419955\n"
     ]
    }
   ],
   "source": [
    "#from scipy.stats import pearsonr\n",
    "# Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "y_pred: tf.RaggedTensor = model_no_lin.predict(x_val)\n",
    "# Calcular la correlaci√≥n de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
    "# Imprimir el coeficiente de correlaci√≥n de Pearson\n",
    "print(f\"Correlaci√≥n de Pearson: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lo mismo pero para el train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:27:56.369002Z",
     "start_time": "2025-05-23T21:27:56.250260Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#from scipy.stats import pearsonr\\n# Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\\ny_pred: tf.RaggedTensor = model_no_lin.predict(x_train)\\n# Calcular la correlaci√≥n de Pearson entre las predicciones y los datos de prueba\\ncorrelation, _ = pearsonr(y_pred.flatten(), y_train.flatten())\\n# Imprimir el coeficiente de correlaci√≥n de Pearson\\nprint(f\"Correlaci√≥n de Pearson: {correlation}\")\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#from scipy.stats import pearsonr\n",
    "# Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "y_pred: tf.RaggedTensor = model_no_lin.predict(x_train)\n",
    "# Calcular la correlaci√≥n de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_train.flatten())\n",
    "# Imprimir el coeficiente de correlaci√≥n de Pearson\n",
    "print(f\"Correlaci√≥n de Pearson: {correlation}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

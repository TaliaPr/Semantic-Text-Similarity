{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip uninstall -y numpy gensim\\n!pip install numpy==1.23.5  \\n!pip install gensim         \\n\\n# Also install TensorFlow for later use\\n!pip install tensorflow==2.19.0  \\n\\n# Force restart the kernel after installing packages\\nprint(\"Please restart the kernel after running this cell.\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''!pip uninstall -y numpy gensim\n",
    "!pip install numpy==1.23.5  \n",
    "!pip install gensim         \n",
    "\n",
    "# Also install TensorFlow for later use\n",
    "!pip install tensorflow==2.19.0  \n",
    "\n",
    "# Force restart the kernel after installing packages\n",
    "print(\"Please restart the kernel after running this cell.\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Semantic Text Similarity\n",
    "Este modelo utiliza gensim para convertir pares de vectores + puntuacions en vectores (word embeddings).\n",
    "Dado un dataset, infere la puntuació de similitud entre ambdues frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:56:38.187276Z",
     "start_time": "2025-05-23T13:56:35.576516Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Requisits\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T13:56:38.193657Z",
     "start_time": "2025-05-23T13:56:38.191213Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tipat\n",
    "from typing import Tuple, List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:00:32.430490Z",
     "start_time": "2025-05-23T13:58:11.606736Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"WV_MODEL_PATH = '/Users/salva/Downloads/cc.ca.300.vec.gz'\\nimport gensim\\nwv_model =  gensim.models.KeyedVectors.load_word2vec_format(WV_MODEL_PATH, binary=False)\\nwv_model\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Models pre-entrenats\n",
    "# WV_MODEL_PATH = \"/Users/salva/Downloads/cc.ca.300.bin.gz\"\n",
    "\n",
    "\n",
    "'''WV_MODEL_PATH = '/Users/salva/Downloads/cc.ca.300.vec.gz'\n",
    "import gensim\n",
    "wv_model =  gensim.models.KeyedVectors.load_word2vec_format(WV_MODEL_PATH, binary=False)\n",
    "wv_model'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llavors podeu carregar el model com a mmap\n",
    "from gensim.models.fasttext import FastTextKeyedVectors\n",
    "wv_model = FastTextKeyedVectors.load('../cc.ca.gensim.bin', mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:01:37.455122Z",
     "start_time": "2025-05-23T14:01:37.445157Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exemple de 10 parells d'oracions amb puntuació de similitud associada\n",
    "input_pairs = [\n",
    "    ('M\\'agrada el futbol', 'Disfruto veient partits de futbol', 4),\n",
    "    ('El cel està despejat', 'Fa un dia bonic', 4.5),\n",
    "    ('M\\'encanta viatjar', 'Explorar nous llocs és una passió', 3.5),\n",
    "    ('Prefereixo l\\'estiu', 'No m\\'agrada el fred de l\\'hivern', 2.5),\n",
    "    ('Tinc gana', 'Què hi ha per sopar?', 2),\n",
    "    ('La música em relaxa', 'Escoltar música és una teràpia', 3),\n",
    "    ('El llibre és emocionant', 'No puc deixar de llegir-lo', 4),\n",
    "    ('M\\'agrada la pizza', 'És el meu menjar preferit', 4.5),\n",
    "    ('Estic cansat', 'Necessito fer una migdiada', 1.5),\n",
    "    ('Avui fa molta calor', 'És un dia sofocant', 3.5)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:01:38.200978Z",
     "start_time": "2025-05-23T14:01:38.197829Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "REMAP_EMBEDDINGS: bool = True\n",
    "USE_PRETRAINED: bool = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:11:24.097338Z",
     "start_time": "2025-05-23T14:11:20.085380Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Text Similarity (STS) dataset (principal per la Pràctica 4)\n",
    "train = load_dataset(\"projecte-aina/sts-ca\", split=\"train\")\n",
    "test = load_dataset(\"projecte-aina/sts-ca\", split=\"test\")\n",
    "val = load_dataset(\"projecte-aina/sts-ca\", split=\"validation\")\n",
    "\n",
    "all_data = load_dataset(\"projecte-aina/sts-ca\", split=\"all\")\n",
    "all_data\n",
    "print(train[0]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:28:24.986973Z",
     "start_time": "2025-05-23T14:28:24.737384Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x23d0ff08a90>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessament de les oracions i creació del diccionari\n",
    "sentences_1_preproc = [simple_preprocess(d[\"sentence_1\"]) for d in all_data]\n",
    "sentences_2_preproc = [simple_preprocess(d[\"sentence_2\"]) for d in all_data]\n",
    "\n",
    "scores = [d[\"label\"] for d in all_data]\n",
    "sentence_pairs = list(zip(sentences_1_preproc, sentences_2_preproc, scores))\n",
    "# Versió aplanada per poder entrenar el model\n",
    "sentences_pairs_flattened = sentences_1_preproc + sentences_2_preproc\n",
    "diccionario = Dictionary(sentences_pairs_flattened)\n",
    "diccionario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Baseline : One-Hot Encoding\n",
    "\n",
    "Una altra alternativa és utilitzar One-Hot Encoding per representar les frases. Això pot ser útil per comparar la similitud entre frases de manera més directa, encara que no captura la semàntica de les paraules com ho fan els word embeddings. La similitud es pot calcular fent servir la distància de Jaccard (número de paraules en comú dividit entre número de paraules totals entre les dues frases) o el coseno entre els vectors resultants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usant la distància Jaccard per avaluar la similitud entre oracions\n",
    "from typing import List, Set, Union\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_evaluation(sent1: List[Union[str, Set[str]]], sent2: List[Union[str, Set[str]]]) -> float:\n",
    "    \"\"\"\n",
    "    Calcular la similitud de Jaccard entre dues oracions\n",
    "    \n",
    "    Args:\n",
    "        sent1: Primera oració tokenitzada com una llista de paraules o conjunts de paraules\n",
    "        sent2: Segona oració tokenitzada com una llista de paraules o conjunts de paraules\n",
    "        \n",
    "    Returns:\n",
    "        float: Puntuació de similitud basada en la distància de Jaccard\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for i in range(len(sent1)):\n",
    "        # Convertir a conjunts si no ho són ja\n",
    "        set1 = set(sent1[i]) if not isinstance(sent1[i], set) else sent1[i]\n",
    "        set2 = set(sent2[i]) if not isinstance(sent2[i], set) else sent2[i]\n",
    "        \n",
    "        # Calcular la similitud de Jaccard\n",
    "        score = len(set1.intersection(set2)) / len(set1.union(set2))\n",
    "        scores.append(score)\n",
    "    \n",
    "    # Retornar la puntuació mitjana si tenim puntuacions vàlides\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usando la distancia coseno para calcular la similitud entre dos oraciones\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "\n",
    "def one_hot_cosine_similarity(sent1: List[Union[str, Set[str]]], sent2: List[Union[str, Set[str]]], vocabulary: Optional[Dictionary] = None) -> List[float]:\n",
    "    \"\"\"\n",
    "    Calcula la similitud del coseno entre pares de oraciones utilizando representación one-hot encoding\n",
    "    \n",
    "    Args:\n",
    "        sent1: Lista de oraciones tokenizadas (primera oración de cada par)\n",
    "        sent2: Lista de oraciones tokenizadas (segunda oración de cada par)\n",
    "        vocabulary: Diccionario opcional para mapear palabras a índices\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: Lista de puntuaciones de similitud basadas en el coseno\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for i in range(len(sent1)):\n",
    "        # Convertir a conjuntos para facilitar el manejo\n",
    "        set1 = set(sent1[i]) if not isinstance(sent1[i], set) else sent1[i]\n",
    "        set2 = set(sent2[i]) if not isinstance(sent2[i], set) else sent2[i]\n",
    "        \n",
    "        if vocabulary:\n",
    "            # Usar el vocabulario proporcionado\n",
    "            vocab_size = len(vocabulary.token2id)\n",
    "            vec1 = np.zeros(vocab_size)\n",
    "            vec2 = np.zeros(vocab_size)\n",
    "            \n",
    "            for word in set1:\n",
    "                if word in vocabulary.token2id:\n",
    "                    vec1[vocabulary.token2id[word]] = 1\n",
    "                    \n",
    "            for word in set2:\n",
    "                if word in vocabulary.token2id:\n",
    "                    vec2[vocabulary.token2id[word]] = 1\n",
    "        else:\n",
    "            # Crear un vocabulario ad-hoc para este par\n",
    "            all_words = set1.union(set2)\n",
    "            word_to_idx = {word: idx for idx, word in enumerate(all_words)}\n",
    "            \n",
    "            vec1 = np.zeros(len(all_words))\n",
    "            vec2 = np.zeros(len(all_words))\n",
    "            \n",
    "            for word in set1:\n",
    "                vec1[word_to_idx[word]] = 1\n",
    "                \n",
    "            for word in set2:\n",
    "                vec2[word_to_idx[word]] = 1\n",
    "        \n",
    "        # Calcular similitud del coseno\n",
    "        # Si los vectores son cero, asignamos una similitud de 0\n",
    "        norm1 = np.linalg.norm(vec1)\n",
    "        norm2 = np.linalg.norm(vec2)\n",
    "        \n",
    "        if norm1 == 0 or norm2 == 0:\n",
    "            scores.append(0.0)\n",
    "        else:\n",
    "            cos_sim = np.dot(vec1, vec2) / (norm1 * norm2)\n",
    "            scores.append(cos_sim)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Ejemplo de uso\n",
    "# similitudes = one_hot_cosine_similarity(sentences_1_preproc[:5], sentences_2_preproc[:5], diccionario)\n",
    "# print(similitudes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo regressió amb atenció\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:28:26.049512Z",
     "start_time": "2025-05-23T14:28:26.044611Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Len: 30 30\n",
      "[0, 11, 13, 1, 9, 10, 5, 14, 8, 7, 2, 8, 12, 2, 6, 4, 3, 15]\n"
     ]
    }
   ],
   "source": [
    "print(\"Max Len:\", max([len(s) for s in sentences_1_preproc]), max([len(s) for s in sentences_2_preproc]))\n",
    "print(list(diccionario.doc2idx(sentences_1_preproc[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:30:40.096782Z",
     "start_time": "2025-05-23T14:30:40.079341Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from typing import Union\n",
    "\n",
    "\n",
    "def map_word_embeddings(\n",
    "        sentence: Union[str, List[str]],\n",
    "        sequence_len: int = 32,\n",
    "        fixed_dictionary: Optional[Dictionary] = None\n",
    "        ) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Map to word-embedding indices\n",
    "    :param sentence:\n",
    "    :param sequence_len:\n",
    "    :param fixed_dictionary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if not isinstance(sentence, list):\n",
    "        sentence_preproc = simple_preprocess(sentence)\n",
    "    else:\n",
    "        sentence_preproc = sentence\n",
    "    _vectors = np.zeros(sequence_len, dtype=np.int32)\n",
    "    index = 0\n",
    "    for word in sentence_preproc:\n",
    "        if fixed_dictionary is not None:\n",
    "            if word in fixed_dictionary.token2id:\n",
    "                # Sumo 1 perquè el valor 0 està reservat a padding\n",
    "                _vectors[index] = fixed_dictionary.token2id[word] + 1\n",
    "                index += 1\n",
    "        else:\n",
    "            if word in wv_model.key_to_index:\n",
    "                _vectors[index] = wv_model.key_to_index[word] + 1\n",
    "                index += 1\n",
    "    return _vectors\n",
    "\n",
    "\n",
    "def map_pairs(\n",
    "        sentence_pairs: List[Tuple[str, str, float]],\n",
    "        sequence_len: int = 32,\n",
    "        fixed_dictionary: Optional[Dictionary] = None\n",
    ") -> List[Tuple[Tuple[np.ndarray, np.ndarray], float]]:\n",
    "    \"\"\"\n",
    "    Mapea els triplets d'oracions a llistes de (x, y), (pares de vectors, score)\n",
    "    :param sentence_pairs:\n",
    "    :param sequence_len:\n",
    "    :param fixed_dictionary:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # Mapeig dels paquets d'oracions a paquets de vectors\n",
    "    pares_vectores = []\n",
    "    for i, (sentence_1, sentence_2, similitud) in enumerate(sentence_pairs):\n",
    "        vector1 = map_word_embeddings(sentence_1, sequence_len, fixed_dictionary)\n",
    "        vector2 = map_word_embeddings(sentence_2, sequence_len, fixed_dictionary)\n",
    "        # Afegir a la llista\n",
    "        pares_vectores.append(((vector1, vector2), similitud))\n",
    "    return pares_vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T14:30:41.100414Z",
     "start_time": "2025-05-23T14:30:41.067881Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((array([ 1, 12, 14,  2, 10, 11,  6, 15,  9,  8,  3,  9, 13,  3,  7,  5,  4,\n",
      "       16,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]), array([10010,     9,  2784,     6,    15,     9,     8,     3,     9,\n",
      "          13,     3,     7,     5,     4,    16,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0])), 3.5)\n"
     ]
    }
   ],
   "source": [
    "# Imprimir els paquets de vectors i la puntuació de similitud associada\n",
    "mapped = map_pairs(sentence_pairs, fixed_dictionary=diccionario if REMAP_EMBEDDINGS else None)\n",
    "# for vectors, similitud in mapped:\n",
    "#     print(f\"Pares de vectores: {vectors[0].shape}, {vectors[1].shape}\")\n",
    "#     print(f\"Puntuació de similitud: {similitud}\")\n",
    "print(mapped[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:29:33.314851Z",
     "start_time": "2025-05-23T21:29:33.309646Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Definir constants d'entrenament\n",
    "batch_size: int = 64\n",
    "num_epochs: int = 128\n",
    "train_val_split: float = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:23:42.790296Z",
     "start_time": "2025-05-23T21:23:42.786818Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3073"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:23:43.042943Z",
     "start_time": "2025-05-23T21:23:43.031829Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Obtener x_train e y_train\n",
    "train_slice: int = int(len(mapped) * train_val_split)\n",
    "\n",
    "def pair_list_to_x_y(pair_list: List[Tuple[Tuple[np.ndarray, np.ndarray], int]]) -> Tuple[Tuple[np.ndarray, np.ndarray], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Otiene las matrices X_1 (N x d) , X_2 (N x d), e Y (n) a partir de listas de parelles de vectors d'oracions - Llistes de (d, d, 1)\n",
    "    :param pair_list:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    _x, _y = zip(*pair_list)\n",
    "    _x_1, _x_2 = zip(*_x)\n",
    "    return (np.row_stack(_x_1), np.row_stack(_x_2)), np.array(_y) / 5.0\n",
    "\n",
    "# Obtener las listas de train y test\n",
    "x_train, y_train = pair_list_to_x_y(mapped[:train_slice])\n",
    "x_val, y_val = pair_list_to_x_y(mapped[train_slice:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:23:43.233872Z",
     "start_time": "2025-05-23T21:23:43.221808Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Preparar els conjunts de dades d'entrenament i validació\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=len(x_train)).batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "val_dataset = val_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.19.0\n",
      "['AggregationMethod', 'Assert', 'CriticalSection', 'DType', 'DeviceSpec', 'GradientTape', 'Graph', 'IndexedSlices', 'IndexedSlicesSpec', 'Module', 'Operation', 'OptionalSpec', 'RaggedTensor', 'RaggedTensorSpec', 'RegisterGradient', 'SparseTensor', 'SparseTensorSpec', 'Tensor', 'TensorArray', 'TensorArraySpec', 'TensorShape', 'TensorSpec', 'TypeSpec', 'UnconnectedGradients', 'Variable', 'VariableAggregation', 'VariableSynchronization', '_API_MODULE', '_KerasLazyLoader', '__all__', '__builtins__', '__cached__', '__compiler_version__', '__cxx11_abi_flag__', '__cxx_version__', '__doc__', '__file__', '__git_version__', '__internal__', '__loader__', '__monolithic_build__', '__name__', '__operators__', '__package__', '__path__', '__spec__', '__version__', '_api', '_compat', '_current_file_location', '_current_module', '_fi', '_initializers', '_inspect', '_kernel_dir', '_ll', '_losses', '_major_api_version', '_metrics', '_module_dir', '_module_util', '_name', '_names_with_underscore', '_optimizers', '_os', '_plugin_dir', '_pywrap_tensorflow', '_running_from_pip_package', '_s', '_scheme', '_site', '_site_packages_dirs', '_sys', '_sysconfig', '_tf2', '_tf_api_dir', '_tf_dir', '_tf_uses_legacy_keras', 'abs', 'acos', 'acosh', 'add', 'add_n', 'approx_top_k', 'argmax', 'argmin', 'argsort', 'as_dtype', 'as_string', 'asin', 'asinh', 'assert_equal', 'assert_greater', 'assert_less', 'assert_rank', 'atan', 'atan2', 'atanh', 'audio', 'autodiff', 'autograph', 'batch_to_space', 'bfloat16', 'bitcast', 'bitwise', 'bool', 'boolean_mask', 'broadcast_dynamic_shape', 'broadcast_static_shape', 'broadcast_to', 'case', 'cast', 'check_pinned', 'clip_by_global_norm', 'clip_by_norm', 'clip_by_value', 'compat', 'complex', 'complex128', 'complex64', 'concat', 'cond', 'config', 'constant', 'constant_initializer', 'control_dependencies', 'conv', 'conv2d_backprop_filter_v2', 'conv2d_backprop_input_v2', 'convert_to_tensor', 'cos', 'cosh', 'cumsum', 'custom_gradient', 'data', 'debugging', 'device', 'distribute', 'divide', 'double', 'dtensor', 'dtypes', 'dynamic_partition', 'dynamic_stitch', 'edit_distance', 'eig', 'eigvals', 'einsum', 'ensure_shape', 'equal', 'errors', 'executing_eagerly', 'exp', 'expand_dims', 'experimental', 'extract_volume_patches', 'eye', 'feature_column', 'fftnd', 'fill', 'fingerprint', 'float16', 'float32', 'float64', 'floor', 'foldl', 'foldr', 'function', 'gather', 'gather_nd', 'get_current_name_scope', 'get_logger', 'get_static_value', 'grad_pass_through', 'gradients', 'graph_util', 'greater', 'greater_equal', 'group', 'guarantee_const', 'half', 'hessians', 'histogram_fixed_width', 'histogram_fixed_width_bins', 'identity', 'identity_n', 'ifftnd', 'image', 'import_graph_def', 'init_scope', 'initializers', 'inside_function', 'int16', 'int32', 'int64', 'int8', 'io', 'irfftnd', 'is_symbolic_tensor', 'is_tensor', 'keras', 'less', 'less_equal', 'linalg', 'linspace', 'lite', 'load_library', 'load_op_library', 'logical_and', 'logical_not', 'logical_or', 'lookup', 'losses', 'make_ndarray', 'make_tensor_proto', 'map_fn', 'math', 'matmul', 'matrix_square_root', 'maximum', 'meshgrid', 'metrics', 'minimum', 'mlir', 'multiply', 'name_scope', 'negative', 'nest', 'newaxis', 'nn', 'no_gradient', 'no_op', 'nondifferentiable_batch_function', 'norm', 'not_equal', 'numpy_function', 'one_hot', 'ones', 'ones_initializer', 'ones_like', 'optimizers', 'pad', 'parallel_stack', 'pow', 'print', 'profiler', 'py_function', 'qint16', 'qint32', 'qint8', 'quantization', 'queue', 'quint16', 'quint8', 'ragged', 'ragged_fill_empty_rows', 'ragged_fill_empty_rows_grad', 'random', 'random_index_shuffle', 'random_normal_initializer', 'random_uniform_initializer', 'range', 'rank', 'raw_ops', 'realdiv', 'recompute_grad', 'reduce_all', 'reduce_any', 'reduce_logsumexp', 'reduce_max', 'reduce_mean', 'reduce_min', 'reduce_prod', 'reduce_sum', 'register_tensor_conversion_function', 'repeat', 'required_space_to_batch_paddings', 'reshape', 'resource', 'reverse', 'reverse_sequence', 'rfftnd', 'roll', 'round', 'saturate_cast', 'saved_model', 'scalar_mul', 'scan', 'scatter_nd', 'searchsorted', 'security', 'sequence_mask', 'sets', 'shape', 'shape_n', 'sigmoid', 'sign', 'signal', 'sin', 'sinh', 'size', 'slice', 'sort', 'space_to_batch', 'space_to_batch_nd', 'sparse', 'split', 'sqrt', 'square', 'squeeze', 'stack', 'stop_gradient', 'strided_slice', 'string', 'strings', 'subtract', 'summary', 'switch_case', 'sysconfig', 'tan', 'tanh', 'tensor_scatter_nd_add', 'tensor_scatter_nd_max', 'tensor_scatter_nd_min', 'tensor_scatter_nd_sub', 'tensor_scatter_nd_update', 'tensordot', 'test', 'tile', 'timestamp', 'tools', 'tpu', 'train', 'transpose', 'truediv', 'truncatediv', 'truncatemod', 'tuple', 'type_spec_from_value', 'types', 'uint16', 'uint32', 'uint64', 'uint8', 'unique', 'unique_with_counts', 'unravel_index', 'unstack', 'variable_creator_scope', 'variant', 'vectorized_map', 'version', 'where', 'while_loop', 'xla', 'zeros', 'zeros_initializer', 'zeros_like']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(dir(tf))  # ¿\"data\" aparece en esta lista?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:23:43.620561Z",
     "start_time": "2025-05-23T21:23:43.432209Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pretrained_weights: Optional[np.ndarray] = None\n",
    "if USE_PRETRAINED:\n",
    "    if REMAP_EMBEDDINGS:\n",
    "        pretrained_weights = np.zeros(\n",
    "            (len(diccionario.token2id) + 1, wv_model.vector_size),  dtype=np.float32)\n",
    "        for token, _id in diccionario.token2id.items():\n",
    "            if token in wv_model:\n",
    "                pretrained_weights[_id + 1] = wv_model[token]\n",
    "            else:\n",
    "                # In W2V, OOV will not have a representation. We will use 0.\n",
    "                pass\n",
    "    else:\n",
    "        # Not recommended (this will consume A LOT of RAM) PORQUE CARGA TODOS LOS VECTORES DEL MODELO.\n",
    "        pretrained_weights = np.zeros((wv_model.vectors.shape[0] + 1, wv_model.vector_size,),  dtype=np.float32)\n",
    "        pretrained_weights[1:, :] = wv_model.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:23:43.628885Z",
     "start_time": "2025-05-23T21:23:43.626056Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "       [-0.0307,  0.0032,  0.0128, ..., -0.0154,  0.0374,  0.0234],\n",
       "       [ 0.0519, -0.0079, -0.0013, ..., -0.0154, -0.0353, -0.0235],\n",
       "       [ 0.0058, -0.0161,  0.062 , ...,  0.0129,  0.019 ,  0.0177],\n",
       "       [-0.042 , -0.0113,  0.0837, ..., -0.0396, -0.0253, -0.0045]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_weights[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:23:43.906826Z",
     "start_time": "2025-05-23T21:23:43.899264Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "\n",
    "class SimpleAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units: int, **kwargs):\n",
    "        super(SimpleAttention, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.dropout_s1 = tf.keras.layers.Dropout(0.3)\n",
    "        self.dropout_s2 = tf.keras.layers.Dropout(0.2)\n",
    "        self.W_s1 = tf.keras.layers.Dense(units, activation='tanh', use_bias=True, name=\"attention_transform\")\n",
    "        # Dense layer to compute attention scores (context vector)\n",
    "        self.W_s2 = tf.keras.layers.Dense(1, use_bias=False, name=\"attention_scorer\")\n",
    "        self.supports_masking = True  # Declare that this layer supports masking\n",
    "\n",
    "    def call(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> tf.Tensor:\n",
    "        # inputs shape: (batch_size, sequence_length, embedding_dim)\n",
    "        # mask shape: (batch_size, sequence_length) boolean tensor\n",
    "\n",
    "        # Attention hidden states\n",
    "        hidden_states = self.dropout_s1(self.W_s1(inputs))\n",
    "\n",
    "        # Compute attention scores\n",
    "        scores = self.dropout_s2(self.W_s2(hidden_states))\n",
    "\n",
    "        if mask is not None:\n",
    "            # Apply the mask to the scores before softmax\n",
    "            expanded_mask = tf.expand_dims(tf.cast(mask, dtype=tf.float32), axis=-1)\n",
    "            # Add a large negative number to masked (padded) scores\n",
    "            scores += (1.0 - expanded_mask) * -1e9\n",
    "\n",
    "        # Compute attention weights\n",
    "        attention_weights = tf.nn.softmax(scores, axis=1)\n",
    "\n",
    "        # Compute the context vector (weighted sum of input embeddings)\n",
    "        context_vector = tf.reduce_sum(inputs * attention_weights, axis=1)\n",
    "\n",
    "        return context_vector\n",
    "\n",
    "    def get_config(self) -> dict:\n",
    "        config = super(SimpleAttention, self).get_config()\n",
    "        config.update({\"units\": self.units})\n",
    "        return config\n",
    "\n",
    "    def compute_mask(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> Optional[tf.Tensor]:\n",
    "        return None\n",
    "\n",
    "\n",
    "def build_and_compile_model_2(\n",
    "        input_length: int = 32,\n",
    "        dictionary_size: int = 1000,\n",
    "        embedding_size: int = 300,\n",
    "        learning_rate: float = 0.001,\n",
    "        trainable_embedding: bool = False,\n",
    "        pretrained_weights: Optional[np.ndarray] = None,\n",
    "        attention_units: int = 4,\n",
    ") -> tf.keras.Model:\n",
    "    input_1 = tf.keras.Input((input_length,), dtype=tf.int32, name=\"input_1\")\n",
    "    input_2 = tf.keras.Input((input_length,), dtype=tf.int32, name=\"input_2\")\n",
    "\n",
    "    # Determine effective embedding parameters\n",
    "    if pretrained_weights is not None:\n",
    "        effective_dictionary_size = pretrained_weights.shape[0]\n",
    "        effective_embedding_size = pretrained_weights.shape[1]\n",
    "        embedding_initializer = tf.keras.initializers.Constant(pretrained_weights)\n",
    "        is_embedding_trainable = trainable_embedding\n",
    "        embedding_layer_name = \"embedding_pretrained\"\n",
    "    else:\n",
    "        effective_dictionary_size = dictionary_size\n",
    "        effective_embedding_size = embedding_size\n",
    "        embedding_initializer = 'uniform'\n",
    "        is_embedding_trainable = True\n",
    "        embedding_layer_name = \"embedding\"\n",
    "\n",
    "    # Shared Embedding Layer\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=effective_dictionary_size,\n",
    "        output_dim=effective_embedding_size,\n",
    "        input_length=input_length,\n",
    "        mask_zero=True,\n",
    "        embeddings_initializer=embedding_initializer,\n",
    "        trainable=is_embedding_trainable,\n",
    "        name=embedding_layer_name\n",
    "    )\n",
    "\n",
    "    # Apply embedding layer to both inputs\n",
    "    embedded_1 = embedding_layer(input_1)  # Shape: (batch_size, input_length, effective_embedding_size)\n",
    "    embedded_2 = embedding_layer(input_2)  # Shape: (batch_size, input_length, effective_embedding_size)\n",
    "\n",
    "    # Shared Attention Layer\n",
    "    # Input: (batch_size, input_length, effective_embedding_size) with a mask\n",
    "    # Output: (batch_size, effective_embedding_size)\n",
    "    sentence_attention_layer = SimpleAttention(units=attention_units, name=\"sentence_attention\")\n",
    "    # sentence_attention_layer = tf.keras.layers.GlobalAveragePooling1D(name=\"sentence_attention_layer\")\n",
    "\n",
    "    sentence_vector_1 = sentence_attention_layer(embedded_1)\n",
    "    sentence_vector_2 = sentence_attention_layer(embedded_2)\n",
    "\n",
    "    # Projection layer\n",
    "    first_projection_layer = tf.keras.layers.Dense(\n",
    "        effective_embedding_size,\n",
    "        activation='tanh',\n",
    "        kernel_initializer=tf.keras.initializers.Identity(),\n",
    "        bias_initializer=tf.keras.initializers.Zeros(),\n",
    "        name=\"projection_layer\"\n",
    "    )\n",
    "    dropout = tf.keras.layers.Dropout(0.2, name=\"projection_dropout\")\n",
    "    projected_1 = dropout(first_projection_layer(sentence_vector_1))\n",
    "    projected_2 = dropout(first_projection_layer(sentence_vector_2))\n",
    "\n",
    "    # Normalize the projected vectors (L2 normalization)\n",
    "    normalized_1 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_1\"\n",
    "    )(projected_1)\n",
    "    normalized_2 = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.linalg.l2_normalize(x, axis=1), name=\"normalize_2\"\n",
    "    )(projected_2)\n",
    "\n",
    "    # Compute Cosine Similarity = X * Y / (||X|| * ||Y||) \n",
    "    similarity_score = tf.keras.layers.Lambda(\n",
    "        lambda x: tf.reduce_sum(x[0] * x[1], axis=1, keepdims=True), name=\"cosine_similarity\"\n",
    "    )([normalized_1, normalized_2])\n",
    "\n",
    "    # Scale similarity from [-1, 1] to [0, 1]\n",
    "    output_layer = tf.keras.layers.Lambda(\n",
    "        lambda x: 0.5 * (1.0 + x), name=\"output_scaling\"\n",
    "    )(similarity_score)\n",
    "\n",
    "    # Define the Keras Model\n",
    "    model = tf.keras.Model(\n",
    "        inputs=[input_1, input_2],\n",
    "        outputs=output_layer,\n",
    "        name=\"sequence_similarity_attention_model\"\n",
    "    )\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        loss='mean_squared_error',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        metrics=['mae'],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:30:21.742746Z",
     "start_time": "2025-05-23T21:29:56.965332Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jiahu\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:232: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Epoch 1/128\n",
      "Epoch 1/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.1322 - mae: 0.3259 - val_loss: 0.1420 - val_mae: 0.3403\n",
      "Epoch 2/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 21ms/step - loss: 0.1322 - mae: 0.3259 - val_loss: 0.1420 - val_mae: 0.3403\n",
      "Epoch 2/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0907 - mae: 0.2633 - val_loss: 0.1263 - val_mae: 0.3158\n",
      "Epoch 3/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0907 - mae: 0.2633 - val_loss: 0.1263 - val_mae: 0.3158\n",
      "Epoch 3/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0780 - mae: 0.2379 - val_loss: 0.1208 - val_mae: 0.3083\n",
      "Epoch 4/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0780 - mae: 0.2379 - val_loss: 0.1208 - val_mae: 0.3083\n",
      "Epoch 4/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0682 - mae: 0.2201 - val_loss: 0.1179 - val_mae: 0.3045\n",
      "Epoch 5/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0682 - mae: 0.2201 - val_loss: 0.1179 - val_mae: 0.3045\n",
      "Epoch 5/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.2038 - val_loss: 0.1157 - val_mae: 0.3015\n",
      "Epoch 6/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0601 - mae: 0.2038 - val_loss: 0.1157 - val_mae: 0.3015\n",
      "Epoch 6/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0572 - mae: 0.1976 - val_loss: 0.1147 - val_mae: 0.3007\n",
      "Epoch 7/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0572 - mae: 0.1976 - val_loss: 0.1147 - val_mae: 0.3007\n",
      "Epoch 7/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0542 - mae: 0.1886 - val_loss: 0.1139 - val_mae: 0.2999\n",
      "Epoch 8/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0542 - mae: 0.1886 - val_loss: 0.1139 - val_mae: 0.2999\n",
      "Epoch 8/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0518 - mae: 0.1859 - val_loss: 0.1127 - val_mae: 0.2980\n",
      "Epoch 9/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0518 - mae: 0.1859 - val_loss: 0.1127 - val_mae: 0.2980\n",
      "Epoch 9/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0500 - mae: 0.1792 - val_loss: 0.1130 - val_mae: 0.2990\n",
      "Epoch 10/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0500 - mae: 0.1792 - val_loss: 0.1130 - val_mae: 0.2990\n",
      "Epoch 10/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0492 - mae: 0.1783 - val_loss: 0.1121 - val_mae: 0.2983\n",
      "Epoch 11/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0492 - mae: 0.1783 - val_loss: 0.1121 - val_mae: 0.2983\n",
      "Epoch 11/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0487 - mae: 0.1790 - val_loss: 0.1121 - val_mae: 0.2991\n",
      "Epoch 12/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0487 - mae: 0.1790 - val_loss: 0.1121 - val_mae: 0.2991\n",
      "Epoch 12/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0459 - mae: 0.1728 - val_loss: 0.1135 - val_mae: 0.3014\n",
      "Epoch 13/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0459 - mae: 0.1728 - val_loss: 0.1135 - val_mae: 0.3014\n",
      "Epoch 13/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0455 - mae: 0.1722 - val_loss: 0.1125 - val_mae: 0.3003\n",
      "Epoch 14/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0455 - mae: 0.1722 - val_loss: 0.1125 - val_mae: 0.3003\n",
      "Epoch 14/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0440 - mae: 0.1702 - val_loss: 0.1123 - val_mae: 0.3004\n",
      "Epoch 15/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0440 - mae: 0.1702 - val_loss: 0.1123 - val_mae: 0.3004\n",
      "Epoch 15/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0447 - mae: 0.1695 - val_loss: 0.1117 - val_mae: 0.2993\n",
      "Epoch 16/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0447 - mae: 0.1695 - val_loss: 0.1117 - val_mae: 0.2993\n",
      "Epoch 16/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0436 - mae: 0.1678 - val_loss: 0.1117 - val_mae: 0.2991\n",
      "Epoch 17/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0436 - mae: 0.1678 - val_loss: 0.1117 - val_mae: 0.2991\n",
      "Epoch 17/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0429 - mae: 0.1670 - val_loss: 0.1127 - val_mae: 0.3009\n",
      "Epoch 18/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0429 - mae: 0.1670 - val_loss: 0.1127 - val_mae: 0.3009\n",
      "Epoch 18/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0426 - mae: 0.1648 - val_loss: 0.1131 - val_mae: 0.3013\n",
      "Epoch 19/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0426 - mae: 0.1648 - val_loss: 0.1131 - val_mae: 0.3013\n",
      "Epoch 19/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0419 - mae: 0.1641 - val_loss: 0.1132 - val_mae: 0.3020\n",
      "Epoch 20/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0419 - mae: 0.1641 - val_loss: 0.1132 - val_mae: 0.3020\n",
      "Epoch 20/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0436 - mae: 0.1655 - val_loss: 0.1132 - val_mae: 0.3024\n",
      "Epoch 21/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0436 - mae: 0.1655 - val_loss: 0.1132 - val_mae: 0.3024\n",
      "Epoch 21/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0411 - mae: 0.1623 - val_loss: 0.1126 - val_mae: 0.3016\n",
      "Epoch 22/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0411 - mae: 0.1623 - val_loss: 0.1126 - val_mae: 0.3016\n",
      "Epoch 22/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0415 - mae: 0.1629 - val_loss: 0.1137 - val_mae: 0.3043\n",
      "Epoch 23/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.0415 - mae: 0.1629 - val_loss: 0.1137 - val_mae: 0.3043\n",
      "Epoch 23/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0416 - mae: 0.1636 - val_loss: 0.1127 - val_mae: 0.3028\n",
      "Epoch 24/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0416 - mae: 0.1636 - val_loss: 0.1127 - val_mae: 0.3028\n",
      "Epoch 24/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0413 - mae: 0.1634 - val_loss: 0.1135 - val_mae: 0.3044\n",
      "Epoch 25/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 0.0413 - mae: 0.1634 - val_loss: 0.1135 - val_mae: 0.3044\n",
      "Epoch 25/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0394 - mae: 0.1602 - val_loss: 0.1129 - val_mae: 0.3035\n",
      "Epoch 26/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 0.0394 - mae: 0.1602 - val_loss: 0.1129 - val_mae: 0.3035\n",
      "Epoch 26/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0409 - mae: 0.1627 - val_loss: 0.1122 - val_mae: 0.3023\n",
      "Epoch 27/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0409 - mae: 0.1627 - val_loss: 0.1122 - val_mae: 0.3023\n",
      "Epoch 27/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0393 - mae: 0.1581 - val_loss: 0.1126 - val_mae: 0.3029\n",
      "Epoch 28/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0393 - mae: 0.1581 - val_loss: 0.1126 - val_mae: 0.3029\n",
      "Epoch 28/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0388 - mae: 0.1566 - val_loss: 0.1128 - val_mae: 0.3035\n",
      "Epoch 29/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0388 - mae: 0.1566 - val_loss: 0.1128 - val_mae: 0.3035\n",
      "Epoch 29/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0376 - mae: 0.1553 - val_loss: 0.1133 - val_mae: 0.3039\n",
      "Epoch 30/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0376 - mae: 0.1553 - val_loss: 0.1133 - val_mae: 0.3039\n",
      "Epoch 30/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0375 - mae: 0.1539 - val_loss: 0.1129 - val_mae: 0.3038\n",
      "Epoch 31/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0375 - mae: 0.1539 - val_loss: 0.1129 - val_mae: 0.3038\n",
      "Epoch 31/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0380 - mae: 0.1552 - val_loss: 0.1121 - val_mae: 0.3022\n",
      "Epoch 32/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0380 - mae: 0.1552 - val_loss: 0.1121 - val_mae: 0.3022\n",
      "Epoch 32/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0385 - mae: 0.1577 - val_loss: 0.1147 - val_mae: 0.3064\n",
      "Epoch 33/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.0385 - mae: 0.1577 - val_loss: 0.1147 - val_mae: 0.3064\n",
      "Epoch 33/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0366 - mae: 0.1516 - val_loss: 0.1129 - val_mae: 0.3038\n",
      "Epoch 34/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0366 - mae: 0.1516 - val_loss: 0.1129 - val_mae: 0.3038\n",
      "Epoch 34/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0382 - mae: 0.1556 - val_loss: 0.1144 - val_mae: 0.3063\n",
      "Epoch 35/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0382 - mae: 0.1556 - val_loss: 0.1144 - val_mae: 0.3063\n",
      "Epoch 35/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0406 - mae: 0.1601 - val_loss: 0.1129 - val_mae: 0.3042\n",
      "Epoch 36/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0406 - mae: 0.1601 - val_loss: 0.1129 - val_mae: 0.3042\n",
      "Epoch 36/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0380 - mae: 0.1553 - val_loss: 0.1139 - val_mae: 0.3060\n",
      "Epoch 37/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0380 - mae: 0.1553 - val_loss: 0.1139 - val_mae: 0.3060\n",
      "Epoch 37/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0371 - mae: 0.1536 - val_loss: 0.1131 - val_mae: 0.3050\n",
      "Epoch 38/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0371 - mae: 0.1536 - val_loss: 0.1131 - val_mae: 0.3050\n",
      "Epoch 38/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0364 - mae: 0.1549 - val_loss: 0.1136 - val_mae: 0.3057\n",
      "Epoch 39/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0364 - mae: 0.1549 - val_loss: 0.1136 - val_mae: 0.3057\n",
      "Epoch 39/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0381 - mae: 0.1567 - val_loss: 0.1140 - val_mae: 0.3067\n",
      "Epoch 40/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0381 - mae: 0.1567 - val_loss: 0.1140 - val_mae: 0.3067\n",
      "Epoch 40/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0380 - mae: 0.1567 - val_loss: 0.1126 - val_mae: 0.3042\n",
      "Epoch 41/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0380 - mae: 0.1567 - val_loss: 0.1126 - val_mae: 0.3042\n",
      "Epoch 41/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0333 - mae: 0.1479 - val_loss: 0.1121 - val_mae: 0.3031\n",
      "Epoch 42/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0333 - mae: 0.1479 - val_loss: 0.1121 - val_mae: 0.3031\n",
      "Epoch 42/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0375 - mae: 0.1531 - val_loss: 0.1134 - val_mae: 0.3059\n",
      "Epoch 43/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0375 - mae: 0.1531 - val_loss: 0.1134 - val_mae: 0.3059\n",
      "Epoch 43/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0350 - mae: 0.1476 - val_loss: 0.1133 - val_mae: 0.3055\n",
      "Epoch 44/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0350 - mae: 0.1476 - val_loss: 0.1133 - val_mae: 0.3055\n",
      "Epoch 44/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0354 - mae: 0.1504 - val_loss: 0.1120 - val_mae: 0.3035\n",
      "Epoch 45/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0354 - mae: 0.1504 - val_loss: 0.1120 - val_mae: 0.3035\n",
      "Epoch 45/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0359 - mae: 0.1504 - val_loss: 0.1125 - val_mae: 0.3047\n",
      "Epoch 46/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0359 - mae: 0.1504 - val_loss: 0.1125 - val_mae: 0.3047\n",
      "Epoch 46/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0356 - mae: 0.1495 - val_loss: 0.1124 - val_mae: 0.3047\n",
      "Epoch 47/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0356 - mae: 0.1495 - val_loss: 0.1124 - val_mae: 0.3047\n",
      "Epoch 47/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0345 - mae: 0.1499 - val_loss: 0.1123 - val_mae: 0.3042\n",
      "Epoch 48/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0345 - mae: 0.1499 - val_loss: 0.1123 - val_mae: 0.3042\n",
      "Epoch 48/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0355 - mae: 0.1474 - val_loss: 0.1124 - val_mae: 0.3046\n",
      "Epoch 49/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0355 - mae: 0.1474 - val_loss: 0.1124 - val_mae: 0.3046\n",
      "Epoch 49/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0351 - mae: 0.1504 - val_loss: 0.1130 - val_mae: 0.3058\n",
      "Epoch 50/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0351 - mae: 0.1504 - val_loss: 0.1130 - val_mae: 0.3058\n",
      "Epoch 50/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0333 - mae: 0.1470 - val_loss: 0.1125 - val_mae: 0.3047\n",
      "Epoch 51/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0333 - mae: 0.1470 - val_loss: 0.1125 - val_mae: 0.3047\n",
      "Epoch 51/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0352 - mae: 0.1486 - val_loss: 0.1134 - val_mae: 0.3062\n",
      "Epoch 52/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0352 - mae: 0.1486 - val_loss: 0.1134 - val_mae: 0.3062\n",
      "Epoch 52/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0350 - mae: 0.1496 - val_loss: 0.1125 - val_mae: 0.3047\n",
      "Epoch 53/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0350 - mae: 0.1496 - val_loss: 0.1125 - val_mae: 0.3047\n",
      "Epoch 53/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0343 - mae: 0.1484 - val_loss: 0.1133 - val_mae: 0.3059\n",
      "Epoch 54/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0343 - mae: 0.1484 - val_loss: 0.1133 - val_mae: 0.3059\n",
      "Epoch 54/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0346 - mae: 0.1475 - val_loss: 0.1121 - val_mae: 0.3043\n",
      "Epoch 55/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0346 - mae: 0.1475 - val_loss: 0.1121 - val_mae: 0.3043\n",
      "Epoch 55/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0339 - mae: 0.1461 - val_loss: 0.1116 - val_mae: 0.3035\n",
      "Epoch 56/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0339 - mae: 0.1461 - val_loss: 0.1116 - val_mae: 0.3035\n",
      "Epoch 56/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0336 - mae: 0.1482 - val_loss: 0.1120 - val_mae: 0.3040\n",
      "Epoch 57/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0336 - mae: 0.1482 - val_loss: 0.1120 - val_mae: 0.3040\n",
      "Epoch 57/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.1123 - val_mae: 0.3049\n",
      "Epoch 58/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0323 - mae: 0.1444 - val_loss: 0.1123 - val_mae: 0.3049\n",
      "Epoch 58/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0327 - mae: 0.1451 - val_loss: 0.1122 - val_mae: 0.3048\n",
      "Epoch 59/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0327 - mae: 0.1451 - val_loss: 0.1122 - val_mae: 0.3048\n",
      "Epoch 59/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0330 - mae: 0.1445 - val_loss: 0.1121 - val_mae: 0.3046\n",
      "Epoch 60/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0330 - mae: 0.1445 - val_loss: 0.1121 - val_mae: 0.3046\n",
      "Epoch 60/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0333 - mae: 0.1451 - val_loss: 0.1130 - val_mae: 0.3057\n",
      "Epoch 61/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0333 - mae: 0.1451 - val_loss: 0.1130 - val_mae: 0.3057\n",
      "Epoch 61/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0317 - mae: 0.1405 - val_loss: 0.1127 - val_mae: 0.3050\n",
      "Epoch 62/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0317 - mae: 0.1405 - val_loss: 0.1127 - val_mae: 0.3050\n",
      "Epoch 62/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0334 - mae: 0.1454 - val_loss: 0.1126 - val_mae: 0.3052\n",
      "Epoch 63/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0334 - mae: 0.1454 - val_loss: 0.1126 - val_mae: 0.3052\n",
      "Epoch 63/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0312 - mae: 0.1410 - val_loss: 0.1114 - val_mae: 0.3035\n",
      "Epoch 64/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0312 - mae: 0.1410 - val_loss: 0.1114 - val_mae: 0.3035\n",
      "Epoch 64/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0321 - mae: 0.1432 - val_loss: 0.1118 - val_mae: 0.3045\n",
      "Epoch 65/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0321 - mae: 0.1432 - val_loss: 0.1118 - val_mae: 0.3045\n",
      "Epoch 65/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0329 - mae: 0.1431 - val_loss: 0.1109 - val_mae: 0.3029\n",
      "Epoch 66/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0329 - mae: 0.1431 - val_loss: 0.1109 - val_mae: 0.3029\n",
      "Epoch 66/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0329 - mae: 0.1435 - val_loss: 0.1117 - val_mae: 0.3041\n",
      "Epoch 67/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0329 - mae: 0.1435 - val_loss: 0.1117 - val_mae: 0.3041\n",
      "Epoch 67/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0321 - mae: 0.1429 - val_loss: 0.1114 - val_mae: 0.3037\n",
      "Epoch 68/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0321 - mae: 0.1429 - val_loss: 0.1114 - val_mae: 0.3037\n",
      "Epoch 68/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0321 - mae: 0.1440 - val_loss: 0.1109 - val_mae: 0.3028\n",
      "Epoch 69/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0321 - mae: 0.1440 - val_loss: 0.1109 - val_mae: 0.3028\n",
      "Epoch 69/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0313 - mae: 0.1406 - val_loss: 0.1117 - val_mae: 0.3042\n",
      "Epoch 70/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0313 - mae: 0.1406 - val_loss: 0.1117 - val_mae: 0.3042\n",
      "Epoch 70/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0302 - mae: 0.1393 - val_loss: 0.1109 - val_mae: 0.3029\n",
      "Epoch 71/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0302 - mae: 0.1393 - val_loss: 0.1109 - val_mae: 0.3029\n",
      "Epoch 71/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.1119 - val_mae: 0.3044\n",
      "Epoch 72/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0310 - mae: 0.1408 - val_loss: 0.1119 - val_mae: 0.3044\n",
      "Epoch 72/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0321 - mae: 0.1415 - val_loss: 0.1118 - val_mae: 0.3045\n",
      "Epoch 73/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0321 - mae: 0.1415 - val_loss: 0.1118 - val_mae: 0.3045\n",
      "Epoch 73/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0323 - mae: 0.1441 - val_loss: 0.1118 - val_mae: 0.3044\n",
      "Epoch 74/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0323 - mae: 0.1441 - val_loss: 0.1118 - val_mae: 0.3044\n",
      "Epoch 74/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0298 - mae: 0.1373 - val_loss: 0.1120 - val_mae: 0.3047\n",
      "Epoch 75/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0298 - mae: 0.1373 - val_loss: 0.1120 - val_mae: 0.3047\n",
      "Epoch 75/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0316 - mae: 0.1410 - val_loss: 0.1114 - val_mae: 0.3037\n",
      "Epoch 76/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0316 - mae: 0.1410 - val_loss: 0.1114 - val_mae: 0.3037\n",
      "Epoch 76/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0312 - mae: 0.1397 - val_loss: 0.1125 - val_mae: 0.3056\n",
      "Epoch 77/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0312 - mae: 0.1397 - val_loss: 0.1125 - val_mae: 0.3056\n",
      "Epoch 77/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0319 - mae: 0.1409 - val_loss: 0.1110 - val_mae: 0.3031\n",
      "Epoch 78/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0319 - mae: 0.1409 - val_loss: 0.1110 - val_mae: 0.3031\n",
      "Epoch 78/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0326 - mae: 0.1433 - val_loss: 0.1106 - val_mae: 0.3028\n",
      "Epoch 79/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0326 - mae: 0.1433 - val_loss: 0.1106 - val_mae: 0.3028\n",
      "Epoch 79/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0303 - mae: 0.1383 - val_loss: 0.1119 - val_mae: 0.3050\n",
      "Epoch 80/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0303 - mae: 0.1383 - val_loss: 0.1119 - val_mae: 0.3050\n",
      "Epoch 80/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0311 - mae: 0.1413 - val_loss: 0.1116 - val_mae: 0.3046\n",
      "Epoch 81/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0311 - mae: 0.1413 - val_loss: 0.1116 - val_mae: 0.3046\n",
      "Epoch 81/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0310 - mae: 0.1401 - val_loss: 0.1111 - val_mae: 0.3036\n",
      "Epoch 82/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0310 - mae: 0.1401 - val_loss: 0.1111 - val_mae: 0.3036\n",
      "Epoch 82/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0294 - mae: 0.1353 - val_loss: 0.1105 - val_mae: 0.3024\n",
      "Epoch 83/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0294 - mae: 0.1353 - val_loss: 0.1105 - val_mae: 0.3024\n",
      "Epoch 83/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0299 - mae: 0.1388 - val_loss: 0.1114 - val_mae: 0.3039\n",
      "Epoch 84/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0299 - mae: 0.1388 - val_loss: 0.1114 - val_mae: 0.3039\n",
      "Epoch 84/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0304 - mae: 0.1383 - val_loss: 0.1110 - val_mae: 0.3036\n",
      "Epoch 85/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 0.0304 - mae: 0.1383 - val_loss: 0.1110 - val_mae: 0.3036\n",
      "Epoch 85/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0314 - mae: 0.1392 - val_loss: 0.1107 - val_mae: 0.3029\n",
      "Epoch 86/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0314 - mae: 0.1392 - val_loss: 0.1107 - val_mae: 0.3029\n",
      "Epoch 86/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0308 - mae: 0.1379 - val_loss: 0.1107 - val_mae: 0.3030\n",
      "Epoch 87/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0308 - mae: 0.1379 - val_loss: 0.1107 - val_mae: 0.3030\n",
      "Epoch 87/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0303 - mae: 0.1381 - val_loss: 0.1113 - val_mae: 0.3040\n",
      "Epoch 88/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0303 - mae: 0.1381 - val_loss: 0.1113 - val_mae: 0.3040\n",
      "Epoch 88/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0297 - mae: 0.1379 - val_loss: 0.1105 - val_mae: 0.3028\n",
      "Epoch 89/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0297 - mae: 0.1379 - val_loss: 0.1105 - val_mae: 0.3028\n",
      "Epoch 89/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0292 - mae: 0.1356 - val_loss: 0.1109 - val_mae: 0.3034\n",
      "Epoch 90/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0292 - mae: 0.1356 - val_loss: 0.1109 - val_mae: 0.3034\n",
      "Epoch 90/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0298 - mae: 0.1385 - val_loss: 0.1110 - val_mae: 0.3035\n",
      "Epoch 91/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0298 - mae: 0.1385 - val_loss: 0.1110 - val_mae: 0.3035\n",
      "Epoch 91/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0319 - mae: 0.1411 - val_loss: 0.1107 - val_mae: 0.3034\n",
      "Epoch 92/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0319 - mae: 0.1411 - val_loss: 0.1107 - val_mae: 0.3034\n",
      "Epoch 92/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0301 - mae: 0.1392 - val_loss: 0.1105 - val_mae: 0.3030\n",
      "Epoch 93/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0301 - mae: 0.1392 - val_loss: 0.1105 - val_mae: 0.3030\n",
      "Epoch 93/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0310 - mae: 0.1369 - val_loss: 0.1095 - val_mae: 0.3013\n",
      "Epoch 94/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0310 - mae: 0.1369 - val_loss: 0.1095 - val_mae: 0.3013\n",
      "Epoch 94/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0296 - mae: 0.1359 - val_loss: 0.1102 - val_mae: 0.3021\n",
      "Epoch 95/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0296 - mae: 0.1359 - val_loss: 0.1102 - val_mae: 0.3021\n",
      "Epoch 95/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0313 - mae: 0.1391 - val_loss: 0.1090 - val_mae: 0.3002\n",
      "Epoch 96/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0313 - mae: 0.1391 - val_loss: 0.1090 - val_mae: 0.3002\n",
      "Epoch 96/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0292 - mae: 0.1361 - val_loss: 0.1103 - val_mae: 0.3020\n",
      "Epoch 97/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0292 - mae: 0.1361 - val_loss: 0.1103 - val_mae: 0.3020\n",
      "Epoch 97/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0305 - mae: 0.1373 - val_loss: 0.1092 - val_mae: 0.3005\n",
      "Epoch 98/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0305 - mae: 0.1373 - val_loss: 0.1092 - val_mae: 0.3005\n",
      "Epoch 98/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0313 - mae: 0.1401 - val_loss: 0.1109 - val_mae: 0.3034\n",
      "Epoch 99/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0313 - mae: 0.1401 - val_loss: 0.1109 - val_mae: 0.3034\n",
      "Epoch 99/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0295 - mae: 0.1357 - val_loss: 0.1096 - val_mae: 0.3013\n",
      "Epoch 100/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.0295 - mae: 0.1357 - val_loss: 0.1096 - val_mae: 0.3013\n",
      "Epoch 100/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0303 - mae: 0.1385 - val_loss: 0.1096 - val_mae: 0.3016\n",
      "Epoch 101/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0303 - mae: 0.1385 - val_loss: 0.1096 - val_mae: 0.3016\n",
      "Epoch 101/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0300 - mae: 0.1384 - val_loss: 0.1104 - val_mae: 0.3032\n",
      "Epoch 102/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0300 - mae: 0.1384 - val_loss: 0.1104 - val_mae: 0.3032\n",
      "Epoch 102/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0305 - mae: 0.1384 - val_loss: 0.1110 - val_mae: 0.3041\n",
      "Epoch 103/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0305 - mae: 0.1384 - val_loss: 0.1110 - val_mae: 0.3041\n",
      "Epoch 103/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0281 - mae: 0.1337 - val_loss: 0.1085 - val_mae: 0.3002\n",
      "Epoch 104/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0281 - mae: 0.1337 - val_loss: 0.1085 - val_mae: 0.3002\n",
      "Epoch 104/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0281 - mae: 0.1332 - val_loss: 0.1104 - val_mae: 0.3034\n",
      "Epoch 105/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0281 - mae: 0.1332 - val_loss: 0.1104 - val_mae: 0.3034\n",
      "Epoch 105/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0301 - mae: 0.1395 - val_loss: 0.1092 - val_mae: 0.3013\n",
      "Epoch 106/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0301 - mae: 0.1395 - val_loss: 0.1092 - val_mae: 0.3013\n",
      "Epoch 106/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0296 - mae: 0.1373 - val_loss: 0.1091 - val_mae: 0.3009\n",
      "Epoch 107/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0296 - mae: 0.1373 - val_loss: 0.1091 - val_mae: 0.3009\n",
      "Epoch 107/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0299 - mae: 0.1380 - val_loss: 0.1086 - val_mae: 0.3004\n",
      "Epoch 108/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0299 - mae: 0.1380 - val_loss: 0.1086 - val_mae: 0.3004\n",
      "Epoch 108/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0286 - mae: 0.1344 - val_loss: 0.1101 - val_mae: 0.3029\n",
      "Epoch 109/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0286 - mae: 0.1344 - val_loss: 0.1101 - val_mae: 0.3029\n",
      "Epoch 109/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0302 - mae: 0.1370 - val_loss: 0.1092 - val_mae: 0.3010\n",
      "Epoch 110/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0302 - mae: 0.1370 - val_loss: 0.1092 - val_mae: 0.3010\n",
      "Epoch 110/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0290 - mae: 0.1369 - val_loss: 0.1091 - val_mae: 0.3010\n",
      "Epoch 111/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0290 - mae: 0.1369 - val_loss: 0.1091 - val_mae: 0.3010\n",
      "Epoch 111/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0293 - mae: 0.1371 - val_loss: 0.1075 - val_mae: 0.2982\n",
      "Epoch 112/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0293 - mae: 0.1371 - val_loss: 0.1075 - val_mae: 0.2982\n",
      "Epoch 112/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0272 - mae: 0.1306 - val_loss: 0.1085 - val_mae: 0.2997\n",
      "Epoch 113/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0272 - mae: 0.1306 - val_loss: 0.1085 - val_mae: 0.2997\n",
      "Epoch 113/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0301 - mae: 0.1377 - val_loss: 0.1081 - val_mae: 0.2994\n",
      "Epoch 114/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0301 - mae: 0.1377 - val_loss: 0.1081 - val_mae: 0.2994\n",
      "Epoch 114/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0304 - mae: 0.1389 - val_loss: 0.1090 - val_mae: 0.3006\n",
      "Epoch 115/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0304 - mae: 0.1389 - val_loss: 0.1090 - val_mae: 0.3006\n",
      "Epoch 115/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0296 - mae: 0.1363 - val_loss: 0.1076 - val_mae: 0.2983\n",
      "Epoch 116/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0296 - mae: 0.1363 - val_loss: 0.1076 - val_mae: 0.2983\n",
      "Epoch 116/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0287 - mae: 0.1349 - val_loss: 0.1080 - val_mae: 0.2991\n",
      "Epoch 117/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0287 - mae: 0.1349 - val_loss: 0.1080 - val_mae: 0.2991\n",
      "Epoch 117/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0299 - mae: 0.1380 - val_loss: 0.1079 - val_mae: 0.2989\n",
      "Epoch 118/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0299 - mae: 0.1380 - val_loss: 0.1079 - val_mae: 0.2989\n",
      "Epoch 118/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0287 - mae: 0.1345 - val_loss: 0.1097 - val_mae: 0.3016\n",
      "Epoch 119/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0287 - mae: 0.1345 - val_loss: 0.1097 - val_mae: 0.3016\n",
      "Epoch 119/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0298 - mae: 0.1369 - val_loss: 0.1079 - val_mae: 0.2988\n",
      "Epoch 120/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0298 - mae: 0.1369 - val_loss: 0.1079 - val_mae: 0.2988\n",
      "Epoch 120/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0286 - mae: 0.1363 - val_loss: 0.1079 - val_mae: 0.2986\n",
      "Epoch 121/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0286 - mae: 0.1363 - val_loss: 0.1079 - val_mae: 0.2986\n",
      "Epoch 121/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0285 - mae: 0.1334 - val_loss: 0.1087 - val_mae: 0.2999\n",
      "Epoch 122/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0285 - mae: 0.1334 - val_loss: 0.1087 - val_mae: 0.2999\n",
      "Epoch 122/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0288 - mae: 0.1350 - val_loss: 0.1076 - val_mae: 0.2982\n",
      "Epoch 123/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0288 - mae: 0.1350 - val_loss: 0.1076 - val_mae: 0.2982\n",
      "Epoch 123/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1327 - val_loss: 0.1078 - val_mae: 0.2986\n",
      "Epoch 124/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0277 - mae: 0.1327 - val_loss: 0.1078 - val_mae: 0.2986\n",
      "Epoch 124/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0288 - mae: 0.1347 - val_loss: 0.1081 - val_mae: 0.2988\n",
      "Epoch 125/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0288 - mae: 0.1347 - val_loss: 0.1081 - val_mae: 0.2988\n",
      "Epoch 125/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0280 - mae: 0.1331 - val_loss: 0.1076 - val_mae: 0.2983\n",
      "Epoch 126/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0280 - mae: 0.1331 - val_loss: 0.1076 - val_mae: 0.2983\n",
      "Epoch 126/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0281 - mae: 0.1336 - val_loss: 0.1076 - val_mae: 0.2982\n",
      "Epoch 127/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0281 - mae: 0.1336 - val_loss: 0.1076 - val_mae: 0.2982\n",
      "Epoch 127/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0272 - mae: 0.1318 - val_loss: 0.1077 - val_mae: 0.2981\n",
      "Epoch 128/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.0272 - mae: 0.1318 - val_loss: 0.1077 - val_mae: 0.2981\n",
      "Epoch 128/128\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0287 - mae: 0.1351 - val_loss: 0.1075 - val_mae: 0.2981\n",
      "\u001b[1m39/39\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.0287 - mae: 0.1351 - val_loss: 0.1075 - val_mae: 0.2981\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x23d1d8bec10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir i compilar el model\n",
    "model = build_and_compile_model_2(pretrained_weights=pretrained_weights, learning_rate=1e-3)\n",
    "# Entrenar el model\n",
    "model.fit(train_dataset, epochs=num_epochs, validation_data=val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:30:21.779796Z",
     "start_time": "2025-05-23T21:30:21.750444Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequence_similarity_attention_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequence_similarity_attention_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_pretrain… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)   │  <span style=\"color: #00af00; text-decoration-color: #00af00\">3,937,800</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ input_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">NotEqual</span>)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sentence_attention  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,208</span> │ embedding_pretra… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleAttention</span>)   │                   │            │ not_equal[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
       "│                     │                   │            │ embedding_pretra… │\n",
       "│                     │                   │            │ not_equal_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ projection_layer    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │     <span style=\"color: #00af00; text-decoration-color: #00af00\">90,300</span> │ sentence_attenti… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │ sentence_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ projection_dropout  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ projection_layer… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)           │                   │            │ projection_layer… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ normalize_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ projection_dropo… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ normalize_2         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">300</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ projection_dropo… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cosine_similarity   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ normalize_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │ normalize_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_scaling      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ cosine_similarit… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)            │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_1             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_2             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding_pretrain… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m300\u001b[0m)   │  \u001b[38;5;34m3,937,800\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ not_equal_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ input_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "│ (\u001b[38;5;33mNotEqual\u001b[0m)          │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sentence_attention  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │      \u001b[38;5;34m1,208\u001b[0m │ embedding_pretra… │\n",
       "│ (\u001b[38;5;33mSimpleAttention\u001b[0m)   │                   │            │ not_equal[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
       "│                     │                   │            │ embedding_pretra… │\n",
       "│                     │                   │            │ not_equal_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ projection_layer    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │     \u001b[38;5;34m90,300\u001b[0m │ sentence_attenti… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │ sentence_attenti… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ projection_dropout  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ projection_layer… │\n",
       "│ (\u001b[38;5;33mDropout\u001b[0m)           │                   │            │ projection_layer… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ normalize_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ projection_dropo… │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ normalize_2         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m300\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ projection_dropo… │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ cosine_similarity   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ normalize_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │ normalize_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_scaling      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ cosine_similarit… │\n",
       "│ (\u001b[38;5;33mLambda\u001b[0m)            │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,212,326</span> (16.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,212,326\u001b[0m (16.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">91,508</span> (357.45 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m91,508\u001b[0m (357.45 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,937,800</span> (15.02 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m3,937,800\u001b[0m (15.02 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">183,018</span> (714.92 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m183,018\u001b[0m (714.92 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:30:22.150879Z",
     "start_time": "2025-05-23T21:30:21.807767Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Correlación de Pearson: 0.5325664179830356\n",
      "Correlación de Pearson: 0.5325664179830356\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "y_pred = model.predict(x_val)\n",
    "# Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_val.flatten())\n",
    "# Imprimir el coeficiente de correlación de Pearson\n",
    "print(f\"Correlación de Pearson: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:30:22.357279Z",
     "start_time": "2025-05-23T21:30:22.160006Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Correlación de Pearson: 0.7845078833995395\n",
      "Correlación de Pearson: 0.7845078833995395\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import pearsonr\n",
    "# Obtener las predicciones del modelo para los datos de prueba. En este ejemplo vamos a utilizar el corpus de training.\n",
    "y_pred = model.predict(x_train)\n",
    "# Calcular la correlación de Pearson entre las predicciones y los datos de prueba\n",
    "correlation, _ = pearsonr(y_pred.flatten(), y_train.flatten())\n",
    "# Imprimir el coeficiente de correlación de Pearson\n",
    "print(f\"Correlación de Pearson: {correlation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T21:31:00.171648Z",
     "start_time": "2025-05-23T21:31:00.165491Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.19.0'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7   0.25  0.734 0.45  0.4   0.55  0.534 0.5   0.5   0.6  ]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of scores: 2458, Length of y_train: 2458\n",
      "Correlación de Pearson: 0.5332081129168438\n",
      "P-value: 1.2050755660899114e-180\n"
     ]
    }
   ],
   "source": [
    "# predecir para los de train\n",
    "# The issue is that one_hot_evaluation returns a list of scores for all pairs,\n",
    "# while y_train is only for the training portion\n",
    "\n",
    "# Calculate scores for train data only (same length as y_train)\n",
    "train_scores = one_hot_evaluation(sentences_1_preproc[:train_slice], sentences_2_preproc[:train_slice])\n",
    "\n",
    "# Convert the list to a numpy array for proper comparison\n",
    "train_scores = np.array(train_scores)\n",
    "\n",
    "print(f\"Length of scores: {len(train_scores)}, Length of y_train: {len(y_train)}\")\n",
    "\n",
    "# Now both arrays have the same length\n",
    "correlation, p_value = pearsonr(train_scores, y_train.flatten())\n",
    "print(f\"Correlación de Pearson: {correlation}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    "\n",
    "- Model difícil de millorar\n",
    "- Truncar les mesures dels embeddings extreient les paraules que pertanyen al corpus (cuidado amb overfitting al train).\n",
    "- Agregació amb altres TF-IDF. (Modificant la capa Atenció)\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
